{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830a3a5d",
   "metadata": {},
   "source": [
    "# Assignmet 2\n",
    "## Part 1 – Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b2d9b",
   "metadata": {},
   "source": [
    "1. Set your working directory and import the dataset Enaho01A-2023-300.csv using Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec7be1",
   "metadata": {},
   "source": [
    "*Configuración del Directorio de Trabajo usando Rutas Relativas con ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02fe722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta absoluta calculada: C:\\Users\\HOME\\Documents\\GitHub\\assingment_2\\data\n",
      "Directorio de trabajo establecido en: C:\\Users\\HOME\\Documents\\GitHub\\assingment_2\\data\n",
      "Dataset importado con encoding ISO-8859-10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Configurar el directorio de trabajo usando rutas relativas con ../\n",
    "# Para subir 4 niveles desde tu ubicación actual y luego bajar a assingment_2/data\n",
    "relative_path = \"../../../../assingment_2/data\"\n",
    "\n",
    "# 2. Convertir la ruta relativa a una ruta absoluta\n",
    "absolute_path = os.path.abspath(relative_path)\n",
    "print(f\"Ruta absoluta calculada: {absolute_path}\")\n",
    "\n",
    "# 3. Verificar si la ruta existe\n",
    "if not os.path.exists(absolute_path):\n",
    "    print(f\"Error: No se encuentra la ruta: {absolute_path}\")\n",
    "    print(\"Por favor, verifica la estructura de carpetas.\")\n",
    "else:\n",
    "    # 4. Establecer el directorio de trabajo\n",
    "    os.chdir(absolute_path)\n",
    "    print(f\"Directorio de trabajo establecido en: {os.getcwd()}\")\n",
    "    \n",
    "    # 5. Importar el dataset Enaho01A-2023-300.csv\n",
    "    try:\n",
    "        # Intentar primero con ISO-8859-10 como se indica en la nota\n",
    "        df = pd.read_csv(\"Enaho01A-2023-300.csv\", encoding=\"ISO-8859-10\", low_memory=False)\n",
    "        print(\"Dataset importado con encoding ISO-8859-10\")\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            # Si ISO-8859-10 falla, intentar con UTF-8\n",
    "            df = pd.read_csv(\"Enaho01A-2023-300.csv\", encoding=\"UTF-8\", low_memory=False)\n",
    "            print(\"Dataset importado con encoding UTF-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: No se pudo importar el dataset: {e}\")\n",
    "            df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d841b",
   "metadata": {},
   "source": [
    "*realizar las operaciones solicitadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f840ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 5 filas del dataset:\n",
      "    AŅO  MES  CONGLOME  VIVIENDA  HOGAR  CODPERSO  UBIGEO  DOMINIO  ESTRATO  \\\n",
      "0  2023    1      5030         2     11         1   10201        7        4   \n",
      "1  2023    1      5030         2     11         2   10201        7        4   \n",
      "2  2023    1      5030         2     11         3   10201        7        4   \n",
      "3  2023    1      5030         2     11         4   10201        7        4   \n",
      "4  2023    1      5030        11     11         1   10201        7        4   \n",
      "\n",
      "   CODINFOR  ...  I311D$5  I311D$6  I311D$7  I3121C I3122C I315B    FACTOR07  \\\n",
      "0         1  ...                                                  118.374542   \n",
      "1         2  ...                                                  118.374542   \n",
      "2         2  ...                                                  118.374542   \n",
      "3         2  ...                          8                       118.374542   \n",
      "4         1  ...                                                  118.374542   \n",
      "\n",
      "    FACTORA07 NCONGLOME SUB_CONGLOME  \n",
      "0  165.623856      6618            0  \n",
      "1  112.328087      6618            0  \n",
      "2  120.091476      6618            0  \n",
      "3  110.974678      6618            0  \n",
      "4   96.035370      6618            0  \n",
      "\n",
      "[5 rows x 511 columns]\n",
      "\n",
      "Nombres de columnas como lista:\n",
      "['AŅO', 'MES', 'CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO', 'UBIGEO', 'DOMINIO', 'ESTRATO', 'CODINFOR', 'P300N', 'P300I', 'P300A', 'P301A', 'P301B', 'P301C', 'P301D', 'P301A0', 'P301A1', 'P301B0', 'P301B1', 'P301B3', 'P302', 'P302X', 'P302A', 'P302B', 'P303', 'P304A', 'P304B', 'P304C', 'P304D', 'P305', 'P306', 'P307', 'P307A1', 'P307A2', 'P307A3', 'P307A4', 'P307A4_5', 'P307A4_6', 'P307A4_7', 'P307B1', 'P307B2', 'P307B3', 'P307B4', 'P307B4_5', 'P307B4_6', 'P307B4_7', 'P307C', 'P308A', 'P308B', 'P308C', 'P308D', 'P308B1', 'P308B2', 'P308B3', 'P308B4', 'P308B5', 'P308C1', 'P308C2', 'P310', 'P310B1', 'P310C0', 'P310C1', 'P310D1', 'P310D2', 'P310E0', 'P310E1', 'P310E3', 'P311N$1', 'P311N$2', 'P311N$3', 'P311N$4', 'P311N$5', 'P311N$6', 'P311N$7', 'P311N$8', 'P311N$9', 'P311$1', 'P311$2', 'P311$3', 'P311$4', 'P311$5', 'P311$6', 'P311$7', 'P311$8', 'P311$9', 'P311A1$1', 'P311A1$2', 'P311A1$3', 'P311A1$4', 'P311A1$5', 'P311A1$6', 'P311A1$7', 'P311A1$8', 'P311A1$9', 'P311A2$1', 'P311A2$2', 'P311A2$3', 'P311A2$4', 'P311A2$5', 'P311A2$6', 'P311A2$7', 'P311A2$8', 'P311A2$9', 'P311A3$1', 'P311A3$2', 'P311A3$3', 'P311A3$4', 'P311A3$5', 'P311A3$6', 'P311A3$7', 'P311A3$8', 'P311A3$9', 'P311A4$1', 'P311A4$2', 'P311A4$3', 'P311A4$4', 'P311A4$5', 'P311A4$6', 'P311A4$7', 'P311A4$8', 'P311A4$9', 'P311A5$1', 'P311A5$2', 'P311A5$3', 'P311A5$4', 'P311A5$5', 'P311A5$6', 'P311A5$7', 'P311A5$8', 'P311A5$9', 'P311A6$1', 'P311A6$2', 'P311A6$3', 'P311A6$4', 'P311A6$5', 'P311A6$6', 'P311A6$7', 'P311A6$8', 'P311A6$9', 'P311A7$1', 'P311A7$2', 'P311A7$3', 'P311A7$4', 'P311A7$5', 'P311A7$6', 'P311A7$7', 'P311A7$8', 'P311A7$9', 'P311B$1', 'P311B$2', 'P311B$3', 'P311B$4', 'P311B$5', 'P311B$6', 'P311B$7', 'P311B$8', 'P311B$9', 'P311C$1', 'P311C$2', 'P311C$3', 'P311C$4', 'P311C$5', 'P311C$6', 'P311C$7', 'P311C$8', 'P311C$9', 'P311D$1', 'P311D$2', 'P311D$3', 'P311D$4', 'P311D$5', 'P311D$6', 'P311D$7', 'P311D$8', 'P311D$9', 'P311D2$1', 'P311D2$2', 'P311D2$3', 'P311D2$4', 'P311D2$5', 'P311D2$6', 'P311D2$7', 'P311D2$8', 'P311D2$9', 'P311D3$1', 'P311D3$2', 'P311D3$3', 'P311D3$4', 'P311D3$5', 'P311D3$6', 'P311D3$7', 'P311D3$8', 'P311D3$9', 'P311D4$1', 'P311D4$2', 'P311D4$3', 'P311D4$4', 'P311D4$5', 'P311D4$6', 'P311D4$7', 'P311D4$8', 'P311D4$9', 'P311D5$1', 'P311D5$2', 'P311D5$3', 'P311D5$4', 'P311D5$5', 'P311D5$6', 'P311D5$7', 'P311D5$8', 'P311D5$9', 'P311D6$1', 'P311D6$2', 'P311D6$3', 'P311D6$4', 'P311D6$5', 'P311D6$6', 'P311D6$7', 'P311D6$8', 'P311D6$9', 'P311D7$1', 'P311D7$2', 'P311D7$3', 'P311D7$4', 'P311D7$5', 'P311D7$6', 'P311D7$7', 'P311D7$8', 'P311D7$9', 'P311E$1', 'P311E$2', 'P311E$3', 'P311E$4', 'P311E$5', 'P311E$6', 'P311E$7', 'P311E$8', 'P311E$9', 'P311T1', 'P311T22', 'P311T23', 'P311T24', 'P311T25', 'P311T26', 'P311T27', 'P311T2', 'P3121', 'P3121A1', 'P3121A2', 'P3121A3', 'P3121A4', 'P3121A5', 'P3121A6', 'P3121B', 'P3121C', 'P3121C2', 'P3121C3', 'P3121C4', 'P3121C5', 'P3121C6', 'P3121D', 'P3122', 'P3122A1', 'P3122A2', 'P3122A3', 'P3122A4', 'P3122A5', 'P3122A6', 'P3122B', 'P3122C', 'P3122C2', 'P3122C3', 'P3122C4', 'P3122C5', 'P3122C6', 'P3122D', 'P312T1', 'P312T22', 'P312T23', 'P312T24', 'P312T25', 'P312T26', 'P312T2', 'P313', 'P314A', 'P314B$1', 'P314B$2', 'P314B$3', 'P314B$4', 'P314B$5', 'P314B$6', 'P314B$7', 'P314B1_1', 'P314B1_2', 'P314B1_6', 'P314B1_7', 'P314B1_8', 'P314B1_9', 'P314D', 'P3151', 'P3152', 'P3153', 'P3154', 'P3155', 'P3156', 'P315A', 'P315B', 'P315B2', 'P315B3', 'P315B4', 'P315B5', 'P315B6', 'P316$1', 'P316$2', 'P316$3', 'P316$4', 'P316$5', 'P316$6', 'P316$7', 'P316$8', 'P316$9', 'P316$10', 'P316$11', 'P316$12', 'P316A1', 'P316A2', 'P316A3', 'P316A4', 'P316A5', 'P316A6', 'P316B', 'P316C1', 'P316C2', 'P316C3', 'P316C4', 'P316C5', 'P316C6', 'P316C7', 'P316C8', 'P316C9', 'P316C10', 'T313A', 'P203', 'P204', 'P205', 'P206', 'P207', 'P208A', 'P209', 'IMPUTADO', 'TICUEST01A', 'D311B$1', 'D311D2$1', 'D311D3$1', 'D311D4$1', 'D311D5$1', 'D311D6$1', 'D311D7$1', 'D311D$1', 'D311B$2', 'D311D2$2', 'D311D3$2', 'D311D4$2', 'D311D5$2', 'D311D6$2', 'D311D7$2', 'D311D$2', 'D311B$3', 'D311D2$3', 'D311D3$3', 'D311D4$3', 'D311D5$3', 'D311D6$3', 'D311D7$3', 'D311D$3', 'D311B$4', 'D311D2$4', 'D311D3$4', 'D311D4$4', 'D311D5$4', 'D311D6$4', 'D311D7$4', 'D311D$4', 'D311B$5', 'D311D2$5', 'D311D3$5', 'D311D4$5', 'D311D5$5', 'D311D6$5', 'D311D7$5', 'D311D$5', 'D311B$6', 'D311D2$6', 'D311D3$6', 'D311D4$6', 'D311D5$6', 'D311D6$6', 'D311D7$6', 'D311D$6', 'D311B$7', 'D311D2$7', 'D311D3$7', 'D311D4$7', 'D311D5$7', 'D311D6$7', 'D311D7$7', 'D311D$7', 'D3121B', 'D3121C2', 'D3121C3', 'D3121C4', 'D3121C5', 'D3121C6', 'D3121C', 'D3122B', 'D3122C2', 'D3122C3', 'D3122C4', 'D3122C5', 'D3122C6', 'D3122C', 'D315A', 'D315B2', 'D315B3', 'D315B4', 'D315B5', 'D315B6', 'D315B', 'I311B$1', 'I311B$2', 'I311B$4', 'I311B$6', 'I311B$3', 'I311B$5', 'I311B$7', 'I311D2$1', 'I311D3$1', 'I311D4$1', 'I311D5$1', 'I311D6$1', 'I311D7$1', 'I311D2$2', 'I311D3$2', 'I311D4$2', 'I311D5$2', 'I311D6$2', 'I311D7$2', 'I311D2$4', 'I311D3$4', 'I311D4$4', 'I311D5$4', 'I311D6$4', 'I311D7$4', 'I311D2$6', 'I311D3$6', 'I311D4$6', 'I311D5$6', 'I311D6$6', 'I311D7$6', 'I311D2$3', 'I311D3$3', 'I311D4$3', 'I311D5$3', 'I311D6$3', 'I311D7$3', 'I311D2$5', 'I311D3$5', 'I311D4$5', 'I311D5$5', 'I311D6$5', 'I311D7$5', 'I311D2$7', 'I311D3$7', 'I311D4$7', 'I311D5$7', 'I311D6$7', 'I311D7$7', 'I3121B', 'I3122B', 'I3121C2', 'I3121C3', 'I3121C4', 'I3121C5', 'I3121C6', 'I3122C2', 'I3122C3', 'I3122C4', 'I3122C5', 'I3122C6', 'I315A', 'I315B2', 'I315B3', 'I315B4', 'I315B5', 'I315B6', 'I311D$1', 'I311D$2', 'I311D$3', 'I311D$4', 'I311D$5', 'I311D$6', 'I311D$7', 'I3121C', 'I3122C', 'I315B', 'FACTOR07', 'FACTORA07', 'NCONGLOME', 'SUB_CONGLOME']\n",
      "\n",
      "Tipos de datos del DataFrame:\n",
      "AŅO               int64\n",
      "MES               int64\n",
      "CONGLOME          int64\n",
      "VIVIENDA          int64\n",
      "HOGAR             int64\n",
      "                 ...   \n",
      "I315B            object\n",
      "FACTOR07        float64\n",
      "FACTORA07       float64\n",
      "NCONGLOME         int64\n",
      "SUB_CONGLOME      int64\n",
      "Length: 511, dtype: object\n",
      "\n",
      "Subconjunto seleccionado con 8 variables:\n",
      "   CONGLOME  VIVIENDA  HOGAR  CODPERSO  P300A  P301A P302A P304A\n",
      "0      5030         2     11         1      4      8            \n",
      "1      5030         2     11         2      4     10            \n",
      "2      5030         2     11         3      4      3           2\n",
      "3      5030         2     11         4      8      3           2\n",
      "4      5030        11     11         1      4      4            \n",
      "\n",
      "Dimensiones del subconjunto: (108354, 8)\n",
      "\n",
      "Tipos de datos del subconjunto:\n",
      "CONGLOME     int64\n",
      "VIVIENDA     int64\n",
      "HOGAR        int64\n",
      "CODPERSO     int64\n",
      "P300A        int64\n",
      "P301A        int64\n",
      "P302A       object\n",
      "P304A       object\n",
      "dtype: object\n",
      "\n",
      "Subconjunto con nombres de variables renombrados:\n",
      "   CONGLOME  VIVIENDA  HOGAR  CODPERSO  lengua_materna  nivel_educativo  \\\n",
      "0      5030         2     11         1               4                8   \n",
      "1      5030         2     11         2               4               10   \n",
      "2      5030         2     11         3               4                3   \n",
      "3      5030         2     11         4               8                3   \n",
      "4      5030        11     11         1               4                4   \n",
      "\n",
      "  programa_alfabetizacion nivel_asistencia_actual  \n",
      "0                                                  \n",
      "1                                                  \n",
      "2                                               2  \n",
      "3                                               2  \n",
      "4                                                  \n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    # a. Leer y mostrar las primeras 5 filas\n",
    "    print(\"\\nPrimeras 5 filas del dataset:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # b. Convertir los nombres de columna a una lista e imprimirla\n",
    "    column_names = df.columns.tolist()\n",
    "    print(\"\\nNombres de columnas como lista:\")\n",
    "    print(column_names)\n",
    "    \n",
    "    # c. Verificar los tipos de datos del DataFrame\n",
    "    print(\"\\nTipos de datos del DataFrame:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # d. Seleccionar un subconjunto con las variables requeridas\n",
    "    variables_base = ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO']\n",
    "    \n",
    "    # Seleccionar 4 variables adicionales de interés educativo\n",
    "    variables_adicionales = [\n",
    "        'P300A',  # Idioma o lengua materna\n",
    "        'P301A',  # Último año o grado de estudios y nivel que aprobó - Nivel\n",
    "        'P302A',  # En los últimos 12 meses, ¿Recibió programa de alfabetización?\n",
    "        'P304A'   # ¿Cuál es el grado o año de estudios al que asistió el año pasado? - Nivel\n",
    "    ]\n",
    "    \n",
    "    # Verificar que las variables existan en el dataset\n",
    "    variables_existentes = [col for col in variables_base + variables_adicionales if col in df.columns]\n",
    "    \n",
    "    # Crear el subconjunto\n",
    "    subsample = df[variables_existentes]\n",
    "    \n",
    "    print(f\"\\nSubconjunto seleccionado con {len(variables_existentes)} variables:\")\n",
    "    print(subsample.head())\n",
    "    \n",
    "    # Mostrar información sobre el subconjunto\n",
    "    print(f\"\\nDimensiones del subconjunto: {subsample.shape}\")\n",
    "    print(\"\\nTipos de datos del subconjunto:\")\n",
    "    print(subsample.dtypes)\n",
    "     # 7. Renombrar las variables adicionales para mayor claridad\n",
    "    nuevos_nombres = {\n",
    "        'P300A': 'lengua_materna',\n",
    "        'P301A': 'nivel_educativo',\n",
    "        'P302A': 'programa_alfabetizacion',\n",
    "        'P304A': 'nivel_asistencia_actual'\n",
    "    }\n",
    "    \n",
    "    subsample = subsample.rename(columns=nuevos_nombres)\n",
    "    \n",
    "    print(\"\\nSubconjunto con nombres de variables renombrados:\")\n",
    "    print(subsample.head())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe9951",
   "metadata": {},
   "source": [
    "2. Data Manipulation (Data Cleaning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c670aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPLORACIÓN INICIAL DEL DATAFRAME ===\n",
      "\n",
      "1. INFORMACIÓN GENERAL DEL DATAFRAME:\n",
      "Dimensiones: (108354, 511) (filas, columnas)\n",
      "Número total de datos: 55368894\n",
      "\n",
      "2. RESUMEN ESTADÍSTICO DE VARIABLES NUMÉRICAS:\n",
      "            AŅO            MES       CONGLOME       VIVIENDA          HOGAR  \\\n",
      "count  108354.0  108354.000000  108354.000000  108354.000000  108354.000000   \n",
      "mean     2023.0       6.495127   16944.756797      77.820330      11.146271   \n",
      "std         0.0       3.445244    3144.386733      68.547022       1.370084   \n",
      "min      2023.0       1.000000    5007.000000       1.000000      11.000000   \n",
      "25%      2023.0       3.000000   16028.000000      31.000000      11.000000   \n",
      "50%      2023.0       7.000000   17500.000000      66.000000      11.000000   \n",
      "75%      2023.0       9.000000   19014.000000     106.000000      11.000000   \n",
      "max      2023.0      12.000000   21001.000000     991.000000      44.000000   \n",
      "\n",
      "            CODPERSO         UBIGEO        DOMINIO        ESTRATO  \\\n",
      "count  108354.000000  108354.000000  108354.000000  108354.000000   \n",
      "mean        2.546293  131185.171087       4.864961       4.138961   \n",
      "std         1.577737   67736.903764       2.404443       2.432348   \n",
      "min         1.000000   10101.000000       1.000000       1.000000   \n",
      "25%         1.000000   80101.000000       2.000000       2.000000   \n",
      "50%         2.000000  140108.000000       5.000000       4.000000   \n",
      "75%         3.000000  180301.000000       7.000000       7.000000   \n",
      "max        22.000000  250401.000000       8.000000       8.000000   \n",
      "\n",
      "            CODINFOR  ...          P301A           P203           P204  \\\n",
      "count  108354.000000  ...  108354.000000  108354.000000  108354.000000   \n",
      "mean        2.033058  ...       5.258652       2.548729       1.005787   \n",
      "std         1.282622  ...       3.807669       1.716218       0.075850   \n",
      "min         0.000000  ...       1.000000       1.000000       1.000000   \n",
      "25%         1.000000  ...       3.000000       1.000000       1.000000   \n",
      "50%         2.000000  ...       5.000000       3.000000       1.000000   \n",
      "75%         2.000000  ...       6.000000       3.000000       1.000000   \n",
      "max        21.000000  ...      99.000000      11.000000       2.000000   \n",
      "\n",
      "                P207          P208A     TICUEST01A       FACTOR07  \\\n",
      "count  108354.000000  108354.000000  108354.000000  108354.000000   \n",
      "mean        1.513428      35.619839       1.999539     305.556281   \n",
      "std         0.499822      22.152408       0.021477     310.795722   \n",
      "min         1.000000       3.000000       1.000000       1.087094   \n",
      "25%         1.000000      16.000000       2.000000     123.659256   \n",
      "50%         2.000000      33.000000       2.000000     233.634140   \n",
      "75%         2.000000      53.000000       2.000000     354.661774   \n",
      "max         2.000000      98.000000       2.000000    1961.005981   \n",
      "\n",
      "           FACTORA07      NCONGLOME   SUB_CONGLOME  \n",
      "count  108354.000000  108354.000000  108354.000000  \n",
      "mean      302.602025   23603.093914       0.282325  \n",
      "std       322.927385   15607.753763       0.662107  \n",
      "min         0.422172       2.000000       0.000000  \n",
      "25%       101.125488    8855.000000       0.000000  \n",
      "50%       218.866760   20186.000000       0.000000  \n",
      "75%       365.609833   37361.000000       0.000000  \n",
      "max      2917.495850   51561.000000       6.000000  \n",
      "\n",
      "[8 rows x 23 columns]\n",
      "\n",
      "3. INFORMACIÓN SOBRE TIPOS DE DATOS Y VALORES NO NULOS:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108354 entries, 0 to 108353\n",
      "Columns: 511 entries, AŅO to SUB_CONGLOME\n",
      "dtypes: float64(2), int64(21), object(488)\n",
      "memory usage: 422.4+ MB\n",
      "None\n",
      "\n",
      "=== IDENTIFICACIÓN DE VALORES MISSING ===\n",
      "\n",
      "Valores missing por columna:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Porcentaje de valores missing por columna:\n",
      "Series([], dtype: float64)\n",
      "\n",
      "=== MANEJO DE VALORES MISSING ===\n",
      "\n",
      "Valores missing en variables clave:\n",
      "CONGLOME    0\n",
      "VIVIENDA    0\n",
      "HOGAR       0\n",
      "CODPERSO    0\n",
      "P300A       0\n",
      "P301A       0\n",
      "P302A       0\n",
      "P304A       0\n",
      "dtype: int64\n",
      "\n",
      "Filas eliminadas por valores missing en variables de identificación: 0\n",
      "Filas restantes: 108354\n",
      "\n",
      "=== VALORES CODIFICADOS COMO MISSING ===\n",
      "\n",
      "Valores codificados como missing en P300A: 105\n",
      "Valores codificados como missing en P301A: 105\n",
      "Valores codificados como missing en P302A: 0\n",
      "Valores codificados como missing en P304A: 0\n",
      "\n",
      "=== ESTADO DESPUÉS DE LA LIMPIEZA ===\n",
      "\n",
      "Valores missing después de la limpieza:\n",
      "P300A    105\n",
      "P301A    105\n",
      "dtype: int64\n",
      "Imputados 0 valores missing en P300A con la mediana: 4.0\n",
      "Imputados 0 valores missing en P301A con la mediana: 5.0\n",
      "\n",
      "=== VERIFICACIÓN FINAL ===\n",
      "\n",
      "Valores missing restantes en todo el DataFrame: 0\n",
      "\n",
      "Dimensiones del dataset después de la limpieza: (108354, 511)\n",
      "\n",
      "Dataset limpio guardado como 'enaho_educacion_clean.csv'\n",
      "\n",
      "Información del dataset limpio:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108354 entries, 0 to 108353\n",
      "Columns: 511 entries, AŅO to SUB_CONGLOME\n",
      "dtypes: float64(4), int64(19), object(488)\n",
      "memory usage: 422.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    # 1. Explorar el DataFrame usando funciones de resumen\n",
    "    print(\"\\n=== EXPLORACIÓN INICIAL DEL DATAFRAME ===\\n\")\n",
    "    \n",
    "    # Información general del DataFrame\n",
    "    print(\"1. INFORMACIÓN GENERAL DEL DATAFRAME:\")\n",
    "    print(f\"Dimensiones: {df.shape} (filas, columnas)\")\n",
    "    print(f\"Número total de datos: {df.size}\")\n",
    "    \n",
    "    # Resumen estadístico de las variables numéricas\n",
    "    print(\"\\n2. RESUMEN ESTADÍSTICO DE VARIABLES NUMÉRICAS:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Información sobre tipos de datos y valores no nulos\n",
    "    print(\"\\n3. INFORMACIÓN SOBRE TIPOS DE DATOS Y VALORES NO NULOS:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # 2. Identificar valores missing (faltantes)\n",
    "    print(\"\\n=== IDENTIFICACIÓN DE VALORES MISSING ===\\n\")\n",
    "    \n",
    "    # Contar valores missing por columna\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"Valores missing por columna:\")\n",
    "    print(missing_values[missing_values > 0])  # Mostrar solo columnas con valores missing\n",
    "    \n",
    "    # Porcentaje de valores missing por columna\n",
    "    missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "    print(\"\\nPorcentaje de valores missing por columna:\")\n",
    "    print(missing_percentage[missing_percentage > 0])  # Mostrar solo columnas con valores missing\n",
    "    \n",
    "    # 3. Manejo de valores missing\n",
    "    print(\"\\n=== MANEJO DE VALORES MISSING ===\\n\")\n",
    "    \n",
    "    # Estrategia para manejar valores missing:\n",
    "    # - Para variables categóricas: Podemos usar una categoría específica para missing values\n",
    "    # - Para variables numéricas: Podemos imputar con la media, mediana o eliminar\n",
    "    \n",
    "    # Primero, verificar si hay valores missing en las variables clave\n",
    "    key_variables = ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO', 'P300A', 'P301A', 'P302A', 'P304A']\n",
    "    key_missing = df[key_variables].isnull().sum()\n",
    "    \n",
    "    print(\"Valores missing en variables clave:\")\n",
    "    print(key_missing)\n",
    "    \n",
    "    # Eliminar filas con valores missing en variables clave (si es necesario)\n",
    "    # Nota: En datasets de encuestas, a veces es mejor no eliminar filas ya que cada una representa una persona/hogar\n",
    "    \n",
    "    # Para este ejemplo, eliminaremos filas donde todas las variables clave tienen valores missing\n",
    "    # Pero primero, verifiquemos cuántas filas serían eliminadas\n",
    "    rows_before = len(df)\n",
    "    \n",
    "    # Eliminar filas donde las variables de identificación tienen valores missing\n",
    "    # (Estas no deberían tener valores missing en un dataset bien construido)\n",
    "    df_clean = df.dropna(subset=['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO'], how='any')\n",
    "    \n",
    "    rows_after = len(df_clean)\n",
    "    rows_removed = rows_before - rows_after\n",
    "    \n",
    "    print(f\"\\nFilas eliminadas por valores missing en variables de identificación: {rows_removed}\")\n",
    "    print(f\"Filas restantes: {rows_after}\")\n",
    "    \n",
    "    # Para variables categóricas con valores missing, podemos asignar una categoría específica\n",
    "    # Basado en el diccionario de datos, sabemos que los valores missing están codificados como 99 o 9\n",
    "    \n",
    "    # 4. Verificar valores específicos codificados como missing (según el diccionario de datos)\n",
    "    print(\"\\n=== VALORES CODIFICADOS COMO MISSING ===\\n\")\n",
    "    \n",
    "    # Definir los valores que representan missing según el diccionario\n",
    "    missing_codes = {\n",
    "        'P300A': [99],  # Lengua materna\n",
    "        'P301A': [99],  # Nivel educativo\n",
    "        'P302A': [9],   # Programa de alfabetización\n",
    "        'P304A': [9]    # Nivel de asistencia actual\n",
    "    }\n",
    "    \n",
    "    # Contar valores codificados como missing\n",
    "    for column, codes in missing_codes.items():\n",
    "        if column in df_clean.columns:\n",
    "            count = df_clean[column].isin(codes).sum()\n",
    "            print(f\"Valores codificados como missing en {column}: {count}\")\n",
    "            \n",
    "            # Reemplazar valores codificados con NaN\n",
    "            df_clean[column] = df_clean[column].replace(codes, float('nan'))\n",
    "    \n",
    "    # 5. Verificar el estado después de la limpieza\n",
    "    print(\"\\n=== ESTADO DESPUÉS DE LA LIMPIEZA ===\\n\")\n",
    "    \n",
    "    # Valores missing después de la limpieza\n",
    "    missing_after = df_clean.isnull().sum()\n",
    "    print(\"Valores missing después de la limpieza:\")\n",
    "    print(missing_after[missing_after > 0])\n",
    "    \n",
    "    # Para variables numéricas, podemos imputar valores missing con la mediana\n",
    "    numeric_columns = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            median_value = df_clean[col].median()\n",
    "            df_clean[col].fillna(median_value, inplace=True)\n",
    "            print(f\"Imputados {df_clean[col].isnull().sum()} valores missing en {col} con la mediana: {median_value}\")\n",
    "    \n",
    "    # Para variables categóricas, podemos imputar con la moda o asignar una categoría \"Desconocido\"\n",
    "    categorical_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            mode_value = df_clean[col].mode()[0]\n",
    "            df_clean[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"Imputados {df_clean[col].isnull().sum()} valores missing en {col} con la moda: {mode_value}\")\n",
    "    \n",
    "    # 6. Verificación final\n",
    "    print(\"\\n=== VERIFICACIÓN FINAL ===\\n\")\n",
    "    \n",
    "    # Comprobar si aún hay valores missing\n",
    "    remaining_missing = df_clean.isnull().sum().sum()\n",
    "    print(f\"Valores missing restantes en todo el DataFrame: {remaining_missing}\")\n",
    "    \n",
    "    # Resumen del dataset limpio\n",
    "    print(f\"\\nDimensiones del dataset después de la limpieza: {df_clean.shape}\")\n",
    "    \n",
    "    # Guardar el dataset limpio (opcional)\n",
    "    df_clean.to_csv(\"enaho_educacion_clean.csv\", index=False, encoding='utf-8')\n",
    "    print(\"\\nDataset limpio guardado como 'enaho_educacion_clean.csv'\")\n",
    "    \n",
    "    # Mostrar información del dataset limpio\n",
    "    print(\"\\nInformación del dataset limpio:\")\n",
    "    print(df_clean.info())\n",
    "    \n",
    "else:\n",
    "    print(\"No se pudo realizar la limpieza de datos debido a problemas con la importación del dataset.\")\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545db414",
   "metadata": {},
   "source": [
    " 3. Importación y Manipulación del Segundo Dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1566b38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de vivienda importado con encoding ISO-8859-10\n",
      "\n",
      "Primeras 5 filas del dataset de vivienda:\n",
      "    AŅO  MES  CONGLOME  VIVIENDA  HOGAR  CODPERSO  UBIGEO  DOMINIO  ESTRATO  \\\n",
      "0  2023    2      5007        22     11         1   10101        4        4   \n",
      "1  2023    2      5007        22     11         2   10101        4        4   \n",
      "2  2023    2      5007        22     11         3   10101        4        4   \n",
      "3  2023    2      5007        31     11         1   10101        4        4   \n",
      "4  2023    2      5007        31     11         2   10101        4        4   \n",
      "\n",
      "               P201P  ...  OCUPAC_R3 OCUPAC_R4 RAMA_R3 RAMA_R4 CODTAREA  \\\n",
      "0  20190050070221101  ...                                                 \n",
      "1  20190050070221102  ...                                                 \n",
      "2  20190050070221104  ...                                                 \n",
      "3  20190050070311102  ...                                                 \n",
      "4  20230050070311102  ...                                                 \n",
      "\n",
      "  CODTIEMPO TICUEST01   FACPOB07 NCONGLOME SUB_CONGLOME  \n",
      "0                   2  50.466671      7070            0  \n",
      "1                   2  50.466671      7070            0  \n",
      "2                   2  50.466671      7070            0  \n",
      "3                   2  50.466671      7070            0  \n",
      "4                   2  50.466671      7070            0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "\n",
      "Nombres de columnas del dataset de vivienda:\n",
      "['AŅO', 'MES', 'CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO', 'UBIGEO', 'DOMINIO', 'ESTRATO', 'P201P', 'P203', 'P203A', 'P203B', 'P204', 'P205', 'P206', 'P207', 'P208A', 'P208B', 'P209', 'P210', 'P211A', 'P211D', 'P212', 'P213', 'P214', 'P215', 'P216', 'P217', 'T211', 'OCUPAC_R3', 'OCUPAC_R4', 'RAMA_R3', 'RAMA_R4', 'CODTAREA', 'CODTIEMPO', 'TICUEST01', 'FACPOB07', 'NCONGLOME', 'SUB_CONGLOME']\n",
      "\n",
      "Tipos de datos del dataset de vivienda:\n",
      "AŅO               int64\n",
      "MES               int64\n",
      "CONGLOME          int64\n",
      "VIVIENDA          int64\n",
      "HOGAR             int64\n",
      "CODPERSO          int64\n",
      "UBIGEO            int64\n",
      "DOMINIO           int64\n",
      "ESTRATO           int64\n",
      "P201P             int64\n",
      "P203              int64\n",
      "P203A            object\n",
      "P203B            object\n",
      "P204             object\n",
      "P205             object\n",
      "P206             object\n",
      "P207             object\n",
      "P208A            object\n",
      "P208B            object\n",
      "P209             object\n",
      "P210             object\n",
      "P211A            object\n",
      "P211D            object\n",
      "P212             object\n",
      "P213             object\n",
      "P214             object\n",
      "P215             object\n",
      "P216             object\n",
      "P217             object\n",
      "T211             object\n",
      "OCUPAC_R3        object\n",
      "OCUPAC_R4        object\n",
      "RAMA_R3          object\n",
      "RAMA_R4          object\n",
      "CODTAREA         object\n",
      "CODTIEMPO        object\n",
      "TICUEST01         int64\n",
      "FACPOB07        float64\n",
      "NCONGLOME         int64\n",
      "SUB_CONGLOME      int64\n",
      "dtype: object\n",
      "\n",
      "Subconjunto seleccionado con 9 variables:\n",
      "   CONGLOME  VIVIENDA  HOGAR  CODPERSO  P203 P204 P205 P206 P207\n",
      "0      5007        22     11         1     1    1    2         1\n",
      "1      5007        22     11         2     2    1    2         2\n",
      "2      5007        22     11         3     3    1    2         1\n",
      "3      5007        31     11         1     1    1    2         2\n",
      "4      5007        31     11         2     3    2         2    1\n",
      "\n",
      "=== MODIFICACIONES AL SUBCONJUNTO ===\n",
      "\n",
      "Tipos de datos antes de las modificaciones:\n",
      "CONGLOME     int64\n",
      "VIVIENDA     int64\n",
      "HOGAR        int64\n",
      "CODPERSO     int64\n",
      "P203         int64\n",
      "P204        object\n",
      "P205        object\n",
      "P206        object\n",
      "P207        object\n",
      "dtype: object\n",
      "\n",
      "A. Cambiando tipo de dato de P205 de object a category\n",
      "Tipo de dato de P205 después del cambio: category\n",
      "\n",
      "B. Modificando valores en la columna P206 (Material de las paredes):\n",
      "Valores originales en P206:\n",
      "P206\n",
      "     115340\n",
      "2      3730\n",
      "1       677\n",
      "Name: count, dtype: int64\n",
      "Valores modificados en P206_modificado:\n",
      "P206_modificado\n",
      "Adobe o tapia                   3730\n",
      "Ladrillo o bloque de cemento     677\n",
      "Name: count, dtype: int64\n",
      "\n",
      "C. Convirtiendo P203 a numérico\n",
      "Tipo de dato de P203 después del cambio: int64\n",
      "\n",
      "D. Convirtiendo P204 a numérico\n",
      "Tipo de dato de P204 después del cambio: float64\n",
      "\n",
      "Subconjunto después de las modificaciones:\n",
      "   CONGLOME  VIVIENDA  HOGAR  CODPERSO  P203  P204 P205 P206 P207  \\\n",
      "0      5007        22     11         1     1   1.0    2         1   \n",
      "1      5007        22     11         2     2   1.0    2         2   \n",
      "2      5007        22     11         3     3   1.0    2         1   \n",
      "3      5007        31     11         1     1   1.0    2         2   \n",
      "4      5007        31     11         2     3   2.0         2    1   \n",
      "\n",
      "  P206_modificado  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4   Adobe o tapia  \n",
      "\n",
      "Tipos de datos después de las modificaciones:\n",
      "CONGLOME              int64\n",
      "VIVIENDA              int64\n",
      "HOGAR                 int64\n",
      "CODPERSO              int64\n",
      "P203                  int64\n",
      "P204                float64\n",
      "P205               category\n",
      "P206                 object\n",
      "P207                 object\n",
      "P206_modificado      object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_13000\\4231310518.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subsample_vivienda['P205'] = subsample_vivienda['P205'].astype('category')\n",
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_13000\\4231310518.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subsample_vivienda['P206_modificado'] = subsample_vivienda['P206'].map(mapeo_p206)\n",
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_13000\\4231310518.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subsample_vivienda['P203'] = pd.to_numeric(subsample_vivienda['P203'], errors='coerce')\n",
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_13000\\4231310518.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subsample_vivienda['P204'] = pd.to_numeric(subsample_vivienda['P204'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subconjunto modificado guardado como 'subsample_vivienda_modificado.csv'\n"
     ]
    }
   ],
   "source": [
    "# Importar el segundo dataset (módulo de vivienda) con el nombre correcto\n",
    "try:\n",
    "    df_vivienda = pd.read_csv(\"Enaho01-2023-200.csv\", encoding=\"ISO-8859-10\", low_memory=False)\n",
    "    print(\"Dataset de vivienda importado con encoding ISO-8859-10\")\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        df_vivienda = pd.read_csv(\"Enaho01-2023-200.csv\", encoding=\"UTF-8\", low_memory=False)\n",
    "        print(\"Dataset de vivienda importado con encoding UTF-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: No se pudo importar el dataset de vivienda: {e}\")\n",
    "        df_vivienda = None\n",
    "\n",
    "# Si la importación fue exitosa, realizar las operaciones solicitadas\n",
    "if df_vivienda is not None:\n",
    "    # 1. Display the first 5 rows\n",
    "    print(\"\\nPrimeras 5 filas del dataset de vivienda:\")\n",
    "    print(df_vivienda.head())\n",
    "    \n",
    "    # 2. Convert the column names into a list and print it\n",
    "    column_names_vivienda = df_vivienda.columns.tolist()\n",
    "    print(\"\\nNombres de columnas del dataset de vivienda:\")\n",
    "    print(column_names_vivienda)\n",
    "    \n",
    "    # 3. Check the data types\n",
    "    print(\"\\nTipos de datos del dataset de vivienda:\")\n",
    "    print(df_vivienda.dtypes)\n",
    "    \n",
    "    # 4. Select a subsample with the required variables\n",
    "    variables_base = ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO']\n",
    "    \n",
    "    # Seleccionar variables adicionales que SÍ existen en el dataset\n",
    "    # Basado en las columnas disponibles que vimos en el output\n",
    "    variables_adicionales_vivienda = [\n",
    "        'P203',   # ¿Cuántos cuartos o habitaciones tiene su hogar? (excluye baños y cocina)\n",
    "        'P204',   # ¿En cuántos de estos cuartos duermen las personas del hogar?\n",
    "        'P205',   # ¿La vivienda que ocupa el hogar es?\n",
    "        'P206',   # Material de las paredes\n",
    "        'P207'    # Material de los pisos\n",
    "    ]\n",
    "    \n",
    "    # Verificar que las variables existan en el dataset\n",
    "    variables_existentes_vivienda = [col for col in variables_base + variables_adicionales_vivienda if col in df_vivienda.columns]\n",
    "    \n",
    "    # Crear el subconjunto\n",
    "    subsample_vivienda = df_vivienda[variables_existentes_vivienda]\n",
    "    \n",
    "    print(f\"\\nSubconjunto seleccionado con {len(variables_existentes_vivienda)} variables:\")\n",
    "    print(subsample_vivienda.head())\n",
    "    \n",
    "    # 5. Perform the following modifications:\n",
    "    print(\"\\n=== MODIFICACIONES AL SUBCONJUNTO ===\\n\")\n",
    "    \n",
    "    # A. Change the data type of a variable\n",
    "    print(\"Tipos de datos antes de las modificaciones:\")\n",
    "    print(subsample_vivienda.dtypes)\n",
    "    \n",
    "    # Convertir P205 (tipo de vivienda) a categórica\n",
    "    if 'P205' in subsample_vivienda.columns:\n",
    "        print(f\"\\nA. Cambiando tipo de dato de P205 de {subsample_vivienda['P205'].dtype} a category\")\n",
    "        subsample_vivienda['P205'] = subsample_vivienda['P205'].astype('category')\n",
    "        print(f\"Tipo de dato de P205 después del cambio: {subsample_vivienda['P205'].dtype}\")\n",
    "    \n",
    "    # B. Modify some values in a specific column\n",
    "    # Modificar valores en la columna P206 (Material de las paredes)\n",
    "    if 'P206' in subsample_vivienda.columns:\n",
    "        print(\"\\nB. Modificando valores en la columna P206 (Material de las paredes):\")\n",
    "        print(\"Valores originales en P206:\")\n",
    "        print(subsample_vivienda['P206'].value_counts())\n",
    "        \n",
    "        # Crear un mapeo de valores (basado en el diccionario de la ENAHO)\n",
    "        # Nota: Estos valores deben verificarse con el diccionario oficial\n",
    "        mapeo_p206 = {\n",
    "            \"1\": \"Ladrillo o bloque de cemento\",\n",
    "            \"2\": \"Adobe o tapia\",\n",
    "            \"3\": \"Quincha\",\n",
    "            \"4\": \"Piedra con barro\",\n",
    "            \"5\": \"Piedra con cal o cemento\",\n",
    "            \"6\": \"Madera\",\n",
    "            \"7\": \"Estera\",\n",
    "            \"8\": \"Otro material\"\n",
    "        }\n",
    "        \n",
    "        # Aplicar el mapeo\n",
    "        subsample_vivienda['P206_modificado'] = subsample_vivienda['P206'].map(mapeo_p206)\n",
    "        \n",
    "        print(\"Valores modificados en P206_modificado:\")\n",
    "        print(subsample_vivienda['P206_modificado'].value_counts())\n",
    "    \n",
    "    # C. Additional modification: Convertir P203 y P204 a numéricas\n",
    "    if 'P203' in subsample_vivienda.columns:\n",
    "        print(f\"\\nC. Convirtiendo P203 a numérico\")\n",
    "        # Algunos valores podrían ser strings, así que los convertimos a numéricos, forzando los no numéricos a NaN\n",
    "        subsample_vivienda['P203'] = pd.to_numeric(subsample_vivienda['P203'], errors='coerce')\n",
    "        print(f\"Tipo de dato de P203 después del cambio: {subsample_vivienda['P203'].dtype}\")\n",
    "    \n",
    "    if 'P204' in subsample_vivienda.columns:\n",
    "        print(f\"\\nD. Convirtiendo P204 a numérico\")\n",
    "        subsample_vivienda['P204'] = pd.to_numeric(subsample_vivienda['P204'], errors='coerce')\n",
    "        print(f\"Tipo de dato de P204 después del cambio: {subsample_vivienda['P204'].dtype}\")\n",
    "    \n",
    "    # Mostrar el resultado final\n",
    "    print(\"\\nSubconjunto después de las modificaciones:\")\n",
    "    print(subsample_vivienda.head())\n",
    "    print(\"\\nTipos de datos después de las modificaciones:\")\n",
    "    print(subsample_vivienda.dtypes)\n",
    "    \n",
    "    # Guardar el subconjunto modificado\n",
    "    subsample_vivienda.to_csv(\"subsample_vivienda_modificado.csv\", index=False, encoding='utf-8')\n",
    "    print(\"\\nSubconjunto modificado guardado como 'subsample_vivienda_modificado.csv'\")\n",
    "    \n",
    "else:\n",
    "    print(\"No se pudo realizar el análisis del dataset de vivienda debido a problemas con la importación.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da862cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "a3b6c34f-b027-4fcb-bdb0-1cfa33f3c4bd",
   "metadata": {},
   "source": [
    "8. Logical Operators [2 pts]\n",
    "Write a function that determines if a person is eligible for a scholarship based on these conditions:\n",
    "The person must have a GPA greater than 3.5 AND\n",
    "Either their extracurricular activities are \"Yes\" OR they have community service hours greater than 50.\n",
    "The function should return:\n",
    "\n",
    "\"Eligible for scholarship.\" if conditions are met.\n",
    "\"Not eligible for scholarship.\" otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ce9dbc8-b4fb-47c5-8893-f252fbe10bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se debe crear una función con las condiciones mencionadas:\n",
    "def becado(gpa, extracurricular, horasservicio):\n",
    "    if gpa > 3.5 and (extracurricular == \"Si\" or horasservicio > 50):\n",
    "        return \"Eligible for scholarship.\"\n",
    "    else:\n",
    "        return \"Not eligible for scholarship.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57b21788-321a-4a29-bae7-360ccc0ab4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eligible for scholarship.\n"
     ]
    }
   ],
   "source": [
    "# Ahora se prueba la función. \n",
    "\n",
    "print(becado(3.8, \"Si\", 20))   \n",
    "\n",
    "# en este caso es elegible debido a que cumple GPA y actividades. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37b3d606-06b9-4a68-8b5c-2f269254e58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not eligible for scholarship.\n"
     ]
    }
   ],
   "source": [
    "print(becado(2.2, \"No\", 20))   \n",
    "\n",
    "# en este caso no es elegible debido a que no cumple con GPA, extracurricular y horas de servicio."
   ]
  },
  {
   "cell_type": "raw",
   "id": "28132f05-1f8d-4793-bb80-4778bd267e90",
   "metadata": {},
   "source": [
    "9. Python Identity Operators\n",
    "Create two lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34450bdb-d6d2-4b41-b13d-6354a8deba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1, 2, 3]\n",
    "list2 = [1, 2, 3]\n",
    "list3 = list1 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d638fc2-0ae5-46c8-9105-b9b859e71813",
   "metadata": {},
   "source": [
    "Check with identity operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f331f5e7-9ac4-4985-b7c6-56a535bce822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 is list2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d85cf6eb-95e9-4b72-8d10-6d115739477a",
   "metadata": {},
   "source": [
    "La respuesta es un bool y es Falso. A pesar de tener los mismos elementos, Python los almacena como dos objetos diferentes debido a que tienen nombres diferentes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a899c9f-7bb9-40c6-bf70-0710e0453c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 is list3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac5250ff-ab67-4575-950e-ff22aee39ab4",
   "metadata": {},
   "source": [
    "La respuesta es un bool y es True. En este caso, se tiene que observar el orden. En primer lugar se ha creado el objeto list1 y luego se ha creado el objeto list3 basado en el list1. Es por esto que python los identifica como una misma lista, y ambos referencian al mismo objeto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c45bf1d-beca-4e7b-bbb7-7441ccb07356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 == list2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e629ecd-c53e-4b18-a9ee-532826bbe9e1",
   "metadata": {},
   "source": [
    "La respuesta es un bool y es True. Como el operador es el simbolo de igual (==), python compara los elementos de ambos objetos y concluye que ambos son iguales. Esto podría entender que \"is\" es para identificar objetos en la memoria y \"==\" es para comparar los elementos de los objetos. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "85535791-5856-4c16-ac4f-9d3942311178",
   "metadata": {},
   "source": [
    "10. Nested If Statement [2 pts]\n",
    "\n",
    "Write a function that takes a student's score and determines the grade:\n",
    "\n",
    "If the score is greater than or equal to 90, return \"A\".\n",
    "If the score is between 80 and 89:\n",
    "Check if the score is exactly 85, then return \"B+\".\n",
    "Otherwise, return \"B\".\n",
    "If the score is between 70 and 79, return \"C\".\n",
    "Otherwise, return \"Fail\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9c8c772-bd92-4688-815f-c3551a5903d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para resolver este ejercicio se necesita utilizar la función if y elif\n",
    "# como piden la creación de función se utiliza def\n",
    "\n",
    "def calificacion(nota):\n",
    "    if nota >= 90:\n",
    "        return \"A\"\n",
    "    elif 80 <= nota <= 89:\n",
    "        if nota == 85: # función anidada (nested)\n",
    "            return \"B+\"\n",
    "        else:\n",
    "            return \"B\"\n",
    "    elif 70 <= nota <= 79:\n",
    "        return \"C\"\n",
    "    else:\n",
    "        return \"Fail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0a023f1-3db5-49ad-aa57-a79187c928e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "# la función está creada. Ahora se le debe someter a prueba\n",
    "print(calificacion(92))\n",
    "# se observa que la función arroja \"A\", y es correcto acorde a la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f271a573-2cc5-4054-b937-ff6b8c0e7712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B+\n"
     ]
    }
   ],
   "source": [
    "print(calificacion(85))\n",
    "# se observa que la función arroja \"B+\". Esto es correcto acorde a la función creada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "567fbd2e-dd81-449d-8789-7e33aaa27b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n"
     ]
    }
   ],
   "source": [
    "print(calificacion(79))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55081f5-1fe6-4dd2-a328-b73c2eb0a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar al caso anterior, se arroja el resultado correcto acorde a la función. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22d34681-8d55-4a8b-8bb2-cfb29d4ecf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail\n"
     ]
    }
   ],
   "source": [
    "print(calificacion(55))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b77e1bc-42e8-4da8-a564-42321ae559bd",
   "metadata": {},
   "source": [
    "Se observa que al colocar un valor fuera del rango de la función, el resultado es correcto acorde a la función (Fail). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af97b8ec-db78-458b-b0b9-936bdf1fd3e2",
   "metadata": {},
   "source": [
    "# Part 3 – For loops (Luis 11 y 12, Valentina 13-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70cb6f0-3bc4-4d12-bb97-88a7ca76af16",
   "metadata": {},
   "source": [
    "## 11. For Loop in NumPy\n",
    "Write a for loop using NumPy to iterate through an array of numbers [10, 20, 30, 40, 50] and print each value multiplied by 2.\n",
    "\n",
    "Re-question: How would you modify the loop so that it stores the results in a new NumPy array instead of just printing them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c050c411-9c07-452d-a3a6-865bbe5eec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en primer lugar se debe crear el array de números\n",
    "import numpy as np\n",
    "arr = np.array([10, 20, 30, 40, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "150c4d14-0ac0-45e4-99e3-6cdc99e0aa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# ahora se debe utilizar for para que cada valor se multiplique por 2\n",
    "for number in arr:\n",
    "    print(number*2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5422fdd-f24c-425c-9eb3-bba27b0646cd",
   "metadata": {},
   "source": [
    "En este caso, al utilizar for, cada valor del objeto \"arr\" se multiplica por 2. \n",
    "Ahora, respecto a al repregunta: se requiere modificar el bucle para guardar en un nuevo resultado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3d54e0d-392a-414b-bbce-4e342c368f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20  40  60  80 100]\n"
     ]
    }
   ],
   "source": [
    "# se debe utilizar la función append\n",
    "arr = np.array([10, 20, 30, 40, 50])\n",
    "results = [] # esta lista servirá para guardar los números del nuevo objeto\n",
    "\n",
    "for number in arr:\n",
    "    results.append(number * 2)\n",
    "\n",
    "nuevo_arr = np.array(results)\n",
    "print(nuevo_arr)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "485c8e25-65ab-4eda-a842-d0beb41dcd9b",
   "metadata": {},
   "source": [
    "Con este código, ya se tiene el objeto que almacena los elementos del bucle. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72481fd-ef15-49b1-ae8e-c453ce5a8d5e",
   "metadata": {},
   "source": [
    "## 12. For Loop in List\n",
    "Create a list of words: [\"python\", \"loop\", \"list\", \"iteration\"].\n",
    "Write a for loop to print the length of each word.\n",
    "\n",
    "Re-question: How can you rewrite the same loop using a list comprehension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3264f82-30ee-415d-8f3f-0e4e67a6e94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4\n",
      "4\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# se debe crear el objeto palabras\n",
    "palabras = [\"python\", \"loop\", \"list\", \"iteration\"]\n",
    "\n",
    "for p in palabras:\n",
    "    print(len(p))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48d7e521-4ec3-48bc-87f6-9021a8c2f90e",
   "metadata": {},
   "source": [
    "Con este código, se puede observar la extensión de cada palabras.\n",
    "\n",
    "Para la repregunta se debe utilizar list comprehension en el mismo loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7733e982-6da1-4b18-b124-7b335ce5b941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 4, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "extension = [len(p) for p in palabras]\n",
    "print(extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86ad7a6-100f-41bd-9f85-36615a161c59",
   "metadata": {},
   "source": [
    "## 13. For Loop in Dictionary\n",
    "\n",
    "Given a dictionary of student scores:\n",
    "{\"Alice\": 85, \"Bob\": 92, \"Charlie\": 78, \"Diana\": 88}\n",
    "\n",
    "Write a for loop to print each student's name along with their score.\n",
    "\n",
    "Re-question: Modify the loop so that it only prints the names of students who scored above 80.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487796fb-aa30-4f9c-91f6-649c7ca6f93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: 85\n",
      "Bob: 92\n",
      "Charlie: 78\n",
      "Diana: 88\n"
     ]
    }
   ],
   "source": [
    "#imprimimos los puntajes\n",
    "scores = {\"Alice\": 85, \"Bob\": 92, \"Charlie\": 78, \"Diana\": 88}\n",
    "\n",
    "for student, score in scores.items():\n",
    "    print(f\"{student}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79db81e1-875b-4d80-881a-ed8e6702e55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: 85\n",
      "Bob: 92\n",
      "Diana: 88\n"
     ]
    }
   ],
   "source": [
    "#imprimimos únicamente estudiantes con notas mayores a 80\n",
    "scores = {\"Alice\": 85, \"Bob\": 92, \"Charlie\": 78, \"Diana\": 88}\n",
    "\n",
    "for student, score in scores.items():\n",
    "    if score > 80:\n",
    "        print(f\"{student}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f8dbd-8c0b-4f30-95f3-c99cf2fca5ba",
   "metadata": {},
   "source": [
    "## 14. For Loop using Range\n",
    "Write a for loop using range() to print all even numbers between 1 and 20.\n",
    "\n",
    "Re-question: How would you change the loop to also calculate the sum of these even numbers while iterating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf2d931-e994-40f0-8f72-2676012899da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for num in range(2, 21, 2):\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "371c2a54-5bf8-42c7-a01b-63b1def7e006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "Sumatoria de números even: 110\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for num in range(2, 21, 2):\n",
    "    print(num)\n",
    "    total += num\n",
    "\n",
    "print(f\"Sumatoria de números even: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf92a0-14cd-4b9a-8bfe-415aba71b91b",
   "metadata": {},
   "source": [
    "## 15. Iterations over Pandas (ENAHO dataset) [2 pts]\n",
    "Suppose you are analyzing the National Household Survey (ENAHO) dataset, specifically the file ENAHO01A-2023-400.\n",
    "The question of interest is P41601: “¿Cuánto fue el monto total por la compra o servicio?”.\n",
    "\n",
    "Write a for loop that iterates over the column P41601 and prints values greater than 5000.\n",
    "\n",
    "Re-question: How would you optimize this task using pandas vectorized operations (e.g., boolean indexing) instead of a for loop, to make the analysis faster and more efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afaa4a90-a90c-4633-a1f0-b37c90eef4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Dataset cargado! Procesando columna P41601...\n",
      "\n",
      "Valores > 5000 usando FOR LOOP:\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "\n",
      "Valores > 5000 usando Pandas (vectorizado):\n",
      "784      99999.9\n",
      "794      99999.9\n",
      "797      99999.9\n",
      "799      99999.9\n",
      "3925     99999.9\n",
      "5731     99999.9\n",
      "7555     99999.9\n",
      "8007     99999.9\n",
      "13958    99999.9\n",
      "17211    99999.9\n",
      "Name: P41601, dtype: float64\n",
      "\n",
      "Total de valores > 5000: 38\n",
      "Valor máximo: 99999.9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "relative_path = \"Downloads/Enaho01A-2023-400.csv\"\n",
    "file_path = os.path.abspath(os.path.expanduser(f\"~/{relative_path}\"))\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"Error: No se encuentra el archivo: {file_path}\")\n",
    "else:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding=\"ISO-8859-10\", usecols=['P41601'], low_memory=False)\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=\"UTF-8\", usecols=['P41601'], low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            df = None\n",
    "\n",
    "    if df is not None:\n",
    "        print(\"¡Dataset cargado! Procesando columna P41601...\\n\")\n",
    "\n",
    "        # Convertimos a numérico (descartando errores)\n",
    "        df['P41601'] = pd.to_numeric(df['P41601'], errors='coerce')\n",
    "\n",
    "        # 1) ENFOQUE CON FOR LOOP (lento)\n",
    "        print(\"Valores > 5000 usando FOR LOOP:\")\n",
    "        for value in df['P41601']:\n",
    "            if pd.notnull(value) and value > 5000:\n",
    "                print(value)\n",
    "\n",
    "        # 2) ENFOQUE OPTIMIZADO (Pandas)\n",
    "        print(\"\\nValores > 5000 usando Pandas (vectorizado):\")\n",
    "        valores_altos = df[df['P41601'] > 5000]['P41601']\n",
    "        print(valores_altos.head(10))  # mostramos solo los primeros 10\n",
    "\n",
    "        # Estadísticos adicionales\n",
    "        print(f\"\\nTotal de valores > 5000: {len(valores_altos)}\")\n",
    "        print(f\"Valor máximo: {valores_altos.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d85acb-647c-4b8d-b438-e42f0f1317c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
