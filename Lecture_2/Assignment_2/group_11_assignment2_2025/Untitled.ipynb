{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830a3a5d",
   "metadata": {},
   "source": [
    "# Assignmet 2\n",
    "## Part 1 – Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b2d9b",
   "metadata": {},
   "source": [
    "1. Set your working directory and import the dataset Enaho01A-2023-300.csv using Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec7be1",
   "metadata": {},
   "source": [
    "*Configuración del Directorio de Trabajo usando Rutas Relativas con ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02fe722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta absoluta calculada: C:\\Users\\HOME\\Documents\\GitHub\\assingment_2\\data\n",
      "Directorio de trabajo establecido en: C:\\Users\\HOME\\Documents\\GitHub\\assingment_2\\data\n",
      "Dataset importado con encoding ISO-8859-10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Configurar el directorio de trabajo usando rutas relativas con ../\n",
    "# Para subir 4 niveles desde tu ubicación actual y luego bajar a assingment_2/data\n",
    "relative_path = \"../../../../assingment_2/data\"\n",
    "\n",
    "# 2. Convertir la ruta relativa a una ruta absoluta\n",
    "absolute_path = os.path.abspath(relative_path)\n",
    "print(f\"Ruta absoluta calculada: {absolute_path}\")\n",
    "\n",
    "# 3. Verificar si la ruta existe\n",
    "if not os.path.exists(absolute_path):\n",
    "    print(f\"Error: No se encuentra la ruta: {absolute_path}\")\n",
    "    print(\"Por favor, verifica la estructura de carpetas.\")\n",
    "else:\n",
    "    # 4. Establecer el directorio de trabajo\n",
    "    os.chdir(absolute_path)\n",
    "    print(f\"Directorio de trabajo establecido en: {os.getcwd()}\")\n",
    "    \n",
    "    # 5. Importar el dataset Enaho01A-2023-300.csv\n",
    "    try:\n",
    "        # Intentar primero con ISO-8859-10 como se indica en la nota\n",
    "        df = pd.read_csv(\"Enaho01A-2023-300.csv\", encoding=\"ISO-8859-10\", low_memory=False)\n",
    "        print(\"Dataset importado con encoding ISO-8859-10\")\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            # Si ISO-8859-10 falla, intentar con UTF-8\n",
    "            df = pd.read_csv(\"Enaho01A-2023-300.csv\", encoding=\"UTF-8\", low_memory=False)\n",
    "            print(\"Dataset importado con encoding UTF-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: No se pudo importar el dataset: {e}\")\n",
    "            df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d841b",
   "metadata": {},
   "source": [
    "*realizar las operaciones solicitadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f840ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 5 filas del dataset:\n",
      "    AŅO  MES  CONGLOME  VIVIENDA  HOGAR  CODPERSO  UBIGEO  DOMINIO  ESTRATO  \\\n",
      "0  2023    1      5030         2     11         1   10201        7        4   \n",
      "1  2023    1      5030         2     11         2   10201        7        4   \n",
      "2  2023    1      5030         2     11         3   10201        7        4   \n",
      "3  2023    1      5030         2     11         4   10201        7        4   \n",
      "4  2023    1      5030        11     11         1   10201        7        4   \n",
      "\n",
      "   CODINFOR  ...  I311D$5  I311D$6  I311D$7  I3121C I3122C I315B    FACTOR07  \\\n",
      "0         1  ...                                                  118.374542   \n",
      "1         2  ...                                                  118.374542   \n",
      "2         2  ...                                                  118.374542   \n",
      "3         2  ...                          8                       118.374542   \n",
      "4         1  ...                                                  118.374542   \n",
      "\n",
      "    FACTORA07 NCONGLOME SUB_CONGLOME  \n",
      "0  165.623856      6618            0  \n",
      "1  112.328087      6618            0  \n",
      "2  120.091476      6618            0  \n",
      "3  110.974678      6618            0  \n",
      "4   96.035370      6618            0  \n",
      "\n",
      "[5 rows x 511 columns]\n",
      "\n",
      "Nombres de columnas como lista:\n",
      "['AŅO', 'MES', 'CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO', 'UBIGEO', 'DOMINIO', 'ESTRATO', 'CODINFOR', 'P300N', 'P300I', 'P300A', 'P301A', 'P301B', 'P301C', 'P301D', 'P301A0', 'P301A1', 'P301B0', 'P301B1', 'P301B3', 'P302', 'P302X', 'P302A', 'P302B', 'P303', 'P304A', 'P304B', 'P304C', 'P304D', 'P305', 'P306', 'P307', 'P307A1', 'P307A2', 'P307A3', 'P307A4', 'P307A4_5', 'P307A4_6', 'P307A4_7', 'P307B1', 'P307B2', 'P307B3', 'P307B4', 'P307B4_5', 'P307B4_6', 'P307B4_7', 'P307C', 'P308A', 'P308B', 'P308C', 'P308D', 'P308B1', 'P308B2', 'P308B3', 'P308B4', 'P308B5', 'P308C1', 'P308C2', 'P310', 'P310B1', 'P310C0', 'P310C1', 'P310D1', 'P310D2', 'P310E0', 'P310E1', 'P310E3', 'P311N$1', 'P311N$2', 'P311N$3', 'P311N$4', 'P311N$5', 'P311N$6', 'P311N$7', 'P311N$8', 'P311N$9', 'P311$1', 'P311$2', 'P311$3', 'P311$4', 'P311$5', 'P311$6', 'P311$7', 'P311$8', 'P311$9', 'P311A1$1', 'P311A1$2', 'P311A1$3', 'P311A1$4', 'P311A1$5', 'P311A1$6', 'P311A1$7', 'P311A1$8', 'P311A1$9', 'P311A2$1', 'P311A2$2', 'P311A2$3', 'P311A2$4', 'P311A2$5', 'P311A2$6', 'P311A2$7', 'P311A2$8', 'P311A2$9', 'P311A3$1', 'P311A3$2', 'P311A3$3', 'P311A3$4', 'P311A3$5', 'P311A3$6', 'P311A3$7', 'P311A3$8', 'P311A3$9', 'P311A4$1', 'P311A4$2', 'P311A4$3', 'P311A4$4', 'P311A4$5', 'P311A4$6', 'P311A4$7', 'P311A4$8', 'P311A4$9', 'P311A5$1', 'P311A5$2', 'P311A5$3', 'P311A5$4', 'P311A5$5', 'P311A5$6', 'P311A5$7', 'P311A5$8', 'P311A5$9', 'P311A6$1', 'P311A6$2', 'P311A6$3', 'P311A6$4', 'P311A6$5', 'P311A6$6', 'P311A6$7', 'P311A6$8', 'P311A6$9', 'P311A7$1', 'P311A7$2', 'P311A7$3', 'P311A7$4', 'P311A7$5', 'P311A7$6', 'P311A7$7', 'P311A7$8', 'P311A7$9', 'P311B$1', 'P311B$2', 'P311B$3', 'P311B$4', 'P311B$5', 'P311B$6', 'P311B$7', 'P311B$8', 'P311B$9', 'P311C$1', 'P311C$2', 'P311C$3', 'P311C$4', 'P311C$5', 'P311C$6', 'P311C$7', 'P311C$8', 'P311C$9', 'P311D$1', 'P311D$2', 'P311D$3', 'P311D$4', 'P311D$5', 'P311D$6', 'P311D$7', 'P311D$8', 'P311D$9', 'P311D2$1', 'P311D2$2', 'P311D2$3', 'P311D2$4', 'P311D2$5', 'P311D2$6', 'P311D2$7', 'P311D2$8', 'P311D2$9', 'P311D3$1', 'P311D3$2', 'P311D3$3', 'P311D3$4', 'P311D3$5', 'P311D3$6', 'P311D3$7', 'P311D3$8', 'P311D3$9', 'P311D4$1', 'P311D4$2', 'P311D4$3', 'P311D4$4', 'P311D4$5', 'P311D4$6', 'P311D4$7', 'P311D4$8', 'P311D4$9', 'P311D5$1', 'P311D5$2', 'P311D5$3', 'P311D5$4', 'P311D5$5', 'P311D5$6', 'P311D5$7', 'P311D5$8', 'P311D5$9', 'P311D6$1', 'P311D6$2', 'P311D6$3', 'P311D6$4', 'P311D6$5', 'P311D6$6', 'P311D6$7', 'P311D6$8', 'P311D6$9', 'P311D7$1', 'P311D7$2', 'P311D7$3', 'P311D7$4', 'P311D7$5', 'P311D7$6', 'P311D7$7', 'P311D7$8', 'P311D7$9', 'P311E$1', 'P311E$2', 'P311E$3', 'P311E$4', 'P311E$5', 'P311E$6', 'P311E$7', 'P311E$8', 'P311E$9', 'P311T1', 'P311T22', 'P311T23', 'P311T24', 'P311T25', 'P311T26', 'P311T27', 'P311T2', 'P3121', 'P3121A1', 'P3121A2', 'P3121A3', 'P3121A4', 'P3121A5', 'P3121A6', 'P3121B', 'P3121C', 'P3121C2', 'P3121C3', 'P3121C4', 'P3121C5', 'P3121C6', 'P3121D', 'P3122', 'P3122A1', 'P3122A2', 'P3122A3', 'P3122A4', 'P3122A5', 'P3122A6', 'P3122B', 'P3122C', 'P3122C2', 'P3122C3', 'P3122C4', 'P3122C5', 'P3122C6', 'P3122D', 'P312T1', 'P312T22', 'P312T23', 'P312T24', 'P312T25', 'P312T26', 'P312T2', 'P313', 'P314A', 'P314B$1', 'P314B$2', 'P314B$3', 'P314B$4', 'P314B$5', 'P314B$6', 'P314B$7', 'P314B1_1', 'P314B1_2', 'P314B1_6', 'P314B1_7', 'P314B1_8', 'P314B1_9', 'P314D', 'P3151', 'P3152', 'P3153', 'P3154', 'P3155', 'P3156', 'P315A', 'P315B', 'P315B2', 'P315B3', 'P315B4', 'P315B5', 'P315B6', 'P316$1', 'P316$2', 'P316$3', 'P316$4', 'P316$5', 'P316$6', 'P316$7', 'P316$8', 'P316$9', 'P316$10', 'P316$11', 'P316$12', 'P316A1', 'P316A2', 'P316A3', 'P316A4', 'P316A5', 'P316A6', 'P316B', 'P316C1', 'P316C2', 'P316C3', 'P316C4', 'P316C5', 'P316C6', 'P316C7', 'P316C8', 'P316C9', 'P316C10', 'T313A', 'P203', 'P204', 'P205', 'P206', 'P207', 'P208A', 'P209', 'IMPUTADO', 'TICUEST01A', 'D311B$1', 'D311D2$1', 'D311D3$1', 'D311D4$1', 'D311D5$1', 'D311D6$1', 'D311D7$1', 'D311D$1', 'D311B$2', 'D311D2$2', 'D311D3$2', 'D311D4$2', 'D311D5$2', 'D311D6$2', 'D311D7$2', 'D311D$2', 'D311B$3', 'D311D2$3', 'D311D3$3', 'D311D4$3', 'D311D5$3', 'D311D6$3', 'D311D7$3', 'D311D$3', 'D311B$4', 'D311D2$4', 'D311D3$4', 'D311D4$4', 'D311D5$4', 'D311D6$4', 'D311D7$4', 'D311D$4', 'D311B$5', 'D311D2$5', 'D311D3$5', 'D311D4$5', 'D311D5$5', 'D311D6$5', 'D311D7$5', 'D311D$5', 'D311B$6', 'D311D2$6', 'D311D3$6', 'D311D4$6', 'D311D5$6', 'D311D6$6', 'D311D7$6', 'D311D$6', 'D311B$7', 'D311D2$7', 'D311D3$7', 'D311D4$7', 'D311D5$7', 'D311D6$7', 'D311D7$7', 'D311D$7', 'D3121B', 'D3121C2', 'D3121C3', 'D3121C4', 'D3121C5', 'D3121C6', 'D3121C', 'D3122B', 'D3122C2', 'D3122C3', 'D3122C4', 'D3122C5', 'D3122C6', 'D3122C', 'D315A', 'D315B2', 'D315B3', 'D315B4', 'D315B5', 'D315B6', 'D315B', 'I311B$1', 'I311B$2', 'I311B$4', 'I311B$6', 'I311B$3', 'I311B$5', 'I311B$7', 'I311D2$1', 'I311D3$1', 'I311D4$1', 'I311D5$1', 'I311D6$1', 'I311D7$1', 'I311D2$2', 'I311D3$2', 'I311D4$2', 'I311D5$2', 'I311D6$2', 'I311D7$2', 'I311D2$4', 'I311D3$4', 'I311D4$4', 'I311D5$4', 'I311D6$4', 'I311D7$4', 'I311D2$6', 'I311D3$6', 'I311D4$6', 'I311D5$6', 'I311D6$6', 'I311D7$6', 'I311D2$3', 'I311D3$3', 'I311D4$3', 'I311D5$3', 'I311D6$3', 'I311D7$3', 'I311D2$5', 'I311D3$5', 'I311D4$5', 'I311D5$5', 'I311D6$5', 'I311D7$5', 'I311D2$7', 'I311D3$7', 'I311D4$7', 'I311D5$7', 'I311D6$7', 'I311D7$7', 'I3121B', 'I3122B', 'I3121C2', 'I3121C3', 'I3121C4', 'I3121C5', 'I3121C6', 'I3122C2', 'I3122C3', 'I3122C4', 'I3122C5', 'I3122C6', 'I315A', 'I315B2', 'I315B3', 'I315B4', 'I315B5', 'I315B6', 'I311D$1', 'I311D$2', 'I311D$3', 'I311D$4', 'I311D$5', 'I311D$6', 'I311D$7', 'I3121C', 'I3122C', 'I315B', 'FACTOR07', 'FACTORA07', 'NCONGLOME', 'SUB_CONGLOME']\n",
      "\n",
      "Tipos de datos del DataFrame:\n",
      "AŅO               int64\n",
      "MES               int64\n",
      "CONGLOME          int64\n",
      "VIVIENDA          int64\n",
      "HOGAR             int64\n",
      "                 ...   \n",
      "I315B            object\n",
      "FACTOR07        float64\n",
      "FACTORA07       float64\n",
      "NCONGLOME         int64\n",
      "SUB_CONGLOME      int64\n",
      "Length: 511, dtype: object\n",
      "\n",
      "Subconjunto seleccionado con 8 variables:\n",
      "   CONGLOME  VIVIENDA  HOGAR  CODPERSO  P300A  P301A P302A P304A\n",
      "0      5030         2     11         1      4      8            \n",
      "1      5030         2     11         2      4     10            \n",
      "2      5030         2     11         3      4      3           2\n",
      "3      5030         2     11         4      8      3           2\n",
      "4      5030        11     11         1      4      4            \n",
      "\n",
      "Dimensiones del subconjunto: (108354, 8)\n",
      "\n",
      "Tipos de datos del subconjunto:\n",
      "CONGLOME     int64\n",
      "VIVIENDA     int64\n",
      "HOGAR        int64\n",
      "CODPERSO     int64\n",
      "P300A        int64\n",
      "P301A        int64\n",
      "P302A       object\n",
      "P304A       object\n",
      "dtype: object\n",
      "\n",
      "Subconjunto con nombres de variables renombrados:\n",
      "   CONGLOME  VIVIENDA  HOGAR  CODPERSO  lengua_materna  nivel_educativo  \\\n",
      "0      5030         2     11         1               4                8   \n",
      "1      5030         2     11         2               4               10   \n",
      "2      5030         2     11         3               4                3   \n",
      "3      5030         2     11         4               8                3   \n",
      "4      5030        11     11         1               4                4   \n",
      "\n",
      "  programa_alfabetizacion nivel_asistencia_actual  \n",
      "0                                                  \n",
      "1                                                  \n",
      "2                                               2  \n",
      "3                                               2  \n",
      "4                                                  \n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    # a. Leer y mostrar las primeras 5 filas\n",
    "    print(\"\\nPrimeras 5 filas del dataset:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # b. Convertir los nombres de columna a una lista e imprimirla\n",
    "    column_names = df.columns.tolist()\n",
    "    print(\"\\nNombres de columnas como lista:\")\n",
    "    print(column_names)\n",
    "    \n",
    "    # c. Verificar los tipos de datos del DataFrame\n",
    "    print(\"\\nTipos de datos del DataFrame:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # d. Seleccionar un subconjunto con las variables requeridas\n",
    "    variables_base = ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO']\n",
    "    \n",
    "    # Seleccionar 4 variables adicionales de interés educativo\n",
    "    variables_adicionales = [\n",
    "        'P300A',  # Idioma o lengua materna\n",
    "        'P301A',  # Último año o grado de estudios y nivel que aprobó - Nivel\n",
    "        'P302A',  # En los últimos 12 meses, ¿Recibió programa de alfabetización?\n",
    "        'P304A'   # ¿Cuál es el grado o año de estudios al que asistió el año pasado? - Nivel\n",
    "    ]\n",
    "    \n",
    "    # Verificar que las variables existan en el dataset\n",
    "    variables_existentes = [col for col in variables_base + variables_adicionales if col in df.columns]\n",
    "    \n",
    "    # Crear el subconjunto\n",
    "    subsample = df[variables_existentes]\n",
    "    \n",
    "    print(f\"\\nSubconjunto seleccionado con {len(variables_existentes)} variables:\")\n",
    "    print(subsample.head())\n",
    "    \n",
    "    # Mostrar información sobre el subconjunto\n",
    "    print(f\"\\nDimensiones del subconjunto: {subsample.shape}\")\n",
    "    print(\"\\nTipos de datos del subconjunto:\")\n",
    "    print(subsample.dtypes)\n",
    "     # 7. Renombrar las variables adicionales para mayor claridad\n",
    "    nuevos_nombres = {\n",
    "        'P300A': 'lengua_materna',\n",
    "        'P301A': 'nivel_educativo',\n",
    "        'P302A': 'programa_alfabetizacion',\n",
    "        'P304A': 'nivel_asistencia_actual'\n",
    "    }\n",
    "    \n",
    "    subsample = subsample.rename(columns=nuevos_nombres)\n",
    "    \n",
    "    print(\"\\nSubconjunto con nombres de variables renombrados:\")\n",
    "    print(subsample.head())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe9951",
   "metadata": {},
   "source": [
    "2. Data Manipulation (Data Cleaning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c670aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPLORACIÓN INICIAL DEL DATAFRAME ===\n",
      "\n",
      "1. INFORMACIÓN GENERAL DEL DATAFRAME:\n",
      "Dimensiones: (108354, 511) (filas, columnas)\n",
      "Número total de datos: 55368894\n",
      "\n",
      "2. RESUMEN ESTADÍSTICO DE VARIABLES NUMÉRICAS:\n",
      "            AŅO            MES       CONGLOME       VIVIENDA          HOGAR  \\\n",
      "count  108354.0  108354.000000  108354.000000  108354.000000  108354.000000   \n",
      "mean     2023.0       6.495127   16944.756797      77.820330      11.146271   \n",
      "std         0.0       3.445244    3144.386733      68.547022       1.370084   \n",
      "min      2023.0       1.000000    5007.000000       1.000000      11.000000   \n",
      "25%      2023.0       3.000000   16028.000000      31.000000      11.000000   \n",
      "50%      2023.0       7.000000   17500.000000      66.000000      11.000000   \n",
      "75%      2023.0       9.000000   19014.000000     106.000000      11.000000   \n",
      "max      2023.0      12.000000   21001.000000     991.000000      44.000000   \n",
      "\n",
      "            CODPERSO         UBIGEO        DOMINIO        ESTRATO  \\\n",
      "count  108354.000000  108354.000000  108354.000000  108354.000000   \n",
      "mean        2.546293  131185.171087       4.864961       4.138961   \n",
      "std         1.577737   67736.903764       2.404443       2.432348   \n",
      "min         1.000000   10101.000000       1.000000       1.000000   \n",
      "25%         1.000000   80101.000000       2.000000       2.000000   \n",
      "50%         2.000000  140108.000000       5.000000       4.000000   \n",
      "75%         3.000000  180301.000000       7.000000       7.000000   \n",
      "max        22.000000  250401.000000       8.000000       8.000000   \n",
      "\n",
      "            CODINFOR  ...          P301A           P203           P204  \\\n",
      "count  108354.000000  ...  108354.000000  108354.000000  108354.000000   \n",
      "mean        2.033058  ...       5.258652       2.548729       1.005787   \n",
      "std         1.282622  ...       3.807669       1.716218       0.075850   \n",
      "min         0.000000  ...       1.000000       1.000000       1.000000   \n",
      "25%         1.000000  ...       3.000000       1.000000       1.000000   \n",
      "50%         2.000000  ...       5.000000       3.000000       1.000000   \n",
      "75%         2.000000  ...       6.000000       3.000000       1.000000   \n",
      "max        21.000000  ...      99.000000      11.000000       2.000000   \n",
      "\n",
      "                P207          P208A     TICUEST01A       FACTOR07  \\\n",
      "count  108354.000000  108354.000000  108354.000000  108354.000000   \n",
      "mean        1.513428      35.619839       1.999539     305.556281   \n",
      "std         0.499822      22.152408       0.021477     310.795722   \n",
      "min         1.000000       3.000000       1.000000       1.087094   \n",
      "25%         1.000000      16.000000       2.000000     123.659256   \n",
      "50%         2.000000      33.000000       2.000000     233.634140   \n",
      "75%         2.000000      53.000000       2.000000     354.661774   \n",
      "max         2.000000      98.000000       2.000000    1961.005981   \n",
      "\n",
      "           FACTORA07      NCONGLOME   SUB_CONGLOME  \n",
      "count  108354.000000  108354.000000  108354.000000  \n",
      "mean      302.602025   23603.093914       0.282325  \n",
      "std       322.927385   15607.753763       0.662107  \n",
      "min         0.422172       2.000000       0.000000  \n",
      "25%       101.125488    8855.000000       0.000000  \n",
      "50%       218.866760   20186.000000       0.000000  \n",
      "75%       365.609833   37361.000000       0.000000  \n",
      "max      2917.495850   51561.000000       6.000000  \n",
      "\n",
      "[8 rows x 23 columns]\n",
      "\n",
      "3. INFORMACIÓN SOBRE TIPOS DE DATOS Y VALORES NO NULOS:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108354 entries, 0 to 108353\n",
      "Columns: 511 entries, AŅO to SUB_CONGLOME\n",
      "dtypes: float64(2), int64(21), object(488)\n",
      "memory usage: 422.4+ MB\n",
      "None\n",
      "\n",
      "=== IDENTIFICACIÓN DE VALORES MISSING ===\n",
      "\n",
      "Valores missing por columna:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Porcentaje de valores missing por columna:\n",
      "Series([], dtype: float64)\n",
      "\n",
      "=== MANEJO DE VALORES MISSING ===\n",
      "\n",
      "Valores missing en variables clave:\n",
      "CONGLOME    0\n",
      "VIVIENDA    0\n",
      "HOGAR       0\n",
      "CODPERSO    0\n",
      "P300A       0\n",
      "P301A       0\n",
      "P302A       0\n",
      "P304A       0\n",
      "dtype: int64\n",
      "\n",
      "Filas eliminadas por valores missing en variables de identificación: 0\n",
      "Filas restantes: 108354\n",
      "\n",
      "=== VALORES CODIFICADOS COMO MISSING ===\n",
      "\n",
      "Valores codificados como missing en P300A: 105\n",
      "Valores codificados como missing en P301A: 105\n",
      "Valores codificados como missing en P302A: 0\n",
      "Valores codificados como missing en P304A: 0\n",
      "\n",
      "=== ESTADO DESPUÉS DE LA LIMPIEZA ===\n",
      "\n",
      "Valores missing después de la limpieza:\n",
      "P300A    105\n",
      "P301A    105\n",
      "dtype: int64\n",
      "Imputados 0 valores missing en P300A con la mediana: 4.0\n",
      "Imputados 0 valores missing en P301A con la mediana: 5.0\n",
      "\n",
      "=== VERIFICACIÓN FINAL ===\n",
      "\n",
      "Valores missing restantes en todo el DataFrame: 0\n",
      "\n",
      "Dimensiones del dataset después de la limpieza: (108354, 511)\n",
      "\n",
      "Dataset limpio guardado como 'enaho_educacion_clean.csv'\n",
      "\n",
      "Información del dataset limpio:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108354 entries, 0 to 108353\n",
      "Columns: 511 entries, AŅO to SUB_CONGLOME\n",
      "dtypes: float64(4), int64(19), object(488)\n",
      "memory usage: 422.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    # 1. Explorar el DataFrame usando funciones de resumen\n",
    "    print(\"\\n=== EXPLORACIÓN INICIAL DEL DATAFRAME ===\\n\")\n",
    "    \n",
    "    # Información general del DataFrame\n",
    "    print(\"1. INFORMACIÓN GENERAL DEL DATAFRAME:\")\n",
    "    print(f\"Dimensiones: {df.shape} (filas, columnas)\")\n",
    "    print(f\"Número total de datos: {df.size}\")\n",
    "    \n",
    "    # Resumen estadístico de las variables numéricas\n",
    "    print(\"\\n2. RESUMEN ESTADÍSTICO DE VARIABLES NUMÉRICAS:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Información sobre tipos de datos y valores no nulos\n",
    "    print(\"\\n3. INFORMACIÓN SOBRE TIPOS DE DATOS Y VALORES NO NULOS:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # 2. Identificar valores missing (faltantes)\n",
    "    print(\"\\n=== IDENTIFICACIÓN DE VALORES MISSING ===\\n\")\n",
    "    \n",
    "    # Contar valores missing por columna\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"Valores missing por columna:\")\n",
    "    print(missing_values[missing_values > 0])  # Mostrar solo columnas con valores missing\n",
    "    \n",
    "    # Porcentaje de valores missing por columna\n",
    "    missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "    print(\"\\nPorcentaje de valores missing por columna:\")\n",
    "    print(missing_percentage[missing_percentage > 0])  # Mostrar solo columnas con valores missing\n",
    "    \n",
    "    # 3. Manejo de valores missing\n",
    "    print(\"\\n=== MANEJO DE VALORES MISSING ===\\n\")\n",
    "    \n",
    "    # Estrategia para manejar valores missing:\n",
    "    # - Para variables categóricas: Podemos usar una categoría específica para missing values\n",
    "    # - Para variables numéricas: Podemos imputar con la media, mediana o eliminar\n",
    "    \n",
    "    # Primero, verificar si hay valores missing en las variables clave\n",
    "    key_variables = ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO', 'P300A', 'P301A', 'P302A', 'P304A']\n",
    "    key_missing = df[key_variables].isnull().sum()\n",
    "    \n",
    "    print(\"Valores missing en variables clave:\")\n",
    "    print(key_missing)\n",
    "    \n",
    "    # Eliminar filas con valores missing en variables clave (si es necesario)\n",
    "    # Nota: En datasets de encuestas, a veces es mejor no eliminar filas ya que cada una representa una persona/hogar\n",
    "    \n",
    "    # Para este ejemplo, eliminaremos filas donde todas las variables clave tienen valores missing\n",
    "    # Pero primero, verifiquemos cuántas filas serían eliminadas\n",
    "    rows_before = len(df)\n",
    "    \n",
    "    # Eliminar filas donde las variables de identificación tienen valores missing\n",
    "    # (Estas no deberían tener valores missing en un dataset bien construido)\n",
    "    df_clean = df.dropna(subset=['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO'], how='any')\n",
    "    \n",
    "    rows_after = len(df_clean)\n",
    "    rows_removed = rows_before - rows_after\n",
    "    \n",
    "    print(f\"\\nFilas eliminadas por valores missing en variables de identificación: {rows_removed}\")\n",
    "    print(f\"Filas restantes: {rows_after}\")\n",
    "    \n",
    "    # Para variables categóricas con valores missing, podemos asignar una categoría específica\n",
    "    # Basado en el diccionario de datos, sabemos que los valores missing están codificados como 99 o 9\n",
    "    \n",
    "    # 4. Verificar valores específicos codificados como missing (según el diccionario de datos)\n",
    "    print(\"\\n=== VALORES CODIFICADOS COMO MISSING ===\\n\")\n",
    "    \n",
    "    # Definir los valores que representan missing según el diccionario\n",
    "    missing_codes = {\n",
    "        'P300A': [99],  # Lengua materna\n",
    "        'P301A': [99],  # Nivel educativo\n",
    "        'P302A': [9],   # Programa de alfabetización\n",
    "        'P304A': [9]    # Nivel de asistencia actual\n",
    "    }\n",
    "    \n",
    "    # Contar valores codificados como missing\n",
    "    for column, codes in missing_codes.items():\n",
    "        if column in df_clean.columns:\n",
    "            count = df_clean[column].isin(codes).sum()\n",
    "            print(f\"Valores codificados como missing en {column}: {count}\")\n",
    "            \n",
    "            # Reemplazar valores codificados con NaN\n",
    "            df_clean[column] = df_clean[column].replace(codes, float('nan'))\n",
    "    \n",
    "    # 5. Verificar el estado después de la limpieza\n",
    "    print(\"\\n=== ESTADO DESPUÉS DE LA LIMPIEZA ===\\n\")\n",
    "    \n",
    "    # Valores missing después de la limpieza\n",
    "    missing_after = df_clean.isnull().sum()\n",
    "    print(\"Valores missing después de la limpieza:\")\n",
    "    print(missing_after[missing_after > 0])\n",
    "    \n",
    "    # Para variables numéricas, podemos imputar valores missing con la mediana\n",
    "    numeric_columns = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            median_value = df_clean[col].median()\n",
    "            df_clean[col].fillna(median_value, inplace=True)\n",
    "            print(f\"Imputados {df_clean[col].isnull().sum()} valores missing en {col} con la mediana: {median_value}\")\n",
    "    \n",
    "    # Para variables categóricas, podemos imputar con la moda o asignar una categoría \"Desconocido\"\n",
    "    categorical_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            mode_value = df_clean[col].mode()[0]\n",
    "            df_clean[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"Imputados {df_clean[col].isnull().sum()} valores missing en {col} con la moda: {mode_value}\")\n",
    "    \n",
    "    # 6. Verificación final\n",
    "    print(\"\\n=== VERIFICACIÓN FINAL ===\\n\")\n",
    "    \n",
    "    # Comprobar si aún hay valores missing\n",
    "    remaining_missing = df_clean.isnull().sum().sum()\n",
    "    print(f\"Valores missing restantes en todo el DataFrame: {remaining_missing}\")\n",
    "    \n",
    "    # Resumen del dataset limpio\n",
    "    print(f\"\\nDimensiones del dataset después de la limpieza: {df_clean.shape}\")\n",
    "    \n",
    "    # Guardar el dataset limpio (opcional)\n",
    "    df_clean.to_csv(\"enaho_educacion_clean.csv\", index=False, encoding='utf-8')\n",
    "    print(\"\\nDataset limpio guardado como 'enaho_educacion_clean.csv'\")\n",
    "    \n",
    "    # Mostrar información del dataset limpio\n",
    "    print(\"\\nInformación del dataset limpio:\")\n",
    "    print(df_clean.info())\n",
    "    \n",
    "else:\n",
    "    print(\"No se pudo realizar la limpieza de datos debido a problemas con la importación del dataset.\")\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545db414",
   "metadata": {},
   "source": [
    " 3. Importación y Manipulación del Segundo Dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1566b38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de vivienda importado con encoding ISO-8859-10\n",
      "\n",
      "Primeras 5 filas del dataset de vivienda:\n",
      "    AŅO  MES  CONGLOME  VIVIENDA  HOGAR  CODPERSO  UBIGEO  DOMINIO  ESTRATO  \\\n",
      "0  2023    2      5007        22     11         1   10101        4        4   \n",
      "1  2023    2      5007        22     11         2   10101        4        4   \n",
      "2  2023    2      5007        22     11         3   10101        4        4   \n",
      "3  2023    2      5007        31     11         1   10101        4        4   \n",
      "4  2023    2      5007        31     11         2   10101        4        4   \n",
      "\n",
      "               P201P  ...  OCUPAC_R3 OCUPAC_R4 RAMA_R3 RAMA_R4 CODTAREA  \\\n",
      "0  20190050070221101  ...                                                 \n",
      "1  20190050070221102  ...                                                 \n",
      "2  20190050070221104  ...                                                 \n",
      "3  20190050070311102  ...                                                 \n",
      "4  20230050070311102  ...                                                 \n",
      "\n",
      "  CODTIEMPO TICUEST01   FACPOB07 NCONGLOME SUB_CONGLOME  \n",
      "0                   2  50.466671      7070            0  \n",
      "1                   2  50.466671      7070            0  \n",
      "2                   2  50.466671      7070            0  \n",
      "3                   2  50.466671      7070            0  \n",
      "4                   2  50.466671      7070            0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "\n",
      "Nombres de columnas del dataset de vivienda:\n",
      "['AŅO', 'MES', 'CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO', 'UBIGEO', 'DOMINIO', 'ESTRATO', 'P201P', 'P203', 'P203A', 'P203B', 'P204', 'P205', 'P206', 'P207', 'P208A', 'P208B', 'P209', 'P210', 'P211A', 'P211D', 'P212', 'P213', 'P214', 'P215', 'P216', 'P217', 'T211', 'OCUPAC_R3', 'OCUPAC_R4', 'RAMA_R3', 'RAMA_R4', 'CODTAREA', 'CODTIEMPO', 'TICUEST01', 'FACPOB07', 'NCONGLOME', 'SUB_CONGLOME']\n",
      "\n",
      "Tipos de datos del dataset de vivienda:\n",
      "AŅO               int64\n",
      "MES               int64\n",
      "CONGLOME          int64\n",
      "VIVIENDA          int64\n",
      "HOGAR             int64\n",
      "CODPERSO          int64\n",
      "UBIGEO            int64\n",
      "DOMINIO           int64\n",
      "ESTRATO           int64\n",
      "P201P             int64\n",
      "P203              int64\n",
      "P203A            object\n",
      "P203B            object\n",
      "P204             object\n",
      "P205             object\n",
      "P206             object\n",
      "P207             object\n",
      "P208A            object\n",
      "P208B            object\n",
      "P209             object\n",
      "P210             object\n",
      "P211A            object\n",
      "P211D            object\n",
      "P212             object\n",
      "P213             object\n",
      "P214             object\n",
      "P215             object\n",
      "P216             object\n",
      "P217             object\n",
      "T211             object\n",
      "OCUPAC_R3        object\n",
      "OCUPAC_R4        object\n",
      "RAMA_R3          object\n",
      "RAMA_R4          object\n",
      "CODTAREA         object\n",
      "CODTIEMPO        object\n",
      "TICUEST01         int64\n",
      "FACPOB07        float64\n",
      "NCONGLOME         int64\n",
      "SUB_CONGLOME      int64\n",
      "dtype: object\n",
      "\n",
      "Subconjunto seleccionado con 9 variables:\n",
      "   CONGLOME  VIVIENDA  HOGAR  CODPERSO  P203 P204 P205 P206 P207\n",
      "0      5007        22     11         1     1    1    2         1\n",
      "1      5007        22     11         2     2    1    2         2\n",
      "2      5007        22     11         3     3    1    2         1\n",
      "3      5007        31     11         1     1    1    2         2\n",
      "4      5007        31     11         2     3    2         2    1\n",
      "\n",
      "=== MODIFICACIONES AL SUBCONJUNTO ===\n",
      "\n",
      "Tipos de datos antes de las modificaciones:\n",
      "CONGLOME     int64\n",
      "VIVIENDA     int64\n",
      "HOGAR        int64\n",
      "CODPERSO     int64\n",
      "P203         int64\n",
      "P204        object\n",
      "P205        object\n",
      "P206        object\n",
      "P207        object\n",
      "dtype: object\n",
      "\n",
      "A. Cambiando tipo de dato de P205 de object a category\n",
      "Tipo de dato de P205 después del cambio: category\n",
      "\n",
      "B. Modificando valores en la columna P206 (Material de las paredes):\n",
      "Valores originales en P206:\n",
      "P206\n",
      "     115340\n",
      "2      3730\n",
      "1       677\n",
      "Name: count, dtype: int64\n",
      "Valores modificados en P206_modificado:\n",
      "P206_modificado\n",
      "Adobe o tapia                   3730\n",
      "Ladrillo o bloque de cemento     677\n",
      "Name: count, dtype: int64\n",
      "\n",
      "C. Convirtiendo P203 a numérico\n",
      "Tipo de dato de P203 después del cambio: int64\n",
      "\n",
      "D. Convirtiendo P204 a numérico\n",
      "Tipo de dato de P204 después del cambio: float64\n",
      "\n",
      "Subconjunto después de las modificaciones:\n",
      "   CONGLOME  VIVIENDA  HOGAR  CODPERSO  P203  P204 P205 P206 P207  \\\n",
      "0      5007        22     11         1     1   1.0    2         1   \n",
      "1      5007        22     11         2     2   1.0    2         2   \n",
      "2      5007        22     11         3     3   1.0    2         1   \n",
      "3      5007        31     11         1     1   1.0    2         2   \n",
      "4      5007        31     11         2     3   2.0         2    1   \n",
      "\n",
      "  P206_modificado  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4   Adobe o tapia  \n",
      "\n",
      "Tipos de datos después de las modificaciones:\n",
      "CONGLOME              int64\n",
      "VIVIENDA              int64\n",
      "HOGAR                 int64\n",
      "CODPERSO              int64\n",
      "P203                  int64\n",
      "P204                float64\n",
      "P205               category\n",
      "P206                 object\n",
      "P207                 object\n",
      "P206_modificado      object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_13000\\4231310518.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subsample_vivienda['P205'] = subsample_vivienda['P205'].astype('category')\n",
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_13000\\4231310518.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subsample_vivienda['P206_modificado'] = subsample_vivienda['P206'].map(mapeo_p206)\n",
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_13000\\4231310518.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subsample_vivienda['P203'] = pd.to_numeric(subsample_vivienda['P203'], errors='coerce')\n",
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_13000\\4231310518.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subsample_vivienda['P204'] = pd.to_numeric(subsample_vivienda['P204'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subconjunto modificado guardado como 'subsample_vivienda_modificado.csv'\n"
     ]
    }
   ],
   "source": [
    "# Importar el segundo dataset (módulo de vivienda) con el nombre correcto\n",
    "try:\n",
    "    df_vivienda = pd.read_csv(\"Enaho01-2023-200.csv\", encoding=\"ISO-8859-10\", low_memory=False)\n",
    "    print(\"Dataset de vivienda importado con encoding ISO-8859-10\")\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        df_vivienda = pd.read_csv(\"Enaho01-2023-200.csv\", encoding=\"UTF-8\", low_memory=False)\n",
    "        print(\"Dataset de vivienda importado con encoding UTF-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: No se pudo importar el dataset de vivienda: {e}\")\n",
    "        df_vivienda = None\n",
    "\n",
    "# Si la importación fue exitosa, realizar las operaciones solicitadas\n",
    "if df_vivienda is not None:\n",
    "    # 1. Display the first 5 rows\n",
    "    print(\"\\nPrimeras 5 filas del dataset de vivienda:\")\n",
    "    print(df_vivienda.head())\n",
    "    \n",
    "    # 2. Convert the column names into a list and print it\n",
    "    column_names_vivienda = df_vivienda.columns.tolist()\n",
    "    print(\"\\nNombres de columnas del dataset de vivienda:\")\n",
    "    print(column_names_vivienda)\n",
    "    \n",
    "    # 3. Check the data types\n",
    "    print(\"\\nTipos de datos del dataset de vivienda:\")\n",
    "    print(df_vivienda.dtypes)\n",
    "    \n",
    "    # 4. Select a subsample with the required variables\n",
    "    variables_base = ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO']\n",
    "    \n",
    "    # Seleccionar variables adicionales que SÍ existen en el dataset\n",
    "    # Basado en las columnas disponibles que vimos en el output\n",
    "    variables_adicionales_vivienda = [\n",
    "        'P203',   # ¿Cuántos cuartos o habitaciones tiene su hogar? (excluye baños y cocina)\n",
    "        'P204',   # ¿En cuántos de estos cuartos duermen las personas del hogar?\n",
    "        'P205',   # ¿La vivienda que ocupa el hogar es?\n",
    "        'P206',   # Material de las paredes\n",
    "        'P207'    # Material de los pisos\n",
    "    ]\n",
    "    \n",
    "    # Verificar que las variables existan en el dataset\n",
    "    variables_existentes_vivienda = [col for col in variables_base + variables_adicionales_vivienda if col in df_vivienda.columns]\n",
    "    \n",
    "    # Crear el subconjunto\n",
    "    subsample_vivienda = df_vivienda[variables_existentes_vivienda]\n",
    "    \n",
    "    print(f\"\\nSubconjunto seleccionado con {len(variables_existentes_vivienda)} variables:\")\n",
    "    print(subsample_vivienda.head())\n",
    "    \n",
    "    # 5. Perform the following modifications:\n",
    "    print(\"\\n=== MODIFICACIONES AL SUBCONJUNTO ===\\n\")\n",
    "    \n",
    "    # A. Change the data type of a variable\n",
    "    print(\"Tipos de datos antes de las modificaciones:\")\n",
    "    print(subsample_vivienda.dtypes)\n",
    "    \n",
    "    # Convertir P205 (tipo de vivienda) a categórica\n",
    "    if 'P205' in subsample_vivienda.columns:\n",
    "        print(f\"\\nA. Cambiando tipo de dato de P205 de {subsample_vivienda['P205'].dtype} a category\")\n",
    "        subsample_vivienda['P205'] = subsample_vivienda['P205'].astype('category')\n",
    "        print(f\"Tipo de dato de P205 después del cambio: {subsample_vivienda['P205'].dtype}\")\n",
    "    \n",
    "    # B. Modify some values in a specific column\n",
    "    # Modificar valores en la columna P206 (Material de las paredes)\n",
    "    if 'P206' in subsample_vivienda.columns:\n",
    "        print(\"\\nB. Modificando valores en la columna P206 (Material de las paredes):\")\n",
    "        print(\"Valores originales en P206:\")\n",
    "        print(subsample_vivienda['P206'].value_counts())\n",
    "        \n",
    "        # Crear un mapeo de valores (basado en el diccionario de la ENAHO)\n",
    "        # Nota: Estos valores deben verificarse con el diccionario oficial\n",
    "        mapeo_p206 = {\n",
    "            \"1\": \"Ladrillo o bloque de cemento\",\n",
    "            \"2\": \"Adobe o tapia\",\n",
    "            \"3\": \"Quincha\",\n",
    "            \"4\": \"Piedra con barro\",\n",
    "            \"5\": \"Piedra con cal o cemento\",\n",
    "            \"6\": \"Madera\",\n",
    "            \"7\": \"Estera\",\n",
    "            \"8\": \"Otro material\"\n",
    "        }\n",
    "        \n",
    "        # Aplicar el mapeo\n",
    "        subsample_vivienda['P206_modificado'] = subsample_vivienda['P206'].map(mapeo_p206)\n",
    "        \n",
    "        print(\"Valores modificados en P206_modificado:\")\n",
    "        print(subsample_vivienda['P206_modificado'].value_counts())\n",
    "    \n",
    "    # C. Additional modification: Convertir P203 y P204 a numéricas\n",
    "    if 'P203' in subsample_vivienda.columns:\n",
    "        print(f\"\\nC. Convirtiendo P203 a numérico\")\n",
    "        # Algunos valores podrían ser strings, así que los convertimos a numéricos, forzando los no numéricos a NaN\n",
    "        subsample_vivienda['P203'] = pd.to_numeric(subsample_vivienda['P203'], errors='coerce')\n",
    "        print(f\"Tipo de dato de P203 después del cambio: {subsample_vivienda['P203'].dtype}\")\n",
    "    \n",
    "    if 'P204' in subsample_vivienda.columns:\n",
    "        print(f\"\\nD. Convirtiendo P204 a numérico\")\n",
    "        subsample_vivienda['P204'] = pd.to_numeric(subsample_vivienda['P204'], errors='coerce')\n",
    "        print(f\"Tipo de dato de P204 después del cambio: {subsample_vivienda['P204'].dtype}\")\n",
    "    \n",
    "    # Mostrar el resultado final\n",
    "    print(\"\\nSubconjunto después de las modificaciones:\")\n",
    "    print(subsample_vivienda.head())\n",
    "    print(\"\\nTipos de datos después de las modificaciones:\")\n",
    "    print(subsample_vivienda.dtypes)\n",
    "    \n",
    "    # Guardar el subconjunto modificado\n",
    "    subsample_vivienda.to_csv(\"subsample_vivienda_modificado.csv\", index=False, encoding='utf-8')\n",
    "    print(\"\\nSubconjunto modificado guardado como 'subsample_vivienda_modificado.csv'\")\n",
    "    \n",
    "else:\n",
    "    print(\"No se pudo realizar el análisis del dataset de vivienda debido a problemas con la importación.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da862cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b9a955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
