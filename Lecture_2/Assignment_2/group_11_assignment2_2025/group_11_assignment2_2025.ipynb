{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830a3a5d",
   "metadata": {},
   "source": [
    "# Assignmet 2\n",
    "# Part 1 – Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b2d9b",
   "metadata": {},
   "source": [
    "## 1. Set your working directory and import the dataset Enaho01A-2023-300.csv using Pandas.\n",
    "\n",
    "Note: Consider the file encoding (UTF-8 or ISO-8859-10).\n",
    "Example: df = pd.read_csv(\"datos.csv\", encoding=\"ISO-8859-10\")\n",
    "\n",
    "Read and display the first 5 rows.\n",
    "Convert the column names into a list and print it.\n",
    "Check the data types of the DataFrame.\n",
    "Select a subsample containing the variables ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO'] and between 3–5 additional variables of your interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec7be1",
   "metadata": {},
   "source": [
    "*Configuración del Directorio de Trabajo usando Rutas Relativas con ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02fe722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta absoluta calculada: C:\\Users\\Usuario\\Documents\\GitHub\\assingment_2\\data\n",
      "Error: No se encuentra la ruta: C:\\Users\\Usuario\\Documents\\GitHub\\assingment_2\\data\n",
      "Por favor, verifica la estructura de carpetas.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Configurar el directorio de trabajo usando rutas relativas con ../\n",
    "# Para subir 4 niveles desde tu ubicación actual y luego bajar a assingment_2/data\n",
    "relative_path = \"../../../../assingment_2/data\"\n",
    "\n",
    "# 2. Convertir la ruta relativa a una ruta absoluta\n",
    "absolute_path = os.path.abspath(relative_path)\n",
    "print(f\"Ruta absoluta calculada: {absolute_path}\")\n",
    "\n",
    "# 3. Verificar si la ruta existe\n",
    "if not os.path.exists(absolute_path):\n",
    "    print(f\"Error: No se encuentra la ruta: {absolute_path}\")\n",
    "    print(\"Por favor, verifica la estructura de carpetas.\")\n",
    "else:\n",
    "    # 4. Establecer el directorio de trabajo\n",
    "    os.chdir(absolute_path)\n",
    "    print(f\"Directorio de trabajo establecido en: {os.getcwd()}\")\n",
    "    \n",
    "    # 5. Importar el dataset Enaho01A-2023-300.csv\n",
    "    try:\n",
    "        # Intentar primero con ISO-8859-10 como se indica en la nota\n",
    "        df = pd.read_csv(\"Enaho01A-2023-300.csv\", encoding=\"ISO-8859-10\", low_memory=False)\n",
    "        print(\"Dataset importado con encoding ISO-8859-10\")\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            # Si ISO-8859-10 falla, intentar con UTF-8\n",
    "            df = pd.read_csv(\"Enaho01A-2023-300.csv\", encoding=\"UTF-8\", low_memory=False)\n",
    "            print(\"Dataset importado con encoding UTF-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: No se pudo importar el dataset: {e}\")\n",
    "            df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d841b",
   "metadata": {},
   "source": [
    "*realizar las operaciones solicitadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f840ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 5 filas del dataset:\n",
      "    AŅO  MES  CONGLOME  VIVIENDA  HOGAR  CODPERSO  UBIGEO  DOMINIO  ESTRATO  \\\n",
      "0  2023    1      5030         2     11         1   10201        7        4   \n",
      "1  2023    1      5030         2     11         2   10201        7        4   \n",
      "2  2023    1      5030         2     11         3   10201        7        4   \n",
      "3  2023    1      5030         2     11         4   10201        7        4   \n",
      "4  2023    1      5030        11     11         1   10201        7        4   \n",
      "\n",
      "   CODINFOR  ...  I311D$5  I311D$6  I311D$7  I3121C I3122C I315B    FACTOR07  \\\n",
      "0         1  ...                                                  118.374542   \n",
      "1         2  ...                                                  118.374542   \n",
      "2         2  ...                                                  118.374542   \n",
      "3         2  ...                          8                       118.374542   \n",
      "4         1  ...                                                  118.374542   \n",
      "\n",
      "    FACTORA07 NCONGLOME SUB_CONGLOME  \n",
      "0  165.623856      6618            0  \n",
      "1  112.328087      6618            0  \n",
      "2  120.091476      6618            0  \n",
      "3  110.974678      6618            0  \n",
      "4   96.035370      6618            0  \n",
      "\n",
      "[5 rows x 511 columns]\n",
      "\n",
      "Nombres de columnas como lista:\n",
      "['AŅO', 'MES', 'CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO', 'UBIGEO', 'DOMINIO', 'ESTRATO', 'CODINFOR', 'P300N', 'P300I', 'P300A', 'P301A', 'P301B', 'P301C', 'P301D', 'P301A0', 'P301A1', 'P301B0', 'P301B1', 'P301B3', 'P302', 'P302X', 'P302A', 'P302B', 'P303', 'P304A', 'P304B', 'P304C', 'P304D', 'P305', 'P306', 'P307', 'P307A1', 'P307A2', 'P307A3', 'P307A4', 'P307A4_5', 'P307A4_6', 'P307A4_7', 'P307B1', 'P307B2', 'P307B3', 'P307B4', 'P307B4_5', 'P307B4_6', 'P307B4_7', 'P307C', 'P308A', 'P308B', 'P308C', 'P308D', 'P308B1', 'P308B2', 'P308B3', 'P308B4', 'P308B5', 'P308C1', 'P308C2', 'P310', 'P310B1', 'P310C0', 'P310C1', 'P310D1', 'P310D2', 'P310E0', 'P310E1', 'P310E3', 'P311N$1', 'P311N$2', 'P311N$3', 'P311N$4', 'P311N$5', 'P311N$6', 'P311N$7', 'P311N$8', 'P311N$9', 'P311$1', 'P311$2', 'P311$3', 'P311$4', 'P311$5', 'P311$6', 'P311$7', 'P311$8', 'P311$9', 'P311A1$1', 'P311A1$2', 'P311A1$3', 'P311A1$4', 'P311A1$5', 'P311A1$6', 'P311A1$7', 'P311A1$8', 'P311A1$9', 'P311A2$1', 'P311A2$2', 'P311A2$3', 'P311A2$4', 'P311A2$5', 'P311A2$6', 'P311A2$7', 'P311A2$8', 'P311A2$9', 'P311A3$1', 'P311A3$2', 'P311A3$3', 'P311A3$4', 'P311A3$5', 'P311A3$6', 'P311A3$7', 'P311A3$8', 'P311A3$9', 'P311A4$1', 'P311A4$2', 'P311A4$3', 'P311A4$4', 'P311A4$5', 'P311A4$6', 'P311A4$7', 'P311A4$8', 'P311A4$9', 'P311A5$1', 'P311A5$2', 'P311A5$3', 'P311A5$4', 'P311A5$5', 'P311A5$6', 'P311A5$7', 'P311A5$8', 'P311A5$9', 'P311A6$1', 'P311A6$2', 'P311A6$3', 'P311A6$4', 'P311A6$5', 'P311A6$6', 'P311A6$7', 'P311A6$8', 'P311A6$9', 'P311A7$1', 'P311A7$2', 'P311A7$3', 'P311A7$4', 'P311A7$5', 'P311A7$6', 'P311A7$7', 'P311A7$8', 'P311A7$9', 'P311B$1', 'P311B$2', 'P311B$3', 'P311B$4', 'P311B$5', 'P311B$6', 'P311B$7', 'P311B$8', 'P311B$9', 'P311C$1', 'P311C$2', 'P311C$3', 'P311C$4', 'P311C$5', 'P311C$6', 'P311C$7', 'P311C$8', 'P311C$9', 'P311D$1', 'P311D$2', 'P311D$3', 'P311D$4', 'P311D$5', 'P311D$6', 'P311D$7', 'P311D$8', 'P311D$9', 'P311D2$1', 'P311D2$2', 'P311D2$3', 'P311D2$4', 'P311D2$5', 'P311D2$6', 'P311D2$7', 'P311D2$8', 'P311D2$9', 'P311D3$1', 'P311D3$2', 'P311D3$3', 'P311D3$4', 'P311D3$5', 'P311D3$6', 'P311D3$7', 'P311D3$8', 'P311D3$9', 'P311D4$1', 'P311D4$2', 'P311D4$3', 'P311D4$4', 'P311D4$5', 'P311D4$6', 'P311D4$7', 'P311D4$8', 'P311D4$9', 'P311D5$1', 'P311D5$2', 'P311D5$3', 'P311D5$4', 'P311D5$5', 'P311D5$6', 'P311D5$7', 'P311D5$8', 'P311D5$9', 'P311D6$1', 'P311D6$2', 'P311D6$3', 'P311D6$4', 'P311D6$5', 'P311D6$6', 'P311D6$7', 'P311D6$8', 'P311D6$9', 'P311D7$1', 'P311D7$2', 'P311D7$3', 'P311D7$4', 'P311D7$5', 'P311D7$6', 'P311D7$7', 'P311D7$8', 'P311D7$9', 'P311E$1', 'P311E$2', 'P311E$3', 'P311E$4', 'P311E$5', 'P311E$6', 'P311E$7', 'P311E$8', 'P311E$9', 'P311T1', 'P311T22', 'P311T23', 'P311T24', 'P311T25', 'P311T26', 'P311T27', 'P311T2', 'P3121', 'P3121A1', 'P3121A2', 'P3121A3', 'P3121A4', 'P3121A5', 'P3121A6', 'P3121B', 'P3121C', 'P3121C2', 'P3121C3', 'P3121C4', 'P3121C5', 'P3121C6', 'P3121D', 'P3122', 'P3122A1', 'P3122A2', 'P3122A3', 'P3122A4', 'P3122A5', 'P3122A6', 'P3122B', 'P3122C', 'P3122C2', 'P3122C3', 'P3122C4', 'P3122C5', 'P3122C6', 'P3122D', 'P312T1', 'P312T22', 'P312T23', 'P312T24', 'P312T25', 'P312T26', 'P312T2', 'P313', 'P314A', 'P314B$1', 'P314B$2', 'P314B$3', 'P314B$4', 'P314B$5', 'P314B$6', 'P314B$7', 'P314B1_1', 'P314B1_2', 'P314B1_6', 'P314B1_7', 'P314B1_8', 'P314B1_9', 'P314D', 'P3151', 'P3152', 'P3153', 'P3154', 'P3155', 'P3156', 'P315A', 'P315B', 'P315B2', 'P315B3', 'P315B4', 'P315B5', 'P315B6', 'P316$1', 'P316$2', 'P316$3', 'P316$4', 'P316$5', 'P316$6', 'P316$7', 'P316$8', 'P316$9', 'P316$10', 'P316$11', 'P316$12', 'P316A1', 'P316A2', 'P316A3', 'P316A4', 'P316A5', 'P316A6', 'P316B', 'P316C1', 'P316C2', 'P316C3', 'P316C4', 'P316C5', 'P316C6', 'P316C7', 'P316C8', 'P316C9', 'P316C10', 'T313A', 'P203', 'P204', 'P205', 'P206', 'P207', 'P208A', 'P209', 'IMPUTADO', 'TICUEST01A', 'D311B$1', 'D311D2$1', 'D311D3$1', 'D311D4$1', 'D311D5$1', 'D311D6$1', 'D311D7$1', 'D311D$1', 'D311B$2', 'D311D2$2', 'D311D3$2', 'D311D4$2', 'D311D5$2', 'D311D6$2', 'D311D7$2', 'D311D$2', 'D311B$3', 'D311D2$3', 'D311D3$3', 'D311D4$3', 'D311D5$3', 'D311D6$3', 'D311D7$3', 'D311D$3', 'D311B$4', 'D311D2$4', 'D311D3$4', 'D311D4$4', 'D311D5$4', 'D311D6$4', 'D311D7$4', 'D311D$4', 'D311B$5', 'D311D2$5', 'D311D3$5', 'D311D4$5', 'D311D5$5', 'D311D6$5', 'D311D7$5', 'D311D$5', 'D311B$6', 'D311D2$6', 'D311D3$6', 'D311D4$6', 'D311D5$6', 'D311D6$6', 'D311D7$6', 'D311D$6', 'D311B$7', 'D311D2$7', 'D311D3$7', 'D311D4$7', 'D311D5$7', 'D311D6$7', 'D311D7$7', 'D311D$7', 'D3121B', 'D3121C2', 'D3121C3', 'D3121C4', 'D3121C5', 'D3121C6', 'D3121C', 'D3122B', 'D3122C2', 'D3122C3', 'D3122C4', 'D3122C5', 'D3122C6', 'D3122C', 'D315A', 'D315B2', 'D315B3', 'D315B4', 'D315B5', 'D315B6', 'D315B', 'I311B$1', 'I311B$2', 'I311B$4', 'I311B$6', 'I311B$3', 'I311B$5', 'I311B$7', 'I311D2$1', 'I311D3$1', 'I311D4$1', 'I311D5$1', 'I311D6$1', 'I311D7$1', 'I311D2$2', 'I311D3$2', 'I311D4$2', 'I311D5$2', 'I311D6$2', 'I311D7$2', 'I311D2$4', 'I311D3$4', 'I311D4$4', 'I311D5$4', 'I311D6$4', 'I311D7$4', 'I311D2$6', 'I311D3$6', 'I311D4$6', 'I311D5$6', 'I311D6$6', 'I311D7$6', 'I311D2$3', 'I311D3$3', 'I311D4$3', 'I311D5$3', 'I311D6$3', 'I311D7$3', 'I311D2$5', 'I311D3$5', 'I311D4$5', 'I311D5$5', 'I311D6$5', 'I311D7$5', 'I311D2$7', 'I311D3$7', 'I311D4$7', 'I311D5$7', 'I311D6$7', 'I311D7$7', 'I3121B', 'I3122B', 'I3121C2', 'I3121C3', 'I3121C4', 'I3121C5', 'I3121C6', 'I3122C2', 'I3122C3', 'I3122C4', 'I3122C5', 'I3122C6', 'I315A', 'I315B2', 'I315B3', 'I315B4', 'I315B5', 'I315B6', 'I311D$1', 'I311D$2', 'I311D$3', 'I311D$4', 'I311D$5', 'I311D$6', 'I311D$7', 'I3121C', 'I3122C', 'I315B', 'FACTOR07', 'FACTORA07', 'NCONGLOME', 'SUB_CONGLOME']\n",
      "\n",
      "Tipos de datos del DataFrame:\n",
      "AŅO               int64\n",
      "MES               int64\n",
      "CONGLOME          int64\n",
      "VIVIENDA          int64\n",
      "HOGAR             int64\n",
      "                 ...   \n",
      "I315B            object\n",
      "FACTOR07        float64\n",
      "FACTORA07       float64\n",
      "NCONGLOME         int64\n",
      "SUB_CONGLOME      int64\n",
      "Length: 511, dtype: object\n",
      "\n",
      "Subconjunto seleccionado con 8 variables:\n",
      "   CONGLOME  VIVIENDA  HOGAR  CODPERSO  P300A  P301A P302A P304A\n",
      "0      5030         2     11         1      4      8            \n",
      "1      5030         2     11         2      4     10            \n",
      "2      5030         2     11         3      4      3           2\n",
      "3      5030         2     11         4      8      3           2\n",
      "4      5030        11     11         1      4      4            \n",
      "\n",
      "Dimensiones del subconjunto: (108354, 8)\n",
      "\n",
      "Tipos de datos del subconjunto:\n",
      "CONGLOME     int64\n",
      "VIVIENDA     int64\n",
      "HOGAR        int64\n",
      "CODPERSO     int64\n",
      "P300A        int64\n",
      "P301A        int64\n",
      "P302A       object\n",
      "P304A       object\n",
      "dtype: object\n",
      "\n",
      "Subconjunto con nombres de variables renombrados:\n",
      "   CONGLOME  VIVIENDA  HOGAR  CODPERSO  lengua_materna  nivel_educativo  \\\n",
      "0      5030         2     11         1               4                8   \n",
      "1      5030         2     11         2               4               10   \n",
      "2      5030         2     11         3               4                3   \n",
      "3      5030         2     11         4               8                3   \n",
      "4      5030        11     11         1               4                4   \n",
      "\n",
      "  programa_alfabetizacion nivel_asistencia_actual  \n",
      "0                                                  \n",
      "1                                                  \n",
      "2                                               2  \n",
      "3                                               2  \n",
      "4                                                  \n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    # a. Leer y mostrar las primeras 5 filas\n",
    "    print(\"\\nPrimeras 5 filas del dataset:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # b. Convertir los nombres de columna a una lista e imprimirla\n",
    "    column_names = df.columns.tolist()\n",
    "    print(\"\\nNombres de columnas como lista:\")\n",
    "    print(column_names)\n",
    "    \n",
    "    # c. Verificar los tipos de datos del DataFrame\n",
    "    print(\"\\nTipos de datos del DataFrame:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # d. Seleccionar un subconjunto con las variables requeridas\n",
    "    variables_base = ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO']\n",
    "    \n",
    "    # Seleccionar 4 variables adicionales de interés educativo\n",
    "    variables_adicionales = [\n",
    "        'P300A',  # Idioma o lengua materna\n",
    "        'P301A',  # Último año o grado de estudios y nivel que aprobó - Nivel\n",
    "        'P302A',  # En los últimos 12 meses, ¿Recibió programa de alfabetización?\n",
    "        'P304A'   # ¿Cuál es el grado o año de estudios al que asistió el año pasado? - Nivel\n",
    "    ]\n",
    "    \n",
    "    # Verificar que las variables existan en el dataset\n",
    "    variables_existentes = [col for col in variables_base + variables_adicionales if col in df.columns]\n",
    "    \n",
    "    # Crear el subconjunto\n",
    "    subsample = df[variables_existentes]\n",
    "    \n",
    "    print(f\"\\nSubconjunto seleccionado con {len(variables_existentes)} variables:\")\n",
    "    print(subsample.head())\n",
    "    \n",
    "    # Mostrar información sobre el subconjunto\n",
    "    print(f\"\\nDimensiones del subconjunto: {subsample.shape}\")\n",
    "    print(\"\\nTipos de datos del subconjunto:\")\n",
    "    print(subsample.dtypes)\n",
    "     # 7. Renombrar las variables adicionales para mayor claridad\n",
    "    nuevos_nombres = {\n",
    "        'P300A': 'lengua_materna',\n",
    "        'P301A': 'nivel_educativo',\n",
    "        'P302A': 'programa_alfabetizacion',\n",
    "        'P304A': 'nivel_asistencia_actual'\n",
    "    }\n",
    "    \n",
    "    subsample = subsample.rename(columns=nuevos_nombres)\n",
    "    \n",
    "    print(\"\\nSubconjunto con nombres de variables renombrados:\")\n",
    "    print(subsample.head())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe9951",
   "metadata": {},
   "source": [
    "## 2. Data Manipulation (Data Cleaning):\n",
    "Explore the DataFrame using summary functions.\n",
    "Identify if there are missing values.\n",
    "If they exist, remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c670aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPLORACIÓN INICIAL DEL DATAFRAME ===\n",
      "\n",
      "1. INFORMACIÓN GENERAL DEL DATAFRAME:\n",
      "Dimensiones: (108354, 511) (filas, columnas)\n",
      "Número total de datos: 55368894\n",
      "\n",
      "2. RESUMEN ESTADÍSTICO DE VARIABLES NUMÉRICAS:\n",
      "            AŅO            MES       CONGLOME       VIVIENDA          HOGAR  \\\n",
      "count  108354.0  108354.000000  108354.000000  108354.000000  108354.000000   \n",
      "mean     2023.0       6.495127   16944.756797      77.820330      11.146271   \n",
      "std         0.0       3.445244    3144.386733      68.547022       1.370084   \n",
      "min      2023.0       1.000000    5007.000000       1.000000      11.000000   \n",
      "25%      2023.0       3.000000   16028.000000      31.000000      11.000000   \n",
      "50%      2023.0       7.000000   17500.000000      66.000000      11.000000   \n",
      "75%      2023.0       9.000000   19014.000000     106.000000      11.000000   \n",
      "max      2023.0      12.000000   21001.000000     991.000000      44.000000   \n",
      "\n",
      "            CODPERSO         UBIGEO        DOMINIO        ESTRATO  \\\n",
      "count  108354.000000  108354.000000  108354.000000  108354.000000   \n",
      "mean        2.546293  131185.171087       4.864961       4.138961   \n",
      "std         1.577737   67736.903764       2.404443       2.432348   \n",
      "min         1.000000   10101.000000       1.000000       1.000000   \n",
      "25%         1.000000   80101.000000       2.000000       2.000000   \n",
      "50%         2.000000  140108.000000       5.000000       4.000000   \n",
      "75%         3.000000  180301.000000       7.000000       7.000000   \n",
      "max        22.000000  250401.000000       8.000000       8.000000   \n",
      "\n",
      "            CODINFOR  ...          P301A           P203           P204  \\\n",
      "count  108354.000000  ...  108354.000000  108354.000000  108354.000000   \n",
      "mean        2.033058  ...       5.258652       2.548729       1.005787   \n",
      "std         1.282622  ...       3.807669       1.716218       0.075850   \n",
      "min         0.000000  ...       1.000000       1.000000       1.000000   \n",
      "25%         1.000000  ...       3.000000       1.000000       1.000000   \n",
      "50%         2.000000  ...       5.000000       3.000000       1.000000   \n",
      "75%         2.000000  ...       6.000000       3.000000       1.000000   \n",
      "max        21.000000  ...      99.000000      11.000000       2.000000   \n",
      "\n",
      "                P207          P208A     TICUEST01A       FACTOR07  \\\n",
      "count  108354.000000  108354.000000  108354.000000  108354.000000   \n",
      "mean        1.513428      35.619839       1.999539     305.556281   \n",
      "std         0.499822      22.152408       0.021477     310.795722   \n",
      "min         1.000000       3.000000       1.000000       1.087094   \n",
      "25%         1.000000      16.000000       2.000000     123.659256   \n",
      "50%         2.000000      33.000000       2.000000     233.634140   \n",
      "75%         2.000000      53.000000       2.000000     354.661774   \n",
      "max         2.000000      98.000000       2.000000    1961.005981   \n",
      "\n",
      "           FACTORA07      NCONGLOME   SUB_CONGLOME  \n",
      "count  108354.000000  108354.000000  108354.000000  \n",
      "mean      302.602025   23603.093914       0.282325  \n",
      "std       322.927385   15607.753763       0.662107  \n",
      "min         0.422172       2.000000       0.000000  \n",
      "25%       101.125488    8855.000000       0.000000  \n",
      "50%       218.866760   20186.000000       0.000000  \n",
      "75%       365.609833   37361.000000       0.000000  \n",
      "max      2917.495850   51561.000000       6.000000  \n",
      "\n",
      "[8 rows x 23 columns]\n",
      "\n",
      "3. INFORMACIÓN SOBRE TIPOS DE DATOS Y VALORES NO NULOS:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108354 entries, 0 to 108353\n",
      "Columns: 511 entries, AŅO to SUB_CONGLOME\n",
      "dtypes: float64(2), int64(21), object(488)\n",
      "memory usage: 422.4+ MB\n",
      "None\n",
      "\n",
      "=== IDENTIFICACIÓN DE VALORES MISSING ===\n",
      "\n",
      "Valores missing por columna:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Porcentaje de valores missing por columna:\n",
      "Series([], dtype: float64)\n",
      "\n",
      "=== MANEJO DE VALORES MISSING ===\n",
      "\n",
      "Valores missing en variables clave:\n",
      "CONGLOME    0\n",
      "VIVIENDA    0\n",
      "HOGAR       0\n",
      "CODPERSO    0\n",
      "P300A       0\n",
      "P301A       0\n",
      "P302A       0\n",
      "P304A       0\n",
      "dtype: int64\n",
      "\n",
      "Filas eliminadas por valores missing en variables de identificación: 0\n",
      "Filas restantes: 108354\n",
      "\n",
      "=== VALORES CODIFICADOS COMO MISSING ===\n",
      "\n",
      "Valores codificados como missing en P300A: 105\n",
      "Valores codificados como missing en P301A: 105\n",
      "Valores codificados como missing en P302A: 0\n",
      "Valores codificados como missing en P304A: 0\n",
      "\n",
      "=== ESTADO DESPUÉS DE LA LIMPIEZA ===\n",
      "\n",
      "Valores missing después de la limpieza:\n",
      "P300A    105\n",
      "P301A    105\n",
      "dtype: int64\n",
      "Imputados 0 valores missing en P300A con la mediana: 4.0\n",
      "Imputados 0 valores missing en P301A con la mediana: 5.0\n",
      "\n",
      "=== VERIFICACIÓN FINAL ===\n",
      "\n",
      "Valores missing restantes en todo el DataFrame: 0\n",
      "\n",
      "Dimensiones del dataset después de la limpieza: (108354, 511)\n",
      "\n",
      "Dataset limpio guardado como 'enaho_educacion_clean.csv'\n",
      "\n",
      "Información del dataset limpio:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108354 entries, 0 to 108353\n",
      "Columns: 511 entries, AŅO to SUB_CONGLOME\n",
      "dtypes: float64(4), int64(19), object(488)\n",
      "memory usage: 422.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    # 1. Explorar el DataFrame usando funciones de resumen\n",
    "    print(\"\\n=== EXPLORACIÓN INICIAL DEL DATAFRAME ===\\n\")\n",
    "    \n",
    "    # Información general del DataFrame\n",
    "    print(\"1. INFORMACIÓN GENERAL DEL DATAFRAME:\")\n",
    "    print(f\"Dimensiones: {df.shape} (filas, columnas)\")\n",
    "    print(f\"Número total de datos: {df.size}\")\n",
    "    \n",
    "    # Resumen estadístico de las variables numéricas\n",
    "    print(\"\\n2. RESUMEN ESTADÍSTICO DE VARIABLES NUMÉRICAS:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Información sobre tipos de datos y valores no nulos\n",
    "    print(\"\\n3. INFORMACIÓN SOBRE TIPOS DE DATOS Y VALORES NO NULOS:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # 2. Identificar valores missing (faltantes)\n",
    "    print(\"\\n=== IDENTIFICACIÓN DE VALORES MISSING ===\\n\")\n",
    "    \n",
    "    # Contar valores missing por columna\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"Valores missing por columna:\")\n",
    "    print(missing_values[missing_values > 0])  # Mostrar solo columnas con valores missing\n",
    "    \n",
    "    # Porcentaje de valores missing por columna\n",
    "    missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "    print(\"\\nPorcentaje de valores missing por columna:\")\n",
    "    print(missing_percentage[missing_percentage > 0])  # Mostrar solo columnas con valores missing\n",
    "    \n",
    "    # 3. Manejo de valores missing\n",
    "    print(\"\\n=== MANEJO DE VALORES MISSING ===\\n\")\n",
    "    \n",
    "    # Estrategia para manejar valores missing:\n",
    "    # - Para variables categóricas: Podemos usar una categoría específica para missing values\n",
    "    # - Para variables numéricas: Podemos imputar con la media, mediana o eliminar\n",
    "    \n",
    "    # Primero, verificar si hay valores missing en las variables clave\n",
    "    key_variables = ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO', 'P300A', 'P301A', 'P302A', 'P304A']\n",
    "    key_missing = df[key_variables].isnull().sum()\n",
    "    \n",
    "    print(\"Valores missing en variables clave:\")\n",
    "    print(key_missing)\n",
    "    \n",
    "    # Eliminar filas con valores missing en variables clave (si es necesario)\n",
    "    # Nota: En datasets de encuestas, a veces es mejor no eliminar filas ya que cada una representa una persona/hogar\n",
    "    \n",
    "    # Para este ejemplo, eliminaremos filas donde todas las variables clave tienen valores missing\n",
    "    # Pero primero, verifiquemos cuántas filas serían eliminadas\n",
    "    rows_before = len(df)\n",
    "    \n",
    "    # Eliminar filas donde las variables de identificación tienen valores missing\n",
    "    # (Estas no deberían tener valores missing en un dataset bien construido)\n",
    "    df_clean = df.dropna(subset=['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO'], how='any')\n",
    "    \n",
    "    rows_after = len(df_clean)\n",
    "    rows_removed = rows_before - rows_after\n",
    "    \n",
    "    print(f\"\\nFilas eliminadas por valores missing en variables de identificación: {rows_removed}\")\n",
    "    print(f\"Filas restantes: {rows_after}\")\n",
    "    \n",
    "    # Para variables categóricas con valores missing, podemos asignar una categoría específica\n",
    "    # Basado en el diccionario de datos, sabemos que los valores missing están codificados como 99 o 9\n",
    "    \n",
    "    # 4. Verificar valores específicos codificados como missing (según el diccionario de datos)\n",
    "    print(\"\\n=== VALORES CODIFICADOS COMO MISSING ===\\n\")\n",
    "    \n",
    "    # Definir los valores que representan missing según el diccionario\n",
    "    missing_codes = {\n",
    "        'P300A': [99],  # Lengua materna\n",
    "        'P301A': [99],  # Nivel educativo\n",
    "        'P302A': [9],   # Programa de alfabetización\n",
    "        'P304A': [9]    # Nivel de asistencia actual\n",
    "    }\n",
    "    \n",
    "    # Contar valores codificados como missing\n",
    "    for column, codes in missing_codes.items():\n",
    "        if column in df_clean.columns:\n",
    "            count = df_clean[column].isin(codes).sum()\n",
    "            print(f\"Valores codificados como missing en {column}: {count}\")\n",
    "            \n",
    "            # Reemplazar valores codificados con NaN\n",
    "            df_clean[column] = df_clean[column].replace(codes, float('nan'))\n",
    "    \n",
    "    # 5. Verificar el estado después de la limpieza\n",
    "    print(\"\\n=== ESTADO DESPUÉS DE LA LIMPIEZA ===\\n\")\n",
    "    \n",
    "    # Valores missing después de la limpieza\n",
    "    missing_after = df_clean.isnull().sum()\n",
    "    print(\"Valores missing después de la limpieza:\")\n",
    "    print(missing_after[missing_after > 0])\n",
    "    \n",
    "    # Para variables numéricas, podemos imputar valores missing con la mediana\n",
    "    numeric_columns = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            median_value = df_clean[col].median()\n",
    "            df_clean[col].fillna(median_value, inplace=True)\n",
    "            print(f\"Imputados {df_clean[col].isnull().sum()} valores missing en {col} con la mediana: {median_value}\")\n",
    "    \n",
    "    # Para variables categóricas, podemos imputar con la moda o asignar una categoría \"Desconocido\"\n",
    "    categorical_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            mode_value = df_clean[col].mode()[0]\n",
    "            df_clean[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"Imputados {df_clean[col].isnull().sum()} valores missing en {col} con la moda: {mode_value}\")\n",
    "    \n",
    "    # 6. Verificación final\n",
    "    print(\"\\n=== VERIFICACIÓN FINAL ===\\n\")\n",
    "    \n",
    "    # Comprobar si aún hay valores missing\n",
    "    remaining_missing = df_clean.isnull().sum().sum()\n",
    "    print(f\"Valores missing restantes en todo el DataFrame: {remaining_missing}\")\n",
    "    \n",
    "    # Resumen del dataset limpio\n",
    "    print(f\"\\nDimensiones del dataset después de la limpieza: {df_clean.shape}\")\n",
    "    \n",
    "    # Guardar el dataset limpio (opcional)\n",
    "    df_clean.to_csv(\"enaho_educacion_clean.csv\", index=False, encoding='utf-8')\n",
    "    print(\"\\nDataset limpio guardado como 'enaho_educacion_clean.csv'\")\n",
    "    \n",
    "    # Mostrar información del dataset limpio\n",
    "    print(\"\\nInformación del dataset limpio:\")\n",
    "    print(df_clean.info())\n",
    "    \n",
    "else:\n",
    "    print(\"No se pudo realizar la limpieza de datos debido a problemas con la importación del dataset.\")\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545db414",
   "metadata": {},
   "source": [
    "## 3. Import a second dataset (choose between Enaho01A-2023-200.csv or Enaho01A-2023-500.csv).\n",
    "Display the first 5 rows.\n",
    "Convert the column names into a list and print it.\n",
    "Check the data types.\n",
    "Select a subsample containing the variables ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO'] and between 3–5 additional variables of your interest.\n",
    "Perform the following modifications:\n",
    "A. Change the data type of a variable (e.g., from text to numeric).\n",
    "B. Modify some values in a specific column.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1566b38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de vivienda importado con encoding ISO-8859-10\n",
      "\n",
      "Primeras 5 filas del dataset de vivienda:\n",
      "    AŅO  MES  CONGLOME  VIVIENDA  HOGAR  CODPERSO  UBIGEO  DOMINIO  ESTRATO  \\\n",
      "0  2023    2      5007        22     11         1   10101        4        4   \n",
      "1  2023    2      5007        22     11         2   10101        4        4   \n",
      "2  2023    2      5007        22     11         3   10101        4        4   \n",
      "3  2023    2      5007        31     11         1   10101        4        4   \n",
      "4  2023    2      5007        31     11         2   10101        4        4   \n",
      "\n",
      "               P201P  ...  OCUPAC_R3 OCUPAC_R4 RAMA_R3 RAMA_R4 CODTAREA  \\\n",
      "0  20190050070221101  ...                                                 \n",
      "1  20190050070221102  ...                                                 \n",
      "2  20190050070221104  ...                                                 \n",
      "3  20190050070311102  ...                                                 \n",
      "4  20230050070311102  ...                                                 \n",
      "\n",
      "  CODTIEMPO TICUEST01   FACPOB07 NCONGLOME SUB_CONGLOME  \n",
      "0                   2  50.466671      7070            0  \n",
      "1                   2  50.466671      7070            0  \n",
      "2                   2  50.466671      7070            0  \n",
      "3                   2  50.466671      7070            0  \n",
      "4                   2  50.466671      7070            0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "\n",
      "Nombres de columnas del dataset de vivienda:\n",
      "['AŅO', 'MES', 'CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO', 'UBIGEO', 'DOMINIO', 'ESTRATO', 'P201P', 'P203', 'P203A', 'P203B', 'P204', 'P205', 'P206', 'P207', 'P208A', 'P208B', 'P209', 'P210', 'P211A', 'P211D', 'P212', 'P213', 'P214', 'P215', 'P216', 'P217', 'T211', 'OCUPAC_R3', 'OCUPAC_R4', 'RAMA_R3', 'RAMA_R4', 'CODTAREA', 'CODTIEMPO', 'TICUEST01', 'FACPOB07', 'NCONGLOME', 'SUB_CONGLOME']\n",
      "\n",
      "Tipos de datos del dataset de vivienda:\n",
      "AŅO               int64\n",
      "MES               int64\n",
      "CONGLOME          int64\n",
      "VIVIENDA          int64\n",
      "HOGAR             int64\n",
      "CODPERSO          int64\n",
      "UBIGEO            int64\n",
      "DOMINIO           int64\n",
      "ESTRATO           int64\n",
      "P201P             int64\n",
      "P203              int64\n",
      "P203A            object\n",
      "P203B            object\n",
      "P204             object\n",
      "P205             object\n",
      "P206             object\n",
      "P207             object\n",
      "P208A            object\n",
      "P208B            object\n",
      "P209             object\n",
      "P210             object\n",
      "P211A            object\n",
      "P211D            object\n",
      "P212             object\n",
      "P213             object\n",
      "P214             object\n",
      "P215             object\n",
      "P216             object\n",
      "P217             object\n",
      "T211             object\n",
      "OCUPAC_R3        object\n",
      "OCUPAC_R4        object\n",
      "RAMA_R3          object\n",
      "RAMA_R4          object\n",
      "CODTAREA         object\n",
      "CODTIEMPO        object\n",
      "TICUEST01         int64\n",
      "FACPOB07        float64\n",
      "NCONGLOME         int64\n",
      "SUB_CONGLOME      int64\n",
      "dtype: object\n",
      "\n",
      "Subconjunto seleccionado con 9 variables:\n",
      "   CONGLOME  VIVIENDA  HOGAR  CODPERSO  P203 P204 P205 P206 P207\n",
      "0      5007        22     11         1     1    1    2         1\n",
      "1      5007        22     11         2     2    1    2         2\n",
      "2      5007        22     11         3     3    1    2         1\n",
      "3      5007        31     11         1     1    1    2         2\n",
      "4      5007        31     11         2     3    2         2    1\n",
      "\n",
      "=== MODIFICACIONES AL SUBCONJUNTO ===\n",
      "\n",
      "Tipos de datos antes de las modificaciones:\n",
      "CONGLOME     int64\n",
      "VIVIENDA     int64\n",
      "HOGAR        int64\n",
      "CODPERSO     int64\n",
      "P203         int64\n",
      "P204        object\n",
      "P205        object\n",
      "P206        object\n",
      "P207        object\n",
      "dtype: object\n",
      "\n",
      "A. Cambiando tipo de dato de P205 de object a category\n",
      "Tipo de dato de P205 después del cambio: category\n",
      "\n",
      "B. Modificando valores en la columna P206 (Material de las paredes):\n",
      "Valores originales en P206:\n",
      "P206\n",
      "     115340\n",
      "2      3730\n",
      "1       677\n",
      "Name: count, dtype: int64\n",
      "Valores modificados en P206_modificado:\n",
      "P206_modificado\n",
      "Adobe o tapia                   3730\n",
      "Ladrillo o bloque de cemento     677\n",
      "Name: count, dtype: int64\n",
      "\n",
      "C. Convirtiendo P203 a numérico\n",
      "Tipo de dato de P203 después del cambio: int64\n",
      "\n",
      "D. Convirtiendo P204 a numérico\n",
      "Tipo de dato de P204 después del cambio: float64\n",
      "\n",
      "Subconjunto después de las modificaciones:\n",
      "   CONGLOME  VIVIENDA  HOGAR  CODPERSO  P203  P204 P205 P206 P207  \\\n",
      "0      5007        22     11         1     1   1.0    2         1   \n",
      "1      5007        22     11         2     2   1.0    2         2   \n",
      "2      5007        22     11         3     3   1.0    2         1   \n",
      "3      5007        31     11         1     1   1.0    2         2   \n",
      "4      5007        31     11         2     3   2.0         2    1   \n",
      "\n",
      "  P206_modificado  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4   Adobe o tapia  \n",
      "\n",
      "Tipos de datos después de las modificaciones:\n",
      "CONGLOME              int64\n",
      "VIVIENDA              int64\n",
      "HOGAR                 int64\n",
      "CODPERSO              int64\n",
      "P203                  int64\n",
      "P204                float64\n",
      "P205               category\n",
      "P206                 object\n",
      "P207                 object\n",
      "P206_modificado      object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_13000\\4231310518.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subsample_vivienda['P205'] = subsample_vivienda['P205'].astype('category')\n",
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_13000\\4231310518.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subsample_vivienda['P206_modificado'] = subsample_vivienda['P206'].map(mapeo_p206)\n",
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_13000\\4231310518.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subsample_vivienda['P203'] = pd.to_numeric(subsample_vivienda['P203'], errors='coerce')\n",
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_13000\\4231310518.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subsample_vivienda['P204'] = pd.to_numeric(subsample_vivienda['P204'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subconjunto modificado guardado como 'subsample_vivienda_modificado.csv'\n"
     ]
    }
   ],
   "source": [
    "# Importar el segundo dataset (módulo de vivienda) con el nombre correcto\n",
    "try:\n",
    "    df_vivienda = pd.read_csv(\"Enaho01-2023-200.csv\", encoding=\"ISO-8859-10\", low_memory=False)\n",
    "    print(\"Dataset de vivienda importado con encoding ISO-8859-10\")\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        df_vivienda = pd.read_csv(\"Enaho01-2023-200.csv\", encoding=\"UTF-8\", low_memory=False)\n",
    "        print(\"Dataset de vivienda importado con encoding UTF-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: No se pudo importar el dataset de vivienda: {e}\")\n",
    "        df_vivienda = None\n",
    "\n",
    "# Si la importación fue exitosa, realizar las operaciones solicitadas\n",
    "if df_vivienda is not None:\n",
    "    # 1. Display the first 5 rows\n",
    "    print(\"\\nPrimeras 5 filas del dataset de vivienda:\")\n",
    "    print(df_vivienda.head())\n",
    "    \n",
    "    # 2. Convert the column names into a list and print it\n",
    "    column_names_vivienda = df_vivienda.columns.tolist()\n",
    "    print(\"\\nNombres de columnas del dataset de vivienda:\")\n",
    "    print(column_names_vivienda)\n",
    "    \n",
    "    # 3. Check the data types\n",
    "    print(\"\\nTipos de datos del dataset de vivienda:\")\n",
    "    print(df_vivienda.dtypes)\n",
    "    \n",
    "    # 4. Select a subsample with the required variables\n",
    "    variables_base = ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO']\n",
    "    \n",
    "    # Seleccionar variables adicionales que SÍ existen en el dataset\n",
    "    # Basado en las columnas disponibles que vimos en el output\n",
    "    variables_adicionales_vivienda = [\n",
    "        'P203',   # ¿Cuántos cuartos o habitaciones tiene su hogar? (excluye baños y cocina)\n",
    "        'P204',   # ¿En cuántos de estos cuartos duermen las personas del hogar?\n",
    "        'P205',   # ¿La vivienda que ocupa el hogar es?\n",
    "        'P206',   # Material de las paredes\n",
    "        'P207'    # Material de los pisos\n",
    "    ]\n",
    "    \n",
    "    # Verificar que las variables existan en el dataset\n",
    "    variables_existentes_vivienda = [col for col in variables_base + variables_adicionales_vivienda if col in df_vivienda.columns]\n",
    "    \n",
    "    # Crear el subconjunto\n",
    "    subsample_vivienda = df_vivienda[variables_existentes_vivienda]\n",
    "    \n",
    "    print(f\"\\nSubconjunto seleccionado con {len(variables_existentes_vivienda)} variables:\")\n",
    "    print(subsample_vivienda.head())\n",
    "    \n",
    "    # 5. Perform the following modifications:\n",
    "    print(\"\\n=== MODIFICACIONES AL SUBCONJUNTO ===\\n\")\n",
    "    \n",
    "    # A. Change the data type of a variable\n",
    "    print(\"Tipos de datos antes de las modificaciones:\")\n",
    "    print(subsample_vivienda.dtypes)\n",
    "    \n",
    "    # Convertir P205 (tipo de vivienda) a categórica\n",
    "    if 'P205' in subsample_vivienda.columns:\n",
    "        print(f\"\\nA. Cambiando tipo de dato de P205 de {subsample_vivienda['P205'].dtype} a category\")\n",
    "        subsample_vivienda['P205'] = subsample_vivienda['P205'].astype('category')\n",
    "        print(f\"Tipo de dato de P205 después del cambio: {subsample_vivienda['P205'].dtype}\")\n",
    "    \n",
    "    # B. Modify some values in a specific column\n",
    "    # Modificar valores en la columna P206 (Material de las paredes)\n",
    "    if 'P206' in subsample_vivienda.columns:\n",
    "        print(\"\\nB. Modificando valores en la columna P206 (Material de las paredes):\")\n",
    "        print(\"Valores originales en P206:\")\n",
    "        print(subsample_vivienda['P206'].value_counts())\n",
    "        \n",
    "        # Crear un mapeo de valores (basado en el diccionario de la ENAHO)\n",
    "        # Nota: Estos valores deben verificarse con el diccionario oficial\n",
    "        mapeo_p206 = {\n",
    "            \"1\": \"Ladrillo o bloque de cemento\",\n",
    "            \"2\": \"Adobe o tapia\",\n",
    "            \"3\": \"Quincha\",\n",
    "            \"4\": \"Piedra con barro\",\n",
    "            \"5\": \"Piedra con cal o cemento\",\n",
    "            \"6\": \"Madera\",\n",
    "            \"7\": \"Estera\",\n",
    "            \"8\": \"Otro material\"\n",
    "        }\n",
    "        \n",
    "        # Aplicar el mapeo\n",
    "        subsample_vivienda['P206_modificado'] = subsample_vivienda['P206'].map(mapeo_p206)\n",
    "        \n",
    "        print(\"Valores modificados en P206_modificado:\")\n",
    "        print(subsample_vivienda['P206_modificado'].value_counts())\n",
    "    \n",
    "    # C. Additional modification: Convertir P203 y P204 a numéricas\n",
    "    if 'P203' in subsample_vivienda.columns:\n",
    "        print(f\"\\nC. Convirtiendo P203 a numérico\")\n",
    "        # Algunos valores podrían ser strings, así que los convertimos a numéricos, forzando los no numéricos a NaN\n",
    "        subsample_vivienda['P203'] = pd.to_numeric(subsample_vivienda['P203'], errors='coerce')\n",
    "        print(f\"Tipo de dato de P203 después del cambio: {subsample_vivienda['P203'].dtype}\")\n",
    "    \n",
    "    if 'P204' in subsample_vivienda.columns:\n",
    "        print(f\"\\nD. Convirtiendo P204 a numérico\")\n",
    "        subsample_vivienda['P204'] = pd.to_numeric(subsample_vivienda['P204'], errors='coerce')\n",
    "        print(f\"Tipo de dato de P204 después del cambio: {subsample_vivienda['P204'].dtype}\")\n",
    "    \n",
    "    # Mostrar el resultado final\n",
    "    print(\"\\nSubconjunto después de las modificaciones:\")\n",
    "    print(subsample_vivienda.head())\n",
    "    print(\"\\nTipos de datos después de las modificaciones:\")\n",
    "    print(subsample_vivienda.dtypes)\n",
    "    \n",
    "    # Guardar el subconjunto modificado\n",
    "    subsample_vivienda.to_csv(\"subsample_vivienda_modificado.csv\", index=False, encoding='utf-8')\n",
    "    print(\"\\nSubconjunto modificado guardado como 'subsample_vivienda_modificado.csv'\")\n",
    "    \n",
    "else:\n",
    "    print(\"No se pudo realizar el análisis del dataset de vivienda debido a problemas con la importación.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5df179",
   "metadata": {},
   "source": [
    "## 4. Merging Datasets: \n",
    "Identify the common columns between the two datasets (from questions 16 and 18).\n",
    "\n",
    "Verify whether the values match in both datasets. If not, correct the mismatched values to ensure a proper merge.\n",
    "\n",
    "Recommendation: Use the following as common columns:\n",
    "common_columns = ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO']\n",
    "in pd.merge(..., on=common_columns, how=...).\n",
    "\n",
    "Perform the merge.\n",
    "\n",
    "Display the first 5 rows of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1968e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_17732\\1046492653.py:5: DtypeWarning: Columns (26,32,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(\"D:/QLAB/Python/Enaho01A-2023-300.csv\", encoding=\"ISO-8859-10\")\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_17732\\1046492653.py:6: DtypeWarning: Columns (218,223,228,231,234,235,236,241,246,251,256,261,266,271,276,281,286,291,296,301,306,311,316,321,326,331,338,341,344,347,350,353,356,359,363,365,367,369,371,373,375,377,379,381,385,386,387,388,389,393,485,486,487,488,935,936,937,938,939,940,941,942,943,944,1015,1018,1023) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(\"D:/QLAB/Python/Enaho01a-2023-500.csv\", encoding=\"ISO-8859-10\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 shape: (108354, 511)\n",
      "Dataset 2 shape: (86654, 1414)\n",
      "\n",
      "==================================================\n",
      "STEP 1: Identifying common columns\n",
      "==================================================\n",
      "Columns in df1: ['AŅO', 'CODINFOR', 'CODPERSO', 'CONGLOME', 'D311B$1', 'D311B$2', 'D311B$3', 'D311B$4', 'D311B$5', 'D311B$6', 'D311B$7', 'D311D$1', 'D311D$2', 'D311D$3', 'D311D$4', 'D311D$5', 'D311D$6', 'D311D$7', 'D311D2$1', 'D311D2$2', 'D311D2$3', 'D311D2$4', 'D311D2$5', 'D311D2$6', 'D311D2$7', 'D311D3$1', 'D311D3$2', 'D311D3$3', 'D311D3$4', 'D311D3$5', 'D311D3$6', 'D311D3$7', 'D311D4$1', 'D311D4$2', 'D311D4$3', 'D311D4$4', 'D311D4$5', 'D311D4$6', 'D311D4$7', 'D311D5$1', 'D311D5$2', 'D311D5$3', 'D311D5$4', 'D311D5$5', 'D311D5$6', 'D311D5$7', 'D311D6$1', 'D311D6$2', 'D311D6$3', 'D311D6$4', 'D311D6$5', 'D311D6$6', 'D311D6$7', 'D311D7$1', 'D311D7$2', 'D311D7$3', 'D311D7$4', 'D311D7$5', 'D311D7$6', 'D311D7$7', 'D3121B', 'D3121C', 'D3121C2', 'D3121C3', 'D3121C4', 'D3121C5', 'D3121C6', 'D3122B', 'D3122C', 'D3122C2', 'D3122C3', 'D3122C4', 'D3122C5', 'D3122C6', 'D315A', 'D315B', 'D315B2', 'D315B3', 'D315B4', 'D315B5', 'D315B6', 'DOMINIO', 'ESTRATO', 'FACTOR07', 'FACTORA07', 'HOGAR', 'I311B$1', 'I311B$2', 'I311B$3', 'I311B$4', 'I311B$5', 'I311B$6', 'I311B$7', 'I311D$1', 'I311D$2', 'I311D$3', 'I311D$4', 'I311D$5', 'I311D$6', 'I311D$7', 'I311D2$1', 'I311D2$2', 'I311D2$3', 'I311D2$4', 'I311D2$5', 'I311D2$6', 'I311D2$7', 'I311D3$1', 'I311D3$2', 'I311D3$3', 'I311D3$4', 'I311D3$5', 'I311D3$6', 'I311D3$7', 'I311D4$1', 'I311D4$2', 'I311D4$3', 'I311D4$4', 'I311D4$5', 'I311D4$6', 'I311D4$7', 'I311D5$1', 'I311D5$2', 'I311D5$3', 'I311D5$4', 'I311D5$5', 'I311D5$6', 'I311D5$7', 'I311D6$1', 'I311D6$2', 'I311D6$3', 'I311D6$4', 'I311D6$5', 'I311D6$6', 'I311D6$7', 'I311D7$1', 'I311D7$2', 'I311D7$3', 'I311D7$4', 'I311D7$5', 'I311D7$6', 'I311D7$7', 'I3121B', 'I3121C', 'I3121C2', 'I3121C3', 'I3121C4', 'I3121C5', 'I3121C6', 'I3122B', 'I3122C', 'I3122C2', 'I3122C3', 'I3122C4', 'I3122C5', 'I3122C6', 'I315A', 'I315B', 'I315B2', 'I315B3', 'I315B4', 'I315B5', 'I315B6', 'IMPUTADO', 'MES', 'NCONGLOME', 'P203', 'P204', 'P205', 'P206', 'P207', 'P208A', 'P209', 'P300A', 'P300I', 'P300N', 'P301A', 'P301A0', 'P301A1', 'P301B', 'P301B0', 'P301B1', 'P301B3', 'P301C', 'P301D', 'P302', 'P302A', 'P302B', 'P302X', 'P303', 'P304A', 'P304B', 'P304C', 'P304D', 'P305', 'P306', 'P307', 'P307A1', 'P307A2', 'P307A3', 'P307A4', 'P307A4_5', 'P307A4_6', 'P307A4_7', 'P307B1', 'P307B2', 'P307B3', 'P307B4', 'P307B4_5', 'P307B4_6', 'P307B4_7', 'P307C', 'P308A', 'P308B', 'P308B1', 'P308B2', 'P308B3', 'P308B4', 'P308B5', 'P308C', 'P308C1', 'P308C2', 'P308D', 'P310', 'P310B1', 'P310C0', 'P310C1', 'P310D1', 'P310D2', 'P310E0', 'P310E1', 'P310E3', 'P311$1', 'P311$2', 'P311$3', 'P311$4', 'P311$5', 'P311$6', 'P311$7', 'P311$8', 'P311$9', 'P311A1$1', 'P311A1$2', 'P311A1$3', 'P311A1$4', 'P311A1$5', 'P311A1$6', 'P311A1$7', 'P311A1$8', 'P311A1$9', 'P311A2$1', 'P311A2$2', 'P311A2$3', 'P311A2$4', 'P311A2$5', 'P311A2$6', 'P311A2$7', 'P311A2$8', 'P311A2$9', 'P311A3$1', 'P311A3$2', 'P311A3$3', 'P311A3$4', 'P311A3$5', 'P311A3$6', 'P311A3$7', 'P311A3$8', 'P311A3$9', 'P311A4$1', 'P311A4$2', 'P311A4$3', 'P311A4$4', 'P311A4$5', 'P311A4$6', 'P311A4$7', 'P311A4$8', 'P311A4$9', 'P311A5$1', 'P311A5$2', 'P311A5$3', 'P311A5$4', 'P311A5$5', 'P311A5$6', 'P311A5$7', 'P311A5$8', 'P311A5$9', 'P311A6$1', 'P311A6$2', 'P311A6$3', 'P311A6$4', 'P311A6$5', 'P311A6$6', 'P311A6$7', 'P311A6$8', 'P311A6$9', 'P311A7$1', 'P311A7$2', 'P311A7$3', 'P311A7$4', 'P311A7$5', 'P311A7$6', 'P311A7$7', 'P311A7$8', 'P311A7$9', 'P311B$1', 'P311B$2', 'P311B$3', 'P311B$4', 'P311B$5', 'P311B$6', 'P311B$7', 'P311B$8', 'P311B$9', 'P311C$1', 'P311C$2', 'P311C$3', 'P311C$4', 'P311C$5', 'P311C$6', 'P311C$7', 'P311C$8', 'P311C$9', 'P311D$1', 'P311D$2', 'P311D$3', 'P311D$4', 'P311D$5', 'P311D$6', 'P311D$7', 'P311D$8', 'P311D$9', 'P311D2$1', 'P311D2$2', 'P311D2$3', 'P311D2$4', 'P311D2$5', 'P311D2$6', 'P311D2$7', 'P311D2$8', 'P311D2$9', 'P311D3$1', 'P311D3$2', 'P311D3$3', 'P311D3$4', 'P311D3$5', 'P311D3$6', 'P311D3$7', 'P311D3$8', 'P311D3$9', 'P311D4$1', 'P311D4$2', 'P311D4$3', 'P311D4$4', 'P311D4$5', 'P311D4$6', 'P311D4$7', 'P311D4$8', 'P311D4$9', 'P311D5$1', 'P311D5$2', 'P311D5$3', 'P311D5$4', 'P311D5$5', 'P311D5$6', 'P311D5$7', 'P311D5$8', 'P311D5$9', 'P311D6$1', 'P311D6$2', 'P311D6$3', 'P311D6$4', 'P311D6$5', 'P311D6$6', 'P311D6$7', 'P311D6$8', 'P311D6$9', 'P311D7$1', 'P311D7$2', 'P311D7$3', 'P311D7$4', 'P311D7$5', 'P311D7$6', 'P311D7$7', 'P311D7$8', 'P311D7$9', 'P311E$1', 'P311E$2', 'P311E$3', 'P311E$4', 'P311E$5', 'P311E$6', 'P311E$7', 'P311E$8', 'P311E$9', 'P311N$1', 'P311N$2', 'P311N$3', 'P311N$4', 'P311N$5', 'P311N$6', 'P311N$7', 'P311N$8', 'P311N$9', 'P311T1', 'P311T2', 'P311T22', 'P311T23', 'P311T24', 'P311T25', 'P311T26', 'P311T27', 'P3121', 'P3121A1', 'P3121A2', 'P3121A3', 'P3121A4', 'P3121A5', 'P3121A6', 'P3121B', 'P3121C', 'P3121C2', 'P3121C3', 'P3121C4', 'P3121C5', 'P3121C6', 'P3121D', 'P3122', 'P3122A1', 'P3122A2', 'P3122A3', 'P3122A4', 'P3122A5', 'P3122A6', 'P3122B', 'P3122C', 'P3122C2', 'P3122C3', 'P3122C4', 'P3122C5', 'P3122C6', 'P3122D', 'P312T1', 'P312T2', 'P312T22', 'P312T23', 'P312T24', 'P312T25', 'P312T26', 'P313', 'P314A', 'P314B$1', 'P314B$2', 'P314B$3', 'P314B$4', 'P314B$5', 'P314B$6', 'P314B$7', 'P314B1_1', 'P314B1_2', 'P314B1_6', 'P314B1_7', 'P314B1_8', 'P314B1_9', 'P314D', 'P3151', 'P3152', 'P3153', 'P3154', 'P3155', 'P3156', 'P315A', 'P315B', 'P315B2', 'P315B3', 'P315B4', 'P315B5', 'P315B6', 'P316$1', 'P316$10', 'P316$11', 'P316$12', 'P316$2', 'P316$3', 'P316$4', 'P316$5', 'P316$6', 'P316$7', 'P316$8', 'P316$9', 'P316A1', 'P316A2', 'P316A3', 'P316A4', 'P316A5', 'P316A6', 'P316B', 'P316C1', 'P316C10', 'P316C2', 'P316C3', 'P316C4', 'P316C5', 'P316C6', 'P316C7', 'P316C8', 'P316C9', 'SUB_CONGLOME', 'T313A', 'TICUEST01A', 'UBIGEO', 'VIVIENDA']\n",
      "\n",
      "Columns in df2: ['AŅO', 'CODINFOR', 'CODPERSO', 'CONGLOME', 'D524A1', 'D524B1', 'D524C1', 'D524D1', 'D524E1', 'D5291B', 'D5292B', 'D5293B', 'D5294B', 'D5295B', 'D5296B', 'D529T', 'D530A', 'D536', 'D538A1', 'D538B1', 'D538C1', 'D538D1', 'D538E1', 'D5401B', 'D5402B', 'D5403B', 'D5404B', 'D5405B', 'D5406B', 'D540T', 'D541A', 'D543', 'D5441B', 'D5442B', 'D5443B', 'D5444B', 'D5445B', 'D5446B', 'D5447B', 'D5448B', 'D544T', 'D55610C', 'D55610E', 'D55611C', 'D55611E', 'D55612C', 'D55612E', 'D55613C', 'D55613E', 'D55614C', 'D55614E', 'D5561C', 'D5561E', 'D55620C', 'D55620E', 'D55621C', 'D55621E', 'D55622C', 'D55622E', 'D55623C', 'D55623E', 'D55624C', 'D55624E', 'D55625C', 'D55625E', 'D55626C', 'D55626E', 'D55627C', 'D55627E', 'D55628C', 'D55628E', 'D5562C', 'D5562E', 'D5563C', 'D5563E', 'D5564C', 'D5564E', 'D5565C', 'D5565E', 'D5566C', 'D5566E', 'D5567C', 'D5567E', 'D5568C', 'D5568E', 'D5569C', 'D5569E', 'D556T1', 'D556T2', 'D5571C', 'D5572C', 'D5573C', 'D5574C', 'D5575C', 'D5576C', 'D5577C', 'D5578C', 'D557T', 'D558T', 'D559D1', 'D559D2', 'D559D3', 'D559D41', 'D559D410', 'D559D411', 'D559D412', 'D559D413', 'D559D414', 'D559D415', 'D559D416', 'D559D417', 'D559D418', 'D559D419', 'D559D42', 'D559D420', 'D559D421', 'D559D422', 'D559D423', 'D559D424', 'D559D425', 'D559D426', 'D559D427', 'D559D428', 'D559D429', 'D559D43', 'D559D430', 'D559D431', 'D559D432', 'D559D433', 'D559D434', 'D559D435', 'D559D436', 'D559D437', 'D559D438', 'D559D439', 'D559D44', 'D559D440', 'D559D441', 'D559D442', 'D559D443', 'D559D444', 'D559D445', 'D559D446', 'D559D447', 'D559D45', 'D559D46', 'D559D47', 'D559D48', 'D559D49', 'D560D1', 'D560D10', 'D560D2', 'D560D3', 'D560D4', 'D560D5', 'D560D6', 'D560D7', 'D560D8', 'D560D9', 'DOMINIO', 'EMPLPSEC', 'ESTRATO', 'FAC500A', 'HOGAR', 'I513T', 'I518', 'I520', 'I524A1', 'I524B1', 'I524C1', 'I524D1', 'I524E1', 'I5294B', 'I530A', 'I538A1', 'I538B1', 'I538C1', 'I538D1', 'I538E1', 'I5404B', 'I541A', 'I559D1', 'I559D2', 'I559D3', 'I559D41', 'I559D410', 'I559D411', 'I559D412', 'I559D413', 'I559D414', 'I559D415', 'I559D416', 'I559D417', 'I559D418', 'I559D419', 'I559D42', 'I559D420', 'I559D421', 'I559D422', 'I559D423', 'I559D424', 'I559D425', 'I559D426', 'I559D427', 'I559D428', 'I559D429', 'I559D43', 'I559D430', 'I559D431', 'I559D432', 'I559D433', 'I559D434', 'I559D435', 'I559D436', 'I559D437', 'I559D438', 'I559D439', 'I559D44', 'I559D440', 'I559D441', 'I559D442', 'I559D443', 'I559D444', 'I559D445', 'I559D446', 'I559D447', 'I559D45', 'I559D46', 'I559D47', 'I559D48', 'I559D49', 'I560D1', 'I560D10', 'I560D2', 'I560D3', 'I560D4', 'I560D5', 'I560D6', 'I560D7', 'I560D8', 'I560D9', 'IMPUTADO', 'MES', 'NCONGLOME', 'OCU500', 'OCUPINF', 'P203', 'P204', 'P205', 'P206', 'P207', 'P208A', 'P209', 'P301A', 'P500A', 'P500B', 'P500B1', 'P500C', 'P500D', 'P500D1', 'P500I', 'P500N', 'P501', 'P502', 'P503', 'P504', 'P5041', 'P50410', 'P50411', 'P5042', 'P5043', 'P5044', 'P5045', 'P5046', 'P5047', 'P5048', 'P5049', 'P505', 'P505B', 'P505R4', 'P506', 'P506R4', 'P507', 'P508', 'P509', 'P510', 'P510A1', 'P510B', 'P5111', 'P51110', 'P51111', 'P51112', 'P5112', 'P5113', 'P5114', 'P5115', 'P5116', 'P5117', 'P5118', 'P5119', 'P511A', 'P512A', 'P512B', 'P513', 'P513A', 'P513A1', 'P513A2', 'P513B', 'P513C', 'P513D', 'P513E', 'P513F', 'P513G', 'P513T', 'P514', 'P5151', 'P51510', 'P51511', 'P5152', 'P5153', 'P5154', 'P5155', 'P5156', 'P5157', 'P5158', 'P5159', 'P516', 'P516R4', 'P517', 'P517A', 'P517B1', 'P517C', 'P517D1', 'P517D2', 'P518', 'P519', 'P520', 'P520A', 'P521', 'P521A', 'P521B', 'P521B1', 'P521C', 'P521D', 'P523', 'P524A1', 'P524A2', 'P524B1', 'P524B2', 'P524C1', 'P524C2', 'P524D1', 'P524D2', 'P524E1', 'P524E2', 'P528', 'P5291A', 'P5291B', 'P5291C', 'P5292A', 'P5292B', 'P5292C', 'P5293A', 'P5293B', 'P5293C', 'P5294A', 'P5294B', 'P5294C', 'P5295A', 'P5295B', 'P5295C', 'P5296A', 'P5296B', 'P5296C', 'P5297A', 'P529T', 'P530A', 'P530B', 'P535', 'P536', 'P5371', 'P53710', 'P53711', 'P53712', 'P5372', 'P5373', 'P5374', 'P5375', 'P5376', 'P5377', 'P5378', 'P5379', 'P538A1', 'P538A2', 'P538B1', 'P538B2', 'P538C1', 'P538C2', 'P538D1', 'P538D2', 'P538E1', 'P538E2', 'P539', 'P5401A', 'P5401B', 'P5401C', 'P5402A', 'P5402B', 'P5402C', 'P5403A', 'P5403B', 'P5403C', 'P5404A', 'P5404B', 'P5404C', 'P5405A', 'P5405B', 'P5405C', 'P5406A', 'P5406B', 'P5406C', 'P5407A', 'P540T', 'P541A', 'P541B', 'P542', 'P543', 'P5441A', 'P5441B', 'P5442A', 'P5442B', 'P5443A', 'P5443B', 'P5444A', 'P5444B', 'P5445A', 'P5445B', 'P5446A', 'P5446B', 'P5447A', 'P5447B', 'P5448A', 'P5448B', 'P544T', 'P545', 'P546', 'P547', 'P548', 'P549', 'P550_1', 'P550_2', 'P550_3', 'P550_4', 'P550_5', 'P550_6', 'P550_7', 'P550_GI', 'P551', 'P552', 'P554', 'P554R4', 'P555', 'P55610A', 'P55610B', 'P55610C', 'P55610D', 'P55610E', 'P55611A', 'P55611B', 'P55611C', 'P55611D', 'P55611E', 'P55612A', 'P55612B', 'P55612C', 'P55612D', 'P55612E', 'P55613A', 'P55613B', 'P55613C', 'P55613D', 'P55613E', 'P55614A', 'P55614B', 'P55614C', 'P55614D', 'P55614E', 'P5561A', 'P5561B', 'P5561C', 'P5561D', 'P5561E', 'P55620A', 'P55620B', 'P55620C', 'P55620D', 'P55620E', 'P55621A', 'P55621B', 'P55621C', 'P55621D', 'P55621E', 'P55622A', 'P55622B', 'P55622C', 'P55622D', 'P55622E', 'P55623A', 'P55623B', 'P55623C', 'P55623D', 'P55623E', 'P55624A', 'P55624B', 'P55624C', 'P55624D', 'P55624E', 'P55625A', 'P55625B', 'P55625C', 'P55625D', 'P55625E', 'P55626A', 'P55626B', 'P55626C', 'P55626D', 'P55626E', 'P55627A', 'P55627B', 'P55627C', 'P55627D', 'P55627E', 'P55628A', 'P55628B', 'P55628C', 'P55628D', 'P55628E', 'P5562A', 'P5562B', 'P5562C', 'P5562D', 'P5562E', 'P5563A', 'P5563B', 'P5563C', 'P5563C1', 'P5563D', 'P5563E', 'P5563F', 'P5563G', 'P5564A', 'P5564B', 'P5564C', 'P5564D', 'P5564E', 'P5565A', 'P5565B', 'P5565C', 'P5565D', 'P5565E', 'P5566A', 'P5566B', 'P5566C', 'P5566D', 'P5566E', 'P5567A', 'P5567B', 'P5567C', 'P5567D', 'P5567E', 'P5568A', 'P5568B', 'P5568C', 'P5568D', 'P5568E', 'P5569A', 'P5569B', 'P5569C', 'P5569D', 'P5569E', 'P556T1', 'P556T2', 'P5571A', 'P5571B', 'P5571C', 'P5572A', 'P5572B', 'P5572C', 'P5573A', 'P5573B', 'P5573C', 'P5574A', 'P5574B', 'P5574C', 'P5575A', 'P5575B', 'P5575C', 'P5576A', 'P5576B', 'P5576C', 'P5577A', 'P5577B', 'P5577C', 'P5578A', 'P5578B', 'P5578C', 'P557T', 'P55810A', 'P55810B', 'P5581A', 'P5581B', 'P5582A', 'P5582B', 'P5583A', 'P5583B', 'P5584A', 'P5584B', 'P5585A', 'P5585B', 'P5586A', 'P5586B', 'P5587A', 'P5587B', 'P5588A', 'P5588B', 'P5589A', 'P5589B', 'P558A1', 'P558A2', 'P558A3', 'P558A4', 'P558A5', 'P558B1', 'P558B2', 'P558B3', 'P558C', 'P558D2_1', 'P558D2_2', 'P558E1_1', 'P558E1_2', 'P558E1_3', 'P558E1_6', 'P558E1_7', 'P558E2_1', 'P558E2_2', 'P558E3_1', 'P558E3_2', 'P558F1A', 'P558F1B', 'P558G1', 'P558G2', 'P558G3', 'P558G5', 'P558G6', 'P558G7', 'P558H10_1', 'P558H10_2', 'P558H10_3', 'P558H10_4', 'P558H10_5', 'P558H10_6', 'P558H11_1', 'P558H11_2', 'P558H11_3', 'P558H11_4', 'P558H11_5', 'P558H11_6', 'P558H12_1', 'P558H12_2', 'P558H12_3', 'P558H12_4', 'P558H12_5', 'P558H12_6', 'P558H1_1', 'P558H1_2', 'P558H1_3', 'P558H1_4', 'P558H1_5', 'P558H1_6', 'P558H2_1', 'P558H2_2', 'P558H2_3', 'P558H2_4', 'P558H2_5', 'P558H2_6', 'P558H3_1', 'P558H3_2', 'P558H3_3', 'P558H3_4', 'P558H3_5', 'P558H3_6', 'P558H4_1', 'P558H4_2', 'P558H4_3', 'P558H4_4', 'P558H4_5', 'P558H4_6', 'P558H5_1', 'P558H5_2', 'P558H5_3', 'P558H5_4', 'P558H5_5', 'P558H5_6', 'P558H6_1', 'P558H6_2', 'P558H6_3', 'P558H6_4', 'P558H6_5', 'P558H6_6', 'P558H7_1', 'P558H7_2', 'P558H7_3', 'P558H7_4', 'P558H7_5', 'P558H7_6', 'P558H8_1', 'P558H8_2', 'P558H8_3', 'P558H8_4', 'P558H8_5', 'P558H8_6', 'P558H9_1', 'P558H9_2', 'P558H9_3', 'P558H9_4', 'P558H9_5', 'P558H9_6', 'P558T', 'P558T1', 'P559$01', 'P559$02', 'P559$03', 'P559$04', 'P559$05', 'P559$06', 'P559$07', 'P559$08', 'P559$09', 'P559$10', 'P559$11', 'P559$12', 'P559$13', 'P559$14', 'P559$15', 'P559$16', 'P559$17', 'P559$18', 'P559$19', 'P559$20', 'P559$21', 'P559$22', 'P559$23', 'P559$24', 'P559$25', 'P559$26', 'P559$27', 'P559$28', 'P559$29', 'P559$30', 'P559$31', 'P559$32', 'P559$33', 'P559$34', 'P559$35', 'P559$36', 'P559$37', 'P559$38', 'P559$39', 'P559$40', 'P559$41', 'P559$42', 'P559$43', 'P559$44', 'P559$45', 'P559$46', 'P559$47', 'P559$48', 'P559$49', 'P559$50', 'P559A$01', 'P559A$02', 'P559A$03', 'P559A$04', 'P559A$05', 'P559A$06', 'P559A$07', 'P559A$08', 'P559A$09', 'P559A$10', 'P559A$11', 'P559A$12', 'P559A$13', 'P559A$14', 'P559A$15', 'P559A$16', 'P559A$17', 'P559A$18', 'P559A$19', 'P559A$20', 'P559A$21', 'P559A$22', 'P559A$23', 'P559A$24', 'P559A$25', 'P559A$26', 'P559A$27', 'P559A$28', 'P559A$29', 'P559A$30', 'P559A$31', 'P559A$32', 'P559A$33', 'P559A$34', 'P559A$35', 'P559A$36', 'P559A$37', 'P559A$38', 'P559A$39', 'P559A$40', 'P559A$41', 'P559A$42', 'P559A$43', 'P559A$44', 'P559A$45', 'P559A$46', 'P559A$47', 'P559A$48', 'P559A$49', 'P559A$50', 'P559B$01', 'P559B$02', 'P559B$03', 'P559B$04', 'P559B$05', 'P559B$06', 'P559B$07', 'P559B$08', 'P559B$09', 'P559B$10', 'P559B$11', 'P559B$12', 'P559B$13', 'P559B$14', 'P559B$15', 'P559B$16', 'P559B$17', 'P559B$18', 'P559B$19', 'P559B$20', 'P559B$21', 'P559B$22', 'P559B$23', 'P559B$24', 'P559B$25', 'P559B$26', 'P559B$27', 'P559B$28', 'P559B$29', 'P559B$30', 'P559B$31', 'P559B$32', 'P559B$33', 'P559B$34', 'P559B$35', 'P559B$36', 'P559B$37', 'P559B$38', 'P559B$39', 'P559B$40', 'P559B$41', 'P559B$42', 'P559B$43', 'P559B$44', 'P559B$45', 'P559B$46', 'P559B$47', 'P559B$48', 'P559B$49', 'P559B$50', 'P559C$01', 'P559C$02', 'P559C$03', 'P559C$04', 'P559C$05', 'P559C$06', 'P559C$07', 'P559C$08', 'P559C$09', 'P559C$10', 'P559C$11', 'P559C$12', 'P559C$13', 'P559C$14', 'P559C$15', 'P559C$16', 'P559C$17', 'P559C$18', 'P559C$19', 'P559C$20', 'P559C$21', 'P559C$22', 'P559C$23', 'P559C$24', 'P559C$25', 'P559C$26', 'P559C$27', 'P559C$28', 'P559C$29', 'P559C$30', 'P559C$31', 'P559C$32', 'P559C$33', 'P559C$34', 'P559C$35', 'P559C$36', 'P559C$37', 'P559C$38', 'P559C$39', 'P559C$40', 'P559C$41', 'P559C$42', 'P559C$43', 'P559C$44', 'P559C$45', 'P559C$46', 'P559C$47', 'P559C$48', 'P559C$49', 'P559C$50', 'P559D$01', 'P559D$02', 'P559D$03', 'P559D$04', 'P559D$05', 'P559D$06', 'P559D$07', 'P559D$08', 'P559D$09', 'P559D$10', 'P559D$11', 'P559D$12', 'P559D$13', 'P559D$14', 'P559D$15', 'P559D$16', 'P559D$17', 'P559D$18', 'P559D$19', 'P559D$20', 'P559D$21', 'P559D$22', 'P559D$23', 'P559D$24', 'P559D$25', 'P559D$26', 'P559D$27', 'P559D$28', 'P559D$29', 'P559D$30', 'P559D$31', 'P559D$32', 'P559D$33', 'P559D$34', 'P559D$35', 'P559D$36', 'P559D$37', 'P559D$38', 'P559D$39', 'P559D$40', 'P559D$41', 'P559D$42', 'P559D$43', 'P559D$44', 'P559D$45', 'P559D$46', 'P559D$47', 'P559D$48', 'P559D$49', 'P559D$50', 'P559E$01', 'P559E$02', 'P559E$03', 'P559E$04', 'P559E$05', 'P559E$06', 'P559E$07', 'P559E$08', 'P559E$09', 'P559E$10', 'P559E$11', 'P559E$12', 'P559E$13', 'P559E$14', 'P559E$15', 'P559E$16', 'P559E$17', 'P559E$18', 'P559E$19', 'P559E$20', 'P559E$21', 'P559E$22', 'P559E$23', 'P559E$24', 'P559E$25', 'P559E$26', 'P559E$27', 'P559E$28', 'P559E$29', 'P559E$30', 'P559E$31', 'P559E$32', 'P559E$33', 'P559E$34', 'P559E$35', 'P559E$36', 'P559E$37', 'P559E$38', 'P559E$39', 'P559E$40', 'P559E$41', 'P559E$42', 'P559E$43', 'P559E$44', 'P559E$45', 'P559E$46', 'P559E$47', 'P559E$48', 'P559E$49', 'P559E$50', 'P559T$01', 'P559T$02', 'P559T$03', 'P559T$04', 'P559T$05', 'P559T$06', 'P559T$07', 'P559T$08', 'P559T$09', 'P559T$10', 'P559T$11', 'P559T$12', 'P559T$13', 'P559T$14', 'P559T$15', 'P559T$16', 'P559T$17', 'P559T$18', 'P559T$19', 'P559T$20', 'P559T$21', 'P559T$22', 'P559T$23', 'P559T$24', 'P559T$25', 'P559T$26', 'P559T$27', 'P559T$28', 'P559T$29', 'P559T$30', 'P559T$31', 'P559T$32', 'P559T$33', 'P559T$34', 'P559T$35', 'P559T$36', 'P559T$37', 'P559T$38', 'P559T$39', 'P559T$40', 'P559T$41', 'P559T$42', 'P559T$43', 'P559T$44', 'P559T$45', 'P559T$46', 'P559T$47', 'P559T$48', 'P559T$49', 'P559T$50', 'P560A$01', 'P560A$02', 'P560A$03', 'P560A$04', 'P560A$05', 'P560A$06', 'P560A$07', 'P560A$08', 'P560A$09', 'P560A$10', 'P560A1$01', 'P560A1$02', 'P560A1$03', 'P560A1$04', 'P560A1$05', 'P560A1$06', 'P560A1$07', 'P560A1$08', 'P560A1$09', 'P560A1$10', 'P560C$01', 'P560C$02', 'P560C$03', 'P560C$04', 'P560C$05', 'P560C$06', 'P560C$07', 'P560C$08', 'P560C$09', 'P560C$10', 'P560D$01', 'P560D$02', 'P560D$03', 'P560D$04', 'P560D$05', 'P560D$06', 'P560D$07', 'P560D$08', 'P560D$09', 'P560D$10', 'P560E$01', 'P560E$02', 'P560E$03', 'P560E$04', 'P560E$05', 'P560E$06', 'P560E$07', 'P560E$08', 'P560E$09', 'P560E$10', 'P560F1$01', 'P560F1$02', 'P560F1$03', 'P560F1$04', 'P560F1$05', 'P560F1$06', 'P560F1$07', 'P560F1$08', 'P560F1$09', 'P560F1$10', 'P560F2$01', 'P560F2$02', 'P560F2$03', 'P560F2$04', 'P560F2$05', 'P560F2$06', 'P560F2$07', 'P560F2$08', 'P560F2$09', 'P560F2$10', 'P560T$01', 'P560T$02', 'P560T$03', 'P560T$04', 'P560T$05', 'P560T$06', 'P560T$07', 'P560T$08', 'P560T$09', 'P560T$10', 'P599', 'P59F1$01', 'P59F1$02', 'P59F1$03', 'P59F1$04', 'P59F1$05', 'P59F1$06', 'P59F1$07', 'P59F1$08', 'P59F1$09', 'P59F1$10', 'P59F1$11', 'P59F1$12', 'P59F1$13', 'P59F1$14', 'P59F1$15', 'P59F1$16', 'P59F1$17', 'P59F1$18', 'P59F1$19', 'P59F1$20', 'P59F1$21', 'P59F1$22', 'P59F1$23', 'P59F1$24', 'P59F1$25', 'P59F1$26', 'P59F1$27', 'P59F1$28', 'P59F1$29', 'P59F1$30', 'P59F1$31', 'P59F1$32', 'P59F1$33', 'P59F1$34', 'P59F1$35', 'P59F1$36', 'P59F1$37', 'P59F1$38', 'P59F1$39', 'P59F1$40', 'P59F1$41', 'P59F1$42', 'P59F1$43', 'P59F1$44', 'P59F1$45', 'P59F1$46', 'P59F1$47', 'P59F1$48', 'P59F1$49', 'P59F1$50', 'P59F2$01', 'P59F2$02', 'P59F2$03', 'P59F2$04', 'P59F2$05', 'P59F2$06', 'P59F2$07', 'P59F2$08', 'P59F2$09', 'P59F2$10', 'P59F2$11', 'P59F2$12', 'P59F2$13', 'P59F2$14', 'P59F2$15', 'P59F2$16', 'P59F2$17', 'P59F2$18', 'P59F2$19', 'P59F2$20', 'P59F2$21', 'P59F2$22', 'P59F2$23', 'P59F2$24', 'P59F2$25', 'P59F2$26', 'P59F2$27', 'P59F2$28', 'P59F2$29', 'P59F2$30', 'P59F2$31', 'P59F2$32', 'P59F2$33', 'P59F2$34', 'P59F2$35', 'P59F2$36', 'P59F2$37', 'P59F2$38', 'P59F2$39', 'P59F2$40', 'P59F2$41', 'P59F2$42', 'P59F2$43', 'P59F2$44', 'P59F2$45', 'P59F2$46', 'P59F2$47', 'P59F2$48', 'P59F2$49', 'P59F2$50', 'R559_01', 'R559_02', 'R559_03', 'R559_04', 'R559_05', 'R559_06', 'R559_07', 'R559_08', 'R559_09', 'R559_10', 'R559_11', 'R559_12', 'R559_13', 'R559_14', 'R559_15', 'R559_16', 'R559_17', 'R559_18', 'R559_19', 'R559_20', 'R559_21', 'R559_22', 'R559_23', 'R559_24', 'R559_25', 'R559_26', 'R559_27', 'R559_28', 'R559_29', 'R559_30', 'R559_31', 'R559_32', 'R559_33', 'R559_34', 'R559_35', 'R559_36', 'R559_37', 'R559_38', 'R559_39', 'R559_40', 'R559_41', 'R559_42', 'R559_43', 'R559_44', 'R559_45', 'R559_46', 'R559_47', 'R559_48', 'R559_49', 'R559_50', 'SUB_CONGLOME', 'T559B$01', 'T559B$02', 'T559B$03', 'T559B$04', 'T559B$05', 'T559B$06', 'T559B$07', 'T559B$08', 'T559B$09', 'T559B$10', 'T559B$11', 'T559B$12', 'T559B$13', 'T559B$14', 'T559B$15', 'T559B$16', 'T559B$17', 'T559B$18', 'T559B$19', 'T559B$20', 'T559B$21', 'T559B$22', 'T559B$23', 'T559B$24', 'T559B$25', 'T559B$26', 'T559B$27', 'T559B$28', 'T559B$29', 'T559B$30', 'T559B$31', 'T559B$32', 'T559B$33', 'T559B$34', 'T559B$35', 'T559B$36', 'T559B$37', 'T559B$38', 'T559B$39', 'T559B$40', 'T559B$41', 'T559B$42', 'T559B$43', 'T559B$44', 'T559B$45', 'T559B$46', 'T559B$47', 'T559B$48', 'T559B$49', 'T559B$50', 'TICUEST01A', 'UBIGEO', 'VIVIENDA', 'Z559B$01', 'Z559B$02', 'Z559B$03', 'Z559B$04', 'Z559B$05', 'Z559B$06', 'Z559B$07', 'Z559B$08', 'Z559B$09', 'Z559B$10', 'Z559B$11', 'Z559B$12', 'Z559B$13', 'Z559B$14', 'Z559B$15', 'Z559B$16', 'Z559B$17', 'Z559B$18', 'Z559B$19', 'Z559B$20', 'Z559B$21', 'Z559B$22', 'Z559B$23', 'Z559B$24', 'Z559B$25', 'Z559B$26', 'Z559B$27', 'Z559B$28', 'Z559B$29', 'Z559B$30', 'Z559B$31', 'Z559B$32', 'Z559B$33', 'Z559B$34', 'Z559B$35', 'Z559B$36', 'Z559B$37', 'Z559B$38', 'Z559B$39', 'Z559B$40', 'Z559B$41', 'Z559B$42', 'Z559B$43', 'Z559B$44', 'Z559B$45', 'Z559B$46', 'Z559B$47', 'Z559B$48', 'Z559B$49', 'Z559B$50']\n",
      "\n",
      "Actual common columns available: ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO']\n",
      "\n",
      "==================================================\n",
      "STEP 2: Verifying data types and values in common columns\n",
      "==================================================\n",
      "\n",
      "Column: CONGLOME\n",
      "df1 - dtype: int64, unique values: 5359\n",
      "df2 - dtype: int64, unique values: 5359\n",
      "\n",
      "Column: VIVIENDA\n",
      "df1 - dtype: int64, unique values: 537\n",
      "df2 - dtype: int64, unique values: 537\n",
      "\n",
      "Column: HOGAR\n",
      "df1 - dtype: int64, unique values: 10\n",
      "df2 - dtype: int64, unique values: 10\n",
      "\n",
      "Column: CODPERSO\n",
      "df1 - dtype: int64, unique values: 22\n",
      "df2 - dtype: int64, unique values: 20\n",
      "\n",
      "==================================================\n",
      "STEP 3: Checking for overlapping records\n",
      "==================================================\n",
      "Unique key combinations in df1: 108354\n",
      "Unique key combinations in df2: 86654\n",
      "Overlapping key combinations: 86654\n",
      "\n",
      "==================================================\n",
      "STEP 4: Performing the merge\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner merge results in 86654 rows\n",
      "Outer merge results in 108354 rows\n",
      "Left merge results in 108354 rows\n",
      "Right merge results in 86654 rows\n",
      "\n",
      "Using inner merge for analysis: (86654, 1921)\n",
      "\n",
      "==================================================\n",
      "STEP 5: Displaying first 5 rows of merged dataset\n",
      "==================================================\n",
      "First 5 rows of merged dataset:\n",
      "   AŅO_df1  MES_df1  CONGLOME  VIVIENDA  HOGAR  CODPERSO  UBIGEO_df1  \\\n",
      "0     2023        1      5030         2     11         1       10201   \n",
      "1     2023        1      5030         2     11         2       10201   \n",
      "2     2023        1      5030        11     11         1       10201   \n",
      "3     2023        1      5030        11     11         2       10201   \n",
      "4     2023        1      5030        11     11         3       10201   \n",
      "\n",
      "   DOMINIO_df1  ESTRATO_df1  CODINFOR_df1  ...  I538E1  I5294B  I5404B  I541A  \\\n",
      "0            7            4             1  ...                                  \n",
      "1            7            4             2  ...                                  \n",
      "2            7            4             1  ...                                  \n",
      "3            7            4             2  ...                                  \n",
      "4            7            4             3  ...                                  \n",
      "\n",
      "  OCU500 OCUPINF EMPLPSEC     FAC500A NCONGLOME_df2 SUB_CONGLOME_df2  \n",
      "0      1       2           165.623856          6618                0  \n",
      "1      1       2           112.328087          6618                0  \n",
      "2      1       1        1   96.035370          6618                0  \n",
      "3      1       1        1   86.868881          6618                0  \n",
      "4      1       1        1  185.492355          6618                0  \n",
      "\n",
      "[5 rows x 1921 columns]\n",
      "\n",
      "Merged dataset info:\n",
      "Shape: (86654, 1921)\n",
      "Columns: 1921\n",
      "Memory usage: 8949.59 MB\n",
      "\n",
      "==================================================\n",
      "STEP 6: Grouping by ESTRATO and calculating statistics\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datasets de la ENAHO\n",
    "df1 = pd.read_csv(\"D:/QLAB/Python/Enaho01A-2023-300.csv\", encoding=\"ISO-8859-10\")\n",
    "df2 = pd.read_csv(\"D:/QLAB/Python/Enaho01a-2023-500.csv\", encoding=\"ISO-8859-10\")\n",
    "\n",
    "print(\"Dataset 1 shape:\", df1.shape)\n",
    "print(\"Dataset 2 shape:\", df2.shape)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Definir columnas en común para unir\n",
    "common_columns = ['CONGLOME', 'VIVIENDA', 'HOGAR', 'CODPERSO']\n",
    "\n",
    "print(\"STEP 1: Identifying common columns\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Revisar si todas las columnas en común existen en ambos datasets\n",
    "df1_columns = set(df1.columns)\n",
    "df2_columns = set(df2.columns)\n",
    "\n",
    "print(\"Columns in df1:\", sorted(df1_columns))\n",
    "print(\"\\nColumns in df2:\", sorted(df2_columns))\n",
    "\n",
    "# Verificar que las columnas en común existan\n",
    "missing_in_df1 = [col for col in common_columns if col not in df1_columns]\n",
    "missing_in_df2 = [col for col in common_columns if col not in df2_columns]\n",
    "\n",
    "if missing_in_df1:\n",
    "    print(f\"\\nMissing in df1: {missing_in_df1}\")\n",
    "if missing_in_df2:\n",
    "    print(f\"\\nMissing in df2: {missing_in_df2}\")\n",
    "\n",
    "# Obtener las verdaders columnas en común que existen en ambos datasets\n",
    "actual_common_columns = [col for col in common_columns if col in df1_columns and col in df2_columns]\n",
    "print(f\"\\nActual common columns available: {actual_common_columns}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 2: Verifying data types and values in common columns\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for col in actual_common_columns:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(f\"df1 - dtype: {df1[col].dtype}, unique values: {df1[col].nunique()}\")\n",
    "    print(f\"df2 - dtype: {df2[col].dtype}, unique values: {df2[col].nunique()}\")\n",
    "    \n",
    "    # Check for any data type mismatches\n",
    "    if df1[col].dtype != df2[col].dtype:\n",
    "        print(f\"  WARNING: Data type mismatch!\")\n",
    "        # Convert both to string for consistent merging\n",
    "        df1[col] = df1[col].astype(str)\n",
    "        df2[col] = df2[col].astype(str)\n",
    "        print(f\"  Converted both columns to string type\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 3: Checking for overlapping records\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if actual_common_columns:\n",
    "    # Crear claves compuestas para revisar si hay traslapes\n",
    "    df1_keys = df1[actual_common_columns].drop_duplicates()\n",
    "    df2_keys = df2[actual_common_columns].drop_duplicates()\n",
    "    \n",
    "    print(f\"Unique key combinations in df1: {len(df1_keys)}\")\n",
    "    print(f\"Unique key combinations in df2: {len(df2_keys)}\")\n",
    "    \n",
    "    # Revisar si hay claves traslapadas\n",
    "    merged_keys = pd.merge(df1_keys, df2_keys, on=actual_common_columns, how='inner')\n",
    "    print(f\"Overlapping key combinations: {len(merged_keys)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 4: Performing the merge\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if actual_common_columns:\n",
    "    # Realizar diferente tipos de uniones para entender mejor la data\n",
    "    merge_types = ['inner', 'outer', 'left', 'right']\n",
    "    \n",
    "    for merge_type in merge_types:\n",
    "        merged_temp = pd.merge(df1, df2, on=actual_common_columns, how=merge_type, suffixes=('_df1', '_df2'))\n",
    "        print(f\"{merge_type.capitalize()} merge results in {merged_temp.shape[0]} rows\")\n",
    "    \n",
    "    # Usar la unión interna para el análisis principal\n",
    "    merged_df = pd.merge(df1, df2, on=actual_common_columns, how='inner', suffixes=('_df1', '_df2'))\n",
    "    print(f\"\\nUsing inner merge for analysis: {merged_df.shape}\")\n",
    "else:\n",
    "    print(\"No common columns available for merging!\")\n",
    "    merged_df = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 5: Displaying first 5 rows of merged dataset\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if merged_df is not None and not merged_df.empty:\n",
    "    print(\"First 5 rows of merged dataset:\")\n",
    "    print(merged_df.head())\n",
    "    \n",
    "    print(f\"\\nMerged dataset info:\")\n",
    "    print(f\"Shape: {merged_df.shape}\")\n",
    "    print(f\"Columns: {len(merged_df.columns)}\")\n",
    "    print(f\"Memory usage: {merged_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"No merged dataset available for display\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STEP 6: Grouping by ESTRATO and calculating statistics\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fbce83",
   "metadata": {},
   "source": [
    "## 5. In the resulting DataFrame:\n",
    "Group the data by a variable of your choice using groupby().\n",
    "\n",
    "Calculate a relevant statistical indicator, for example: the average income per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11a74a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available ESTRATO columns: ['ESTRATO_df1', 'ESTRATO_df2']\n",
      "Using column: ESTRATO_df1\n",
      "Potential income/spending columns: []\n",
      "No income-related columns found for statistical analysis\n",
      "\n",
      "General statistics for numeric columns by ESTRATO_df1:\n",
      "\n",
      "AŅO_df1:\n",
      "               mean  count\n",
      "ESTRATO_df1               \n",
      "1            2023.0  16634\n",
      "2            2023.0  17133\n",
      "3            2023.0   7216\n",
      "4            2023.0   6535\n",
      "5            2023.0  10963\n",
      "6            2023.0   4615\n",
      "7            2023.0  16982\n",
      "8            2023.0   6576\n",
      "\n",
      "MES_df1:\n",
      "             mean  count\n",
      "ESTRATO_df1             \n",
      "1            6.47  16634\n",
      "2            6.42  17133\n",
      "3            6.48   7216\n",
      "4            6.47   6535\n",
      "5            6.59  10963\n",
      "6            6.47   4615\n",
      "7            6.52  16982\n",
      "8            6.55   6576\n",
      "\n",
      "CONGLOME:\n",
      "                 mean  count\n",
      "ESTRATO_df1                 \n",
      "1            16634.40  16634\n",
      "2            17241.33  17133\n",
      "3            17744.40   7216\n",
      "4            16747.18   6535\n",
      "5            17231.51  10963\n",
      "6            16498.64   4615\n",
      "7            16458.77  16982\n",
      "8            16970.51   6576\n",
      "\n",
      "VIVIENDA:\n",
      "               mean  count\n",
      "ESTRATO_df1               \n",
      "1             75.49  16634\n",
      "2             67.10  17133\n",
      "3             66.01   7216\n",
      "4             61.84   6535\n",
      "5             61.99  10963\n",
      "6             55.27   4615\n",
      "7            127.29  16982\n",
      "8             51.46   6576\n",
      "\n",
      "HOGAR:\n",
      "              mean  count\n",
      "ESTRATO_df1              \n",
      "1            11.23  16634\n",
      "2            11.24  17133\n",
      "3            11.12   7216\n",
      "4            11.17   6535\n",
      "5            11.08  10963\n",
      "6            11.02   4615\n",
      "7            11.01  16982\n",
      "8            11.00   6576\n",
      "\n",
      "==================================================\n",
      "ANALYSIS COMPLETE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if merged_df is not None and not merged_df.empty:\n",
    "    # Revisar si la columna ESTRATO existe\n",
    "    estrato_columns = [col for col in merged_df.columns if 'ESTRATO' in col.upper()]\n",
    "    print(f\"Available ESTRATO columns: {estrato_columns}\")\n",
    "    \n",
    "    if estrato_columns:\n",
    "        # Usar la primera columna disponible de ESTRATO\n",
    "        estrato_col = estrato_columns[0]\n",
    "        print(f\"Using column: {estrato_col}\")\n",
    "        \n",
    "        # Encontrar potenciales columnas de ingresos \n",
    "        income_columns = [col for col in merged_df.columns if any(keyword in col.upper() \n",
    "                         for keyword in ['INGRESO', 'INCOME', 'SUELDO', 'SALARIO', 'GASTO'])]\n",
    "        print(f\"Potential income/spending columns: {income_columns}\")\n",
    "        \n",
    "        if income_columns:\n",
    "            # Agrupar ESTRATO y calular las estadísticas\n",
    "            grouped = merged_df.groupby(estrato_col)\n",
    "            \n",
    "            print(f\"\\nGrouping by {estrato_col}:\")\n",
    "            print(f\"Number of groups: {grouped.ngroups}\")\n",
    "            print(f\"Group sizes:\")\n",
    "            print(grouped.size())\n",
    "            \n",
    "            # Calcular las estadísticas para cada columnas de ingreso\n",
    "            for income_col in income_columns[:3]:  # Limitar los 3 primeros para evitar sobrecargar el resultado\n",
    "                print(f\"\\n--- Statistics for {income_col} by {estrato_col} ---\")\n",
    "                try:\n",
    "                    # Convertir a valores numéricos, manteniendo cualquier valor no-numerico\n",
    "                    merged_df[income_col] = pd.to_numeric(merged_df[income_col], errors='coerce')\n",
    "                    \n",
    "                    stats = grouped[income_col].agg(['count', 'mean', 'median', 'std', 'min', 'max'])\n",
    "                    print(stats.round(2))\n",
    "                    \n",
    "                    # Calcular información adicional\n",
    "                    print(f\"\\nAdditional insights for {income_col}:\")\n",
    "                    total_by_estrato = grouped[income_col].sum().sort_values(ascending=False)\n",
    "                    print(\"Total by ESTRATO (descending):\")\n",
    "                    print(total_by_estrato)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {income_col}: {e}\")\n",
    "        else:\n",
    "            print(\"No income-related columns found for statistical analysis\")\n",
    "            # Mostrar estadísticas generales en su lugar\n",
    "            numeric_cols = merged_df.select_dtypes(include=[np.number]).columns[:5]\n",
    "            if len(numeric_cols) > 0:\n",
    "                print(f\"\\nGeneral statistics for numeric columns by {estrato_col}:\")\n",
    "                for col in numeric_cols:\n",
    "                    print(f\"\\n{col}:\")\n",
    "                    print(merged_df.groupby(estrato_col)[col].agg(['mean', 'count']).round(2))\n",
    "    else:\n",
    "        print(\"No ESTRATO column found in merged dataset\")\n",
    "        print(\"Available columns:\", list(merged_df.columns))\n",
    "else:\n",
    "    print(\"No merged dataset available for grouping analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd4af12-7c83-4e6b-bc12-53b76a445920",
   "metadata": {},
   "source": [
    "# Part 2 - If conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49200bd-a375-4324-9ad0-9290a0f0c7d4",
   "metadata": {},
   "source": [
    "## 6. Basic If Condition Write a Python function that checks if a given number is positive.\n",
    "\n",
    "If the number is greater than zero, return \"The number X is positive. \"Otherwise, return \"The number X is not positive.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc815a9-0a47-40c6-bd36-937e95fdf46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number 5 is positive.\n",
      "The number -3 is not positive.\n",
      "The number 0 is not positive.\n",
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# se debe crear una función las condiciones mencionadas:\n",
    "def check_positive(number):\n",
    "    if number > 0:\n",
    "        return f\"The number {number} is positive.\"\n",
    "    else:\n",
    "        return f\"The number {number} is not positive.\"\n",
    "\n",
    "# Aplicamos la función y verificamos los resultados:\n",
    "print(check_positive(5))    # The number 5 is positive.\n",
    "print(check_positive(-3))   # The number -3 is not positive.\n",
    "print(check_positive(0))    # The number 0 is not positive.\n",
    "\n",
    "# Efectivamente, la función arroja los resultados esperados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb5516-5852-40dd-aaa8-c3217635e9e7",
   "metadata": {},
   "source": [
    "## 7. If Condition with Multiple Expressions Create a program that checks the temperature (in Celsius) and returns a message depending on the value:\n",
    "If the temperature is below 0, return \"It is freezing.\". If the temperature is between 0 and 20, return \"It is cold.\". If the temperature is between 21 and 30, return \"It is warm.\". If the temperature is greater than 30, return \"It is hot.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0429ea3-f789-4c89-873b-105b67fc8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se debe crear una función las condiciones mencionadas:\n",
    "def check_temperature(temp):\n",
    "    if temp < 0:\n",
    "        return \"It is freezing.\"\n",
    "    elif temp <= 20:\n",
    "        return \"It is cold.\"\n",
    "    elif temp <= 30:\n",
    "        return \"It is warm.\"\n",
    "    else:\n",
    "        return \"It is hot.\"\n",
    "\n",
    "# Aplicamos la función y verificamos los resultados:\n",
    "print(check_temperature(-5))   # It is freezing.\n",
    "print(check_temperature(0))    # It is cold.\n",
    "print(check_temperature(15))   # It is cold.\n",
    "print(check_temperature(25))   # It is warm.\n",
    "print(check_temperature(35))   # It is hot.\n",
    "\n",
    "# Efectivamente, la función arroja los resultados esperados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfd6171-db41-462e-aa38-85197c0a1694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eligible for scholarship.\n",
      "Not eligible for scholarship.\n",
      "A\n",
      "B+\n",
      "C\n",
      "Fail\n"
     ]
    }
   ],
   "source": [
    "# se debe crear una función con las condiciones mencionadas:\n",
    "def becado(gpa, extracurricular, horasservicio):\n",
    "    if gpa > 3.5 and (extracurricular == \"Si\" or horasservicio > 50):\n",
    "        return \"Eligible for scholarship.\"\n",
    "    else:\n",
    "        return \"Not eligible for scholarship.\"\n",
    "\n",
    "# Ahora se prueba la función. \n",
    "\n",
    "print(becado(3.8, \"Si\", 20))   \n",
    "\n",
    "# en este caso es elegible debido a que cumple GPA y actividades. \n",
    "\n",
    "print(becado(2.2, \"No\", 20))   \n",
    "\n",
    "# en este caso no es elegible debido a que no cumple con GPA, extracurricular y horas de servicio.\n",
    "\n",
    "list1 = [1, 2, 3]\n",
    "list2 = [1, 2, 3]\n",
    "list3 = list1 \n",
    "list1 is list2\n",
    "list1 is list3\n",
    "list1 == list2\n",
    "\n",
    "# para resolver este ejercicio se necesita utilizar la función if y elif\n",
    "# como piden la creación de función se utiliza def\n",
    "\n",
    "def calificacion(nota):\n",
    "    if nota >= 90:\n",
    "        return \"A\"\n",
    "    elif 80 <= nota <= 89:\n",
    "        if nota == 85: # función anidada (nested)\n",
    "            return \"B+\"\n",
    "        else:\n",
    "            return \"B\"\n",
    "    elif 70 <= nota <= 79:\n",
    "        return \"C\"\n",
    "    else:\n",
    "        return \"Fail\"\n",
    "\n",
    "# la función está creada. Ahora se le debe someter a prueba\n",
    "print(calificacion(92))\n",
    "# se observa que la función arroja \"A\", y es correcto acorde a la función.\n",
    "\n",
    "print(calificacion(85))\n",
    "# se observa que la función arroja \"B+\". Esto es correcto acorde a la función creada. \n",
    "\n",
    "print(calificacion(79))\n",
    "\n",
    "# similar al caso anterior, se arroja el resultado correcto acorde a la función. \n",
    "print(calificacion(55))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de698b41-9251-4f72-ac31-de9126fa36dc",
   "metadata": {},
   "source": [
    "## 8. Logical Operators [2 pts]\n",
    "Write a function that determines if a person is eligible for a scholarship based on these conditions:\n",
    "The person must have a GPA greater than 3.5 AND\n",
    "Either their extracurricular activities are \"Yes\" OR they have community service hours greater than 50.\n",
    "The function should return:\n",
    "\n",
    "\"Eligible for scholarship.\" if conditions are met.\n",
    "\"Not eligible for scholarship.\" otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ce9dbc8-b4fb-47c5-8893-f252fbe10bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se debe crear una función con las condiciones mencionadas:\n",
    "def becado(gpa, extracurricular, horasservicio):\n",
    "    if gpa > 3.5 and (extracurricular == \"Si\" or horasservicio > 50):\n",
    "        return \"Eligible for scholarship.\"\n",
    "    else:\n",
    "        return \"Not eligible for scholarship.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57b21788-321a-4a29-bae7-360ccc0ab4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eligible for scholarship.\n"
     ]
    }
   ],
   "source": [
    "# Ahora se prueba la función. \n",
    "\n",
    "print(becado(3.8, \"Si\", 20))   \n",
    "\n",
    "# en este caso es elegible debido a que cumple GPA y actividades. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37b3d606-06b9-4a68-8b5c-2f269254e58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not eligible for scholarship.\n"
     ]
    }
   ],
   "source": [
    "print(becado(2.2, \"No\", 20))   \n",
    "\n",
    "# en este caso no es elegible debido a que no cumple con GPA, extracurricular y horas de servicio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a86b9b4-d509-4bc2-8801-2d7b86b85c3e",
   "metadata": {},
   "source": [
    "## 9. Python Identity Operators\n",
    "Create two lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34450bdb-d6d2-4b41-b13d-6354a8deba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1, 2, 3]\n",
    "list2 = [1, 2, 3]\n",
    "list3 = list1 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d638fc2-0ae5-46c8-9105-b9b859e71813",
   "metadata": {},
   "source": [
    "Check with identity operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f331f5e7-9ac4-4985-b7c6-56a535bce822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 is list2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d85cf6eb-95e9-4b72-8d10-6d115739477a",
   "metadata": {},
   "source": [
    "La respuesta es un bool y es Falso. A pesar de tener los mismos elementos, Python los almacena como dos objetos diferentes debido a que tienen nombres diferentes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a899c9f-7bb9-40c6-bf70-0710e0453c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 is list3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac5250ff-ab67-4575-950e-ff22aee39ab4",
   "metadata": {},
   "source": [
    "La respuesta es un bool y es True. En este caso, se tiene que observar el orden. En primer lugar se ha creado el objeto list1 y luego se ha creado el objeto list3 basado en el list1. Es por esto que python los identifica como una misma lista, y ambos referencian al mismo objeto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c45bf1d-beca-4e7b-bbb7-7441ccb07356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 == list2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e629ecd-c53e-4b18-a9ee-532826bbe9e1",
   "metadata": {},
   "source": [
    "La respuesta es un bool y es True. Como el operador es el simbolo de igual (==), python compara los elementos de ambos objetos y concluye que ambos son iguales. Esto podría entender que \"is\" es para identificar objetos en la memoria y \"==\" es para comparar los elementos de los objetos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8062ba3-2d6e-4db5-ad49-849f8b93c703",
   "metadata": {},
   "source": [
    "## 10. Nested If Statement [2 pts]\n",
    "\n",
    "Write a function that takes a student's score and determines the grade:\n",
    "\n",
    "If the score is greater than or equal to 90, return \"A\".\n",
    "If the score is between 80 and 89:\n",
    "Check if the score is exactly 85, then return \"B+\".\n",
    "Otherwise, return \"B\".\n",
    "If the score is between 70 and 79, return \"C\".\n",
    "Otherwise, return \"Fail\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9c8c772-bd92-4688-815f-c3551a5903d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para resolver este ejercicio se necesita utilizar la función if y elif\n",
    "# como piden la creación de función se utiliza def\n",
    "\n",
    "def calificacion(nota):\n",
    "    if nota >= 90:\n",
    "        return \"A\"\n",
    "    elif 80 <= nota <= 89:\n",
    "        if nota == 85: # función anidada (nested)\n",
    "            return \"B+\"\n",
    "        else:\n",
    "            return \"B\"\n",
    "    elif 70 <= nota <= 79:\n",
    "        return \"C\"\n",
    "    else:\n",
    "        return \"Fail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0a023f1-3db5-49ad-aa57-a79187c928e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "# la función está creada. Ahora se le debe someter a prueba\n",
    "print(calificacion(92))\n",
    "# se observa que la función arroja \"A\", y es correcto acorde a la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f271a573-2cc5-4054-b937-ff6b8c0e7712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B+\n"
     ]
    }
   ],
   "source": [
    "print(calificacion(85))\n",
    "# se observa que la función arroja \"B+\". Esto es correcto acorde a la función creada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "567fbd2e-dd81-449d-8789-7e33aaa27b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n"
     ]
    }
   ],
   "source": [
    "print(calificacion(79))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55081f5-1fe6-4dd2-a328-b73c2eb0a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar al caso anterior, se arroja el resultado correcto acorde a la función. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22d34681-8d55-4a8b-8bb2-cfb29d4ecf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail\n"
     ]
    }
   ],
   "source": [
    "print(calificacion(55))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b77e1bc-42e8-4da8-a564-42321ae559bd",
   "metadata": {},
   "source": [
    "Se observa que al colocar un valor fuera del rango de la función, el resultado es correcto acorde a la función (Fail). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af97b8ec-db78-458b-b0b9-936bdf1fd3e2",
   "metadata": {},
   "source": [
    "# Part 3 – For loops (Luis 11 y 12, Valentina 13-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70cb6f0-3bc4-4d12-bb97-88a7ca76af16",
   "metadata": {},
   "source": [
    "## 11. For Loop in NumPy\n",
    "Write a for loop using NumPy to iterate through an array of numbers [10, 20, 30, 40, 50] and print each value multiplied by 2.\n",
    "\n",
    "Re-question: How would you modify the loop so that it stores the results in a new NumPy array instead of just printing them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c050c411-9c07-452d-a3a6-865bbe5eec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en primer lugar se debe crear el array de números\n",
    "import numpy as np\n",
    "arr = np.array([10, 20, 30, 40, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "150c4d14-0ac0-45e4-99e3-6cdc99e0aa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# ahora se debe utilizar for para que cada valor se multiplique por 2\n",
    "for number in arr:\n",
    "    print(number*2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5422fdd-f24c-425c-9eb3-bba27b0646cd",
   "metadata": {},
   "source": [
    "En este caso, al utilizar for, cada valor del objeto \"arr\" se multiplica por 2. \n",
    "Ahora, respecto a al repregunta: se requiere modificar el bucle para guardar en un nuevo resultado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3d54e0d-392a-414b-bbce-4e342c368f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20  40  60  80 100]\n"
     ]
    }
   ],
   "source": [
    "# se debe utilizar la función append\n",
    "arr = np.array([10, 20, 30, 40, 50])\n",
    "results = [] # esta lista servirá para guardar los números del nuevo objeto\n",
    "\n",
    "for number in arr:\n",
    "    results.append(number * 2)\n",
    "\n",
    "nuevo_arr = np.array(results)\n",
    "print(nuevo_arr)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "485c8e25-65ab-4eda-a842-d0beb41dcd9b",
   "metadata": {},
   "source": [
    "Con este código, ya se tiene el objeto que almacena los elementos del bucle. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72481fd-ef15-49b1-ae8e-c453ce5a8d5e",
   "metadata": {},
   "source": [
    "## 12. For Loop in List\n",
    "Create a list of words: [\"python\", \"loop\", \"list\", \"iteration\"].\n",
    "Write a for loop to print the length of each word.\n",
    "\n",
    "Re-question: How can you rewrite the same loop using a list comprehension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3264f82-30ee-415d-8f3f-0e4e67a6e94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4\n",
      "4\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# se debe crear el objeto palabras\n",
    "palabras = [\"python\", \"loop\", \"list\", \"iteration\"]\n",
    "\n",
    "for p in palabras:\n",
    "    print(len(p))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48d7e521-4ec3-48bc-87f6-9021a8c2f90e",
   "metadata": {},
   "source": [
    "Con este código, se puede observar la extensión de cada palabras.\n",
    "\n",
    "Para la repregunta se debe utilizar list comprehension en el mismo loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7733e982-6da1-4b18-b124-7b335ce5b941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 4, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "extension = [len(p) for p in palabras]\n",
    "print(extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86ad7a6-100f-41bd-9f85-36615a161c59",
   "metadata": {},
   "source": [
    "## 13. For Loop in Dictionary\n",
    "\n",
    "Given a dictionary of student scores:\n",
    "{\"Alice\": 85, \"Bob\": 92, \"Charlie\": 78, \"Diana\": 88}\n",
    "\n",
    "Write a for loop to print each student's name along with their score.\n",
    "\n",
    "Re-question: Modify the loop so that it only prints the names of students who scored above 80.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487796fb-aa30-4f9c-91f6-649c7ca6f93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: 85\n",
      "Bob: 92\n",
      "Charlie: 78\n",
      "Diana: 88\n"
     ]
    }
   ],
   "source": [
    "#imprimimos los puntajes\n",
    "scores = {\"Alice\": 85, \"Bob\": 92, \"Charlie\": 78, \"Diana\": 88}\n",
    "\n",
    "for student, score in scores.items():\n",
    "    print(f\"{student}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79db81e1-875b-4d80-881a-ed8e6702e55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: 85\n",
      "Bob: 92\n",
      "Diana: 88\n"
     ]
    }
   ],
   "source": [
    "#imprimimos únicamente estudiantes con notas mayores a 80\n",
    "scores = {\"Alice\": 85, \"Bob\": 92, \"Charlie\": 78, \"Diana\": 88}\n",
    "\n",
    "for student, score in scores.items():\n",
    "    if score > 80:\n",
    "        print(f\"{student}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f8dbd-8c0b-4f30-95f3-c99cf2fca5ba",
   "metadata": {},
   "source": [
    "## 14. For Loop using Range\n",
    "Write a for loop using range() to print all even numbers between 1 and 20.\n",
    "\n",
    "Re-question: How would you change the loop to also calculate the sum of these even numbers while iterating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf2d931-e994-40f0-8f72-2676012899da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for num in range(2, 21, 2):\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "371c2a54-5bf8-42c7-a01b-63b1def7e006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "Sumatoria de números even: 110\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for num in range(2, 21, 2):\n",
    "    print(num)\n",
    "    total += num\n",
    "\n",
    "print(f\"Sumatoria de números even: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf92a0-14cd-4b9a-8bfe-415aba71b91b",
   "metadata": {},
   "source": [
    "## 15. Iterations over Pandas (ENAHO dataset) [2 pts]\n",
    "Suppose you are analyzing the National Household Survey (ENAHO) dataset, specifically the file ENAHO01A-2023-400.\n",
    "The question of interest is P41601: “¿Cuánto fue el monto total por la compra o servicio?”.\n",
    "\n",
    "Write a for loop that iterates over the column P41601 and prints values greater than 5000.\n",
    "\n",
    "Re-question: How would you optimize this task using pandas vectorized operations (e.g., boolean indexing) instead of a for loop, to make the analysis faster and more efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afaa4a90-a90c-4633-a1f0-b37c90eef4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Dataset cargado! Procesando columna P41601...\n",
      "\n",
      "Valores > 5000 usando FOR LOOP:\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "99999.9\n",
      "\n",
      "Valores > 5000 usando Pandas (vectorizado):\n",
      "784      99999.9\n",
      "794      99999.9\n",
      "797      99999.9\n",
      "799      99999.9\n",
      "3925     99999.9\n",
      "5731     99999.9\n",
      "7555     99999.9\n",
      "8007     99999.9\n",
      "13958    99999.9\n",
      "17211    99999.9\n",
      "Name: P41601, dtype: float64\n",
      "\n",
      "Total de valores > 5000: 38\n",
      "Valor máximo: 99999.9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "relative_path = \"Downloads/Enaho01A-2023-400.csv\"\n",
    "file_path = os.path.abspath(os.path.expanduser(f\"~/{relative_path}\"))\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"Error: No se encuentra el archivo: {file_path}\")\n",
    "else:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding=\"ISO-8859-10\", usecols=['P41601'], low_memory=False)\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=\"UTF-8\", usecols=['P41601'], low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            df = None\n",
    "\n",
    "    if df is not None:\n",
    "        print(\"¡Dataset cargado! Procesando columna P41601...\\n\")\n",
    "\n",
    "        # Convertimos a numérico (descartando errores)\n",
    "        df['P41601'] = pd.to_numeric(df['P41601'], errors='coerce')\n",
    "\n",
    "        # 1) ENFOQUE CON FOR LOOP (lento)\n",
    "        print(\"Valores > 5000 usando FOR LOOP:\")\n",
    "        for value in df['P41601']:\n",
    "            if pd.notnull(value) and value > 5000:\n",
    "                print(value)\n",
    "\n",
    "        # 2) ENFOQUE OPTIMIZADO (Pandas)\n",
    "        print(\"\\nValores > 5000 usando Pandas (vectorizado):\")\n",
    "        valores_altos = df[df['P41601'] > 5000]['P41601']\n",
    "        print(valores_altos.head(10))  # mostramos solo los primeros 10\n",
    "\n",
    "        # Estadísticos adicionales\n",
    "        print(f\"\\nTotal de valores > 5000: {len(valores_altos)}\")\n",
    "        print(f\"Valor máximo: {valores_altos.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d85acb-647c-4b8d-b438-e42f0f1317c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
