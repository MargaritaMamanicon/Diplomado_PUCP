# Proyecto: Web Scrapping - Parte 1 (API de Reddit)

## üéØ Objetivo
Recopilar datos de publicaciones y comentarios de subreddits pol√≠ticos utilizando la API de Reddit (PRAW), identificar las publicaciones m√°s comunes y sus comentarios.

---

## üìå Parte 1: Configuraci√≥n de la API de Reddit y recopilaci√≥n de datos

### 1) Credenciales de la API de Reddit: Cuenta de desarrollador
- Ingres√© a [https://www.reddit.com/prefs/apps/](https://www.reddit.com/prefs/apps/).
- Cre√© una aplicaci√≥n con los siguientes datos:
  - **Nombre**: `webscrapping_group4`
  - **Tipo**: `script`
  - **About URL**: `http://localhost`
  - **Redirect URI**: `http://localhost:8080`
- Esto gener√≥ mis credenciales:
  - `client_id`
  - `client_secret`
- Las guard√© en un archivo oculto **.env** para mantenerlas seguras:
  ```env
  REDDIT_CLIENT_ID=xxxxxxxx
  REDDIT_CLIENT_SECRET=xxxxxxxx
  REDDIT_USER_AGENT=python:webscrappingProject:v1.0 (by /u/CranberryEqual8083)
  ```


### 2) Configuraci√≥n del entorno
- Activ√© mi entorno de conda:
```bash
conda activate envseleniumvivi
```
- Instal√© las librer√≠as necesarias:
```bash
pip install praw python-dotenv pandas
```
- Verifiqu√© que el archivo .env cargaba correctamente con:
```bash
python -c "import os; from dotenv import load_dotenv; load_dotenv(); \
print('ID:', os.getenv('REDDIT_CLIENT_ID')); \
print('SEC:', (os.getenv('REDDIT_CLIENT_SECRET') or '')[:5]+'...'); \
print('UA:', os.getenv('REDDIT_USER_AGENT'))"
```

‚úÖ Resultado esperado: se muestran los valores de client_id, un fragmento de client_secret y el user_agent.



### 3) Conexi√≥n API (PRAW)
- Cre√© el archivo code/api_reddit.py con el siguiente c√≥digo base:
```python
import praw
import os
from dotenv import load_dotenv

load_dotenv()

reddit = praw.Reddit(
    client_id=os.getenv("REDDIT_CLIENT_ID"),
    client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
    user_agent=os.getenv("REDDIT_USER_AGENT"),
    username=os.getenv("REDDIT_USERNAME"),
    password=os.getenv("REDDIT_PASSWORD"),
)

print("‚úÖ Conexi√≥n exitosa. Autenticado como:", reddit.user.me())
```
- Prob√© la conexi√≥n ejecutando en terminal:
```bash
python code/api_reddit.py
```
- ‚úÖ Resultado esperado:
```yaml
‚úÖ Conexi√≥n exitosa. Autenticado como: CranberryEqual8083
```
- Luego ejecut√© consultas para distintos subreddits, por ejemplo:
```bash
python code/api_reddit.py --subreddit peru --query "economia" --limit 10 --comments --max_comments 5
```
- Los resultados se guardaron en la carpeta ./salida/ como:
  - reddit_posts.csv
  - reddit_comments.csv

- ‚úÖ Ejemplo de salida:
```bash
‚úÖ Posts guardados en: ./salida/reddit_posts.csv  (10 filas)
‚úÖ Comentarios guardados en: ./salida/reddit_comments.csv  (38 filas)
```


## üìä Parte 2: Recopilaci√≥n de datos y almacenamiento

### 4) Recopilar publicaciones de subreddits
- **Subreddits de destino**:
  - `r/politics`
  - `r/PoliticalDiscussion`
  - `r/worldnews`

- **Tarea**: para cada uno de los tres subreddits, recopil√© **20 publicaciones** en modo ‚Äútop‚Äù (principales).

- **Extracci√≥n (por cada publicaci√≥n)**:
  - `title` (t√≠tulo)
  - `score` (votos positivos)
  - `num_comments` (n√∫mero de comentarios)
  - `id` (identificador √∫nico)
  - `url`

- **Almacenamiento**: los datos se guardaron en archivos CSV dentro de la carpeta `./salida/`.

### 5) Recopilar comentarios
- **Tarea**: para cada publicaci√≥n recopilada, obtuve hasta **5 comentarios**.

- **Extracci√≥n (por cada comentario)**:
  - `body` (texto del comentario)
  - `score` (votos positivos del comentario)
  - `post_id` (para enlazar a la publicaci√≥n original)

- **Almacenamiento**: los comentarios se guardaron en archivos CSV dentro de la carpeta `./salida/`.

### 6) Ejecuci√≥n en Windows CMD
Desde el directorio del proyecto ejecut√© en **CMD**:

```bat
python code\api_reddit.py --batch_subs politics,PoliticalDiscussion,worldnews --batch_mode top --batch_limit 20 --batch_max_comments 5
```

‚úÖ Resultados esperados

- Para cada subreddit:

  - Se genera un archivo *_posts.csv con 20 publicaciones.

  - Se genera un archivo *_comments.csv con 100 comentarios (20 publicaciones √ó 5 comentarios c/u).

- Ejemplo de salida:

  - ./salida/politics_posts.csv ‚Üí 20 filas

  - ./salida/politics_comments.csv ‚Üí 100 filas

  - ./salida/PoliticalDiscussion_posts.csv ‚Üí 20 filas

  - ./salida/PoliticalDiscussion_comments.csv ‚Üí 100 filas

  - ./salida/worldnews_posts.csv ‚Üí 20 filas

  - ./salida/worldnews_comments.csv ‚Üí 100 filas

üîç Validaci√≥n r√°pida con Python (en Windows CMD)
  - Para confirmar que los CSV se generaron correctamente, corr√≠ este bloque en CMD:
  ```bat
  python
  ```

```python
import pandas as pd, glob

for f in sorted(glob.glob("./salida/*_posts.csv")):
    df = pd.read_csv(f)
    print(f, len(df))

for f in sorted(glob.glob("./salida/*_comments.csv")):
    df = pd.read_csv(f)
    print(f, len(df))
```

- Salida esperada:
```bash
./salida/politics_posts.csv 20
./salida/politics_comments.csv 100
./salida/PoliticalDiscussion_posts.csv 20
./salida/PoliticalDiscussion_comments.csv 100
./salida/worldnews_posts.csv 20
./salida/worldnews_comments.csv 100
```

```yaml
---
üëâ Con esto, tu README ya documenta **lo que hiciste en la Parte 2 paso a paso**, incluyendo la validaci√≥n.  
```
